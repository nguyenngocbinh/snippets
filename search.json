[
  {
    "objectID": "Crash-Course-Python/Exercise 1 - Python.html",
    "href": "Crash-Course-Python/Exercise 1 - Python.html",
    "title": "Introduction to Python programming",
    "section": "",
    "text": "Python code is usually stored in text files with the file ending “.py”:\n\nmyprogram.py\n\nEvery line in a Python program file is assumed to be a Python statement, or part thereof.\n\nThe only exception is comment lines, which start with the character # (optionally preceded by an arbitrary number of white-space characters, i.e., tabs or spaces). Comment lines are usually ignored by the Python interpreter.\n\nTo run our Python program from the command line we use:\n\n$ python myprogram.py\n\nOn UNIX systems it is common to define the path to the interpreter on the first line of the program (note that this is a comment line as far as the Python interpreter is concerned):\n\n#!/usr/bin/env python\nIf we do, and if we additionally set the file script to be executable, we can run the program like this:\n$ myprogram.py\n\n\n\nThis file - an IPython notebook - does not follow the standard pattern with Python code in a text file. Instead, an IPython notebook is stored as a file in the JSON format. The advantage is that we can mix formatted text, Python code and code output. It requires the IPython notebook server to run it though, and therefore isn’t a stand-alone Python program as described above. Other than that, there is no difference between the Python code that goes into a program file or an IPython notebook.\n\n\n\nMost of the functionality in Python is provided by modules. The Python Standard Library is a large collection of modules that provides cross-platform implementations of common facilities such as access to the operating system, file I/O, string management, network communication, and much more.\n\n\n\nThe Python Language Reference: http://docs.python.org/2/reference/index.html\nThe Python Standard Library: http://docs.python.org/2/library/\n\nTo use a module in a Python program it first has to be imported. A module can be imported using the import statement. For example, to import the module math, which contains many standard mathematical functions, we can do:\n\nimport math\n\nThis includes the whole module and makes it available for use later in the program. For example, we can do:\n\nimport math\n\nx = math.cos(2 * math.pi)\n\nprint(x)\n\n1.0\n\n\nAlternatively, we can chose to import all symbols (functions and variables) in a module to the current namespace (so that we don’t need to use the prefix “math.” every time we use something from the math module:\n\nfrom math import *\n\nx = cos(2 * pi)\n\nprint(x)\n\n1.0\n\n\nThis pattern can be very convenient, but in large programs that include many modules it is often a good idea to keep the symbols from each module in their own namespaces, by using the import math pattern. This would elminate potentially confusing problems with name space collisions.\nAs a third alternative, we can chose to import only a few selected symbols from a module by explicitly listing which ones we want to import instead of using the wildcard character *:\n\nfrom math import cos, pi\n\nx = cos(2 * pi)\n\nprint(x)\n\n1.0\n\n\n\n\n\nOnce a module is imported, we can list the symbols it provides using the dir function:\n\nimport math\n\nprint(dir(math))\n\n['__doc__', '__file__', '__name__', '__package__', 'acos', 'acosh', 'asin', 'asinh', 'atan', 'atan2', 'atanh', 'ceil', 'copysign', 'cos', 'cosh', 'degrees', 'e', 'erf', 'erfc', 'exp', 'expm1', 'fabs', 'factorial', 'floor', 'fmod', 'frexp', 'fsum', 'gamma', 'hypot', 'isinf', 'isnan', 'ldexp', 'lgamma', 'log', 'log10', 'log1p', 'modf', 'pi', 'pow', 'radians', 'sin', 'sinh', 'sqrt', 'tan', 'tanh', 'trunc']\n\n\nAnd using the function help we can get a description of each function (almost .. not all functions have docstrings, as they are technically called, but the vast majority of functions are documented this way).\n\nhelp(math.log)\n\nHelp on built-in function log in module math:\n\nlog(...)\n    log(x[, base])\n    \n    Return the logarithm of x to the given base.\n    If the base not specified, returns the natural logarithm (base e) of x.\n\n\n\n\nlog(10)\n\n2.302585092994046\n\n\n\nlog(10, 2)\n\n3.3219280948873626\n\n\nWe can also use the help function directly on modules: Try\nhelp(math) \nSome very useful modules form the Python standard library are os, sys, math, shutil, re, subprocess, multiprocessing, threading.\nA complete lists of standard modules for Python 2 and Python 3 are available at http://docs.python.org/2/library/ and http://docs.python.org/3/library/, respectively.\n\n\n\n\n\n\nVariable names in Python can contain alphanumerical characters a-z, A-Z, 0-9 and some special characters such as _. Normal variable names must start with a letter.\nBy convention, variable names start with a lower-case letter, and Class names start with a capital letter.\nIn addition, there are a number of Python keywords that cannot be used as variable names. These keywords are:\nand, as, assert, break, class, continue, def, del, elif, else, except, \nexec, finally, for, from, global, if, import, in, is, lambda, not, or,\npass, print, raise, return, try, while, with, yield\nNote: Be aware of the keyword lambda, which could easily be a natural variable name in a scientific program. But being a keyword, it cannot be used as a variable name.\n\n\n\nThe assignment operator in Python is =. Python is a dynamically typed language, so we do not need to specify the type of a variable when we create one.\nAssigning a value to a new variable creates the variable:\n\n# variable assignments\nx = 1.0\nmy_variable = 12.2\n\nAlthough not explicitly specified, a variable does have a type associated with it. The type is derived from the value that was assigned to it.\n\ntype(x)\n\nfloat\n\n\nIf we assign a new value to a variable, its type can change.\n\nx = 1\n\n\ntype(x)\n\nint\n\n\nIf we try to use a variable that has not yet been defined we get an NameError:\n\nprint(y)\n\nNameError: name 'y' is not defined\n\n\n\n\n\n\n# integers\nx = 1\ntype(x)\n\nint\n\n\nNOTE: In a normal python file, you would have to give print(type(x)). You can skip the print command in ann IPython notebook IF it’s the last statement in that cell\n\n# float\nx = 1.0\ntype(x)\n\nfloat\n\n\n\n# boolean\nb1 = True\nb2 = False\n\ntype(b1)\n\nbool\n\n\n\n# complex numbers: note the use of `j` to specify the imaginary part\nx = 1.0 - 1.0j\ntype(x)\n\ncomplex\n\n\n\nprint(x)\n\n(1-1j)\n\n\n\nprint(x.real, x.imag)\n\n(1.0, -1.0)\n\n\n\n\n\nThe module types contains a number of type name definitions that can be used to test if variables are of certain types:\n\nimport types\n\n# print all types defined in the `types` module\nprint(dir(types))\n\n['BooleanType', 'BufferType', 'BuiltinFunctionType', 'BuiltinMethodType', 'ClassType', 'CodeType', 'ComplexType', 'DictProxyType', 'DictType', 'DictionaryType', 'EllipsisType', 'FileType', 'FloatType', 'FrameType', 'FunctionType', 'GeneratorType', 'GetSetDescriptorType', 'InstanceType', 'IntType', 'LambdaType', 'ListType', 'LongType', 'MemberDescriptorType', 'MethodType', 'ModuleType', 'NoneType', 'NotImplementedType', 'ObjectType', 'SliceType', 'StringType', 'StringTypes', 'TracebackType', 'TupleType', 'TypeType', 'UnboundMethodType', 'UnicodeType', 'XRangeType', '__all__', '__builtins__', '__doc__', '__file__', '__name__', '__package__']\n\n\n\nx = 1.0\n\n# check if the variable x is a float\ntype(x) is float\n\nTrue\n\n\n\n# check if the variable x is an int\ntype(x) is int\n\nFalse\n\n\nWe can also use the isinstance method for testing types of variables:\n\nisinstance(x, float)\n\nTrue\n\n\n\n\n\n\nx = 1.5\n\nprint(x, type(x))\n\n(1.5, &lt;type 'float'&gt;)\n\n\n\nx = int(x)\n\nprint(x, type(x))\n\n(1, &lt;type 'int'&gt;)\n\n\n\nz = complex(x)\n\nprint(z, type(z))\n\n((1+0j), &lt;type 'complex'&gt;)\n\n\n\nx = float(z)\n\nTypeError: can't convert complex to float\n\n\nComplex variables cannot be cast to floats or integers. We need to use z.real or z.imag to extract the part of the complex number we want:\n\ny = bool(z.real)\n\nprint(z.real, \" -&gt; \", y, type(y))\n\ny = bool(z.imag)\n\nprint(z.imag, \" -&gt; \", y, type(y))\n\n(1.0, ' -&gt; ', True, &lt;type 'bool'&gt;)\n(0.0, ' -&gt; ', False, &lt;type 'bool'&gt;)\n\n\n\n\n\n\nMost operators and comparisons in Python work as one would expect:\n\nArithmetic operators +, -, *, /, // (integer division), ’**’ power\n\n\n1 + 2, 1 - 2, 1 * 2, 1 / 2\n\n(3, -1, 2, 0)\n\n\n\n1.0 + 2.0, 1.0 - 2.0, 1.0 * 2.0, 1.0 / 2.0\n\n(3.0, -1.0, 2.0, 0.5)\n\n\n\n# Integer division of float numbers\n3.0 // 2.0\n\n1.0\n\n\n\n# Note! The power operators in python isn't ^, but **\n2 ** 2\n\n4\n\n\nNote: The / operator always performs a floating point division in Python 3.x. This is not true in Python 2.x, where the result of / is always an integer if the operands are integers. to be more specific, 1/2 = 0.5 (float) in Python 3.x, and 1/2 = 0 (int) in Python 2.x (but 1.0/2 = 0.5 in Python 2.x).\n\nThe boolean operators are spelled out as the words and, not, or.\n\n\nTrue and False\n\nFalse\n\n\n\nnot False\n\nTrue\n\n\n\nTrue or False\n\nTrue\n\n\n\nComparison operators &gt;, &lt;, &gt;= (greater or equal), &lt;= (less or equal), == equality, is identical.\n\n\n2 &gt; 1, 2 &lt; 1\n\n(True, False)\n\n\n\n2 &gt; 2, 2 &lt; 2\n\n(False, False)\n\n\n\n2 &gt;= 2, 2 &lt;= 2\n\n(True, True)\n\n\n\n# equality\n[1,2] == [1,2]\n\nTrue\n\n\n\n# objects identical?\nl1 = l2 = [1,2]\n\nl1 is l2\n\nTrue\n\n\n\n\n\n\n\nStrings are the variable type that is used for storing text messages.\n\ns = \"Hello world\"\ntype(s)\n\nstr\n\n\n\n# length of the string: the number of characters\nlen(s)\n\n11\n\n\n\n# replace a substring in a string with something else\ns2 = s.replace(\"world\", \"test\")\nprint(s2)\n\nHello test\n\n\nWe can index a character in a string using []:\n\ns[0]\n\n'H'\n\n\nHeads up MATLAB users: Indexing start at 0!\nWe can extract a part of a string using the syntax [start:stop], which extracts characters between index start and stop -1 (the character at index stop is not included):\n\ns[0:5]\n\n'Hello'\n\n\n\ns[4:5]\n\n'o'\n\n\nIf we omit either (or both) of start or stop from [start:stop], the default is the beginning and the end of the string, respectively:\n\ns[:5]\n\n'Hello'\n\n\n\ns[6:]\n\n'world'\n\n\n\ns[:]\n\n'Hello world'\n\n\nWe can also define the step size using the syntax [start:end:step] (the default value for step is 1, as we saw above):\n\ns[::1]\n\n'Hello world'\n\n\n\ns[::2]\n\n'Hlowrd'\n\n\nThis technique is called slicing. Read more about the syntax here: http://docs.python.org/release/2.7.3/library/functions.html?highlight=slice#slice\nPython has a very rich set of functions for text processing. See for example http://docs.python.org/2/library/string.html for more information.\n\n\n\nprint(\"str1\", \"str2\", \"str3\")  # The print statement concatenates strings with a space\n\n('str1', 'str2', 'str3')\n\n\n\nprint(\"str1\", 1.0, False, -1j)  # The print statements converts all arguments to strings\n\n('str1', 1.0, False, -1j)\n\n\n\nprint(\"str1\" + \"str2\" + \"str3\") # strings added with + are concatenated without space\n\nstr1str2str3\n\n\n\nprint(\"value = %f\" % 1.0)       # we can use C-style string formatting\n\nvalue = 1.000000\n\n\n\n# this formatting creates a string\ns2 = \"value1 = %.2f. value2 = %d\" % (3.1415, 1.5)\n\nprint(s2)\n\nvalue1 = 3.14. value2 = 1\n\n\n\n# alternative, more intuitive way of formatting a string \ns3 = 'value1 = {0}, value2 = {1}'.format(3.1415, 1.5)\n\nprint(s3)\n\nvalue1 = 3.1415, value2 = 1.5\n\n\n\n\n\n\nLists are very similar to strings, except that each element can be of any type.\nThe syntax for creating lists in Python is [...]:\n\nl = [1,2,3,4]\n\nprint(type(l))\nprint(l)\n\n&lt;type 'list'&gt;\n[1, 2, 3, 4]\n\n\nWe can use the same slicing techniques to manipulate lists as we could use on strings:\n\nprint(l)\n\nprint(l[1:3])\n\nprint(l[::2])\n\n[1, 2, 3, 4]\n[2, 3]\n[1, 3]\n\n\nHeads up MATLAB users: Indexing starts at 0!\n\nl[0]\n\n1\n\n\nElements in a list do not all have to be of the same type:\n\nl = [1, 'a', 1.0, 1-1j]\n\nprint(l)\n\n[1, 'a', 1.0, (1-1j)]\n\n\nPython lists can be inhomogeneous and arbitrarily nested:\n\nnested_list = [1, [2, [3, [4, [5]]]]]\n\nnested_list\n\n[1, [2, [3, [4, [5]]]]]\n\n\nLists play a very important role in Python. For example they are used in loops and other flow control structures (discussed below). There are a number of convenient functions for generating lists of various types, for example the range function:\n\nstart = 10\nstop = 30\nstep = 2\n\nrange(start, stop, step)\n\n[10, 12, 14, 16, 18, 20, 22, 24, 26, 28]\n\n\n\n# in python 3 range generates an interator, which can be converted to a list using 'list(...)'.\n# It has no effect in python 2\nlist(range(start, stop, step))\n\n[10, 12, 14, 16, 18, 20, 22, 24, 26, 28]\n\n\n\nlist(range(-10, 10))\n\n[-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\ns\n\n'Hello world'\n\n\n\n# convert a string to a list by type casting:\ns2 = list(s)\n\ns2\n\n['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n\n\n\n# sorting lists\ns2.sort()\n\nprint(s2)\n\n[' ', 'H', 'd', 'e', 'l', 'l', 'l', 'o', 'o', 'r', 'w']\n\n\n\n\n\n# create a new empty list\nl = []\n\n# add an elements using `append`\nl.append(\"A\")\nl.append(\"d\")\nl.append(\"d\")\n\nprint(l)\n\n['A', 'd', 'd']\n\n\nWe can modify lists by assigning new values to elements in the list. In technical jargon, lists are mutable.\n\nl[1] = \"p\"\nl[2] = \"p\"\n\nprint(l)\n\n['A', 'p', 'p']\n\n\n\nl[1:3] = [\"d\", \"d\"]\n\nprint(l)\n\n['A', 'd', 'd']\n\n\nInsert an element at an specific index using insert\n\nl.insert(0, \"i\")\nl.insert(1, \"n\")\nl.insert(2, \"s\")\nl.insert(3, \"e\")\nl.insert(4, \"r\")\nl.insert(5, \"t\")\n\nprint(l)\n\n['i', 'n', 's', 'e', 'r', 't', 'A', 'd', 'd']\n\n\nRemove first element with specific value using ‘remove’\n\nl.remove(\"A\")\n\nprint(l)\n\n['i', 'n', 's', 'e', 'r', 't', 'd', 'd']\n\n\nRemove an element at a specific location using del:\n\ndel l[7]\ndel l[6]\n\nprint(l)\n\n['i', 'n', 's', 'e', 'r', 't']\n\n\nSee help(list) for more details, or read the online documentation\n\n\n\n\nTuples are like lists, except that they cannot be modified once created, that is they are immutable.\nIn Python, tuples are created using the syntax (..., ..., ...), or even ..., ...:\n\npoint = (10, 20)\n\nprint(point, type(point))\n\n((10, 20), &lt;type 'tuple'&gt;)\n\n\n\npoint = 10, 20\n\nprint(point, type(point))\n\n((10, 20), &lt;type 'tuple'&gt;)\n\n\nWe can unpack a tuple by assigning it to a comma-separated list of variables:\n\nx, y = point\n\nprint(\"x =\", x)\nprint(\"y =\", y)\n\n('x =', 10)\n('y =', 20)\n\n\nIf we try to assign a new value to an element in a tuple we get an error:\n\npoint[0] = 20\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n\n\nDictionaries are also like lists, except that each element is a key-value pair. The syntax for dictionaries is {key1 : value1, ...}:\n\nparams = {\"parameter1\" : 1.0,\n          \"parameter2\" : 2.0,\n          \"parameter3\" : 3.0,}\n\nprint(type(params))\nprint(params)\n\n&lt;class 'dict'&gt;\n{'parameter1': 1.0, 'parameter2': 2.0, 'parameter3': 3.0}\n\n\n\nprint(\"parameter1 = \" + str(params[\"parameter1\"]))\nprint(\"parameter2 = \" + str(params[\"parameter2\"]))\nprint(\"parameter3 = \" + str(params[\"parameter3\"]))\n\nparameter1 = 1.0\nparameter2 = 2.0\nparameter3 = 3.0\n\n\n\nparams[\"parameter1\"] = \"A\"\nparams[\"parameter2\"] = \"B\"\n\n# add a new entry\nparams[\"parameter4\"] = \"D\"\n\nprint(\"parameter1 = \" + str(params[\"parameter1\"]))\nprint(\"parameter2 = \" + str(params[\"parameter2\"]))\nprint(\"parameter3 = \" + str(params[\"parameter3\"]))\nprint(\"parameter4 = \" + str(params[\"parameter4\"]))\n\nparameter1 = A\nparameter2 = B\nparameter3 = 3.0\nparameter4 = D\n\n\n\n\n\n\n\n\nThe Python syntax for conditional execution of code uses the keywords if, elif (else if), else:\n\nstatement1 = False\nstatement2 = False\n\nif statement1:\n    print(\"statement1 is True\")\n    \nelif statement2:\n    print(\"statement2 is True\")\n    \nelse:\n    print(\"statement1 and statement2 are False\")\n\nstatement1 and statement2 are False\n\n\nFor the first time, here we encounted a peculiar and unusual aspect of the Python programming language: Program blocks are defined by their indentation level.\nCompare to the equivalent C code:\nif (statement1)\n{\n    printf(\"statement1 is True\\n\");\n}\nelse if (statement2)\n{\n    printf(\"statement2 is True\\n\");\n}\nelse\n{\n    printf(\"statement1 and statement2 are False\\n\");\n}\nIn C blocks are defined by the enclosing curly brakets { and }. And the level of indentation (white space before the code statements) does not matter (completely optional).\nBut in Python, the extent of a code block is defined by the indentation level (usually a tab or say four white spaces). This means that we have to be careful to indent our code correctly, or else we will get syntax errors.\n\n\n\nstatement1 = statement2 = True\n\nif statement1:\n    if statement2:\n        print(\"both statement1 and statement2 are True\")\n\nboth statement1 and statement2 are True\n\n\n\n# Bad indentation!\nif statement1:\n    if statement2:\n    print(\"both statement1 and statement2 are True\")  # this line is not properly indented\n\nIndentationError: expected an indented block (&lt;ipython-input-8-ac4109c9123a&gt;, line 4)\n\n\n\nstatement1 = False \n\nif statement1:\n    print(\"printed if statement1 is True\")\n    \n    print(\"still inside the if block\")\n\n\nif statement1:\n    print(\"printed if statement1 is True\")\n    \nprint(\"now outside the if block\")\n\nnow outside the if block\n\n\n\n\n\n\n\nIn Python, loops can be programmed in a number of different ways. The most common is the for loop, which is used together with iterable objects, such as lists. The basic syntax is:\n\n\n\nfor x in [1,2,3]:\n    print(x)\n\n1\n2\n3\n\n\nThe for loop iterates over the elements of the supplied list, and executes the containing block once for each element. Any kind of list can be used in the for loop. For example:\n\nfor x in range(4): # by default range start at 0\n    print(x)\n\n0\n1\n2\n3\n\n\nNote: range(4) does not include 4 !\n\nfor x in range(-3,3):\n    print(x)\n\n-3\n-2\n-1\n0\n1\n2\n\n\n\nfor word in [\"scientific\", \"computing\", \"with\", \"python\"]:\n    print(word)\n\nscientific\ncomputing\nwith\npython\n\n\nTo iterate over key-value pairs of a dictionary:\n\nfor key, value in params.items():\n    print(key + \" = \" + str(value))\n\nparameter4 = D\nparameter1 = A\nparameter3 = 3.0\nparameter2 = B\n\n\n\nparams.items()\n\ndict_items([('parameter1', 'A'), ('parameter2', 'B'), ('parameter3', 3.0), ('parameter4', 'D')])\n\n\nSometimes it is useful to have access to the indices of the values when iterating over a list. We can use the enumerate function for this:\n\nfor idx, x in enumerate(range(-3,3)):\n    print(idx, x)\n\n(0, -3)\n(1, -2)\n(2, -1)\n(3, 0)\n(4, 1)\n(5, 2)\n\n\n\n\n\nA convenient and compact way to initialize lists:\n\nl1 = [x**2 for x in range(0,5)]\n\nprint(l1)\n\n[0, 1, 4, 9, 16]\n\n\n\n\n\n\ni = 0\n\nwhile i &lt; 5:\n    print(i)\n    \n    i = i + 1\n    \nprint(\"done\")\n\n0\n1\n2\n3\n4\ndone\n\n\nNote that the print(\"done\") statement is not part of the while loop body because of the difference in indentation.\n\n\n\n\nA function in Python is defined using the keyword def, followed by a function name, a signature within parentheses (), and a colon :. The following code, with one additional level of indentation, is the function body.\n\ndef func0():   \n    print(\"test\")\n\n\nfunc0()\n\ntest\n\n\nOptionally, but highly recommended, we can define a so called “docstring”, which is a description of the functions purpose and behaivor. The docstring should follow directly after the function definition, before the code in the function body.\n\ndef func1(s):\n    \"\"\"\n    Print a string 's' and tell how many characters it has    \n    \"\"\"\n    \n    print(s + \" has \" + str(len(s)) + \" characters\")\n\n\nhelp(func1)\n\nHelp on function func1 in module __main__:\n\nfunc1(s)\n    Print a string 's' and tell how many characters it has\n\n\n\n\nfunc1(\"test\")\n\ntest has 4 characters\n\n\nFunctions that returns a value use the return keyword:\n\ndef square(x):\n    \"\"\"\n    Return the square of x.\n    \"\"\"\n    return x ** 2\n\n\nsquare(4)\n\n16\n\n\nWe can return multiple values from a function using tuples (see above):\n\ndef powers(x):\n    \"\"\"\n    Return a few powers of x.\n    \"\"\"\n    return x ** 2, x ** 3, x ** 4\n\n\npowers(3)\n\n(9, 27, 81)\n\n\n\nx2, x3, x4 = powers(3)\n\nprint(x3)\n\n27\n\n\n\n\nIn a definition of a function, we can give default values to the arguments the function takes:\n\ndef myfunc(x, p=2, debug=False):\n    if debug:\n        print(\"evaluating myfunc for x = \" + str(x) + \" using exponent p = \" + str(p))\n    return x**p\n\nIf we don’t provide a value of the debug argument when calling the the function myfunc it defaults to the value provided in the function definition:\n\nmyfunc(5)\n\n25\n\n\n\nmyfunc(5, debug=True)\n\nevaluating myfunc for x = 5 using exponent p = 2\n\n\n25\n\n\nIf we explicitly list the name of the arguments in the function calls, they do not need to come in the same order as in the function definition. This is called keyword arguments, and is often very useful in functions that takes a lot of optional arguments.\n\nmyfunc(p=3, debug=True, x=7)\n\nevaluating myfunc for x = 7 using exponent p = 3\n\n\n343\n\n\n\n\n\nIn Python we can also create unnamed functions, using the lambda keyword:\n\nf1 = lambda x: x**2\n    \n# is equivalent to \n\ndef f2(x):\n    return x**2\n\n\nf1(2), f2(2)\n\n(4, 4)\n\n\nThis technique is useful for example when we want to pass a simple function as an argument to another function, like this:\n\n# map is a built-in python function\nmap(lambda x: x**2, range(-3,4))\n\n[9, 4, 1, 0, 1, 4, 9]\n\n\n\n# in python 3 we can use `list(...)` to convert the iterator to an explicit list\nlist(map(lambda x: x**2, range(-3,4)))\n\n[9, 4, 1, 0, 1, 4, 9]\n\n\n\n\n\n\nClasses are the key features of object-oriented programming. A class is a structure for representing an object and the operations that can be performed on the object.\nIn Python a class can contain attributes (variables) and methods (functions).\nA class is defined almost like a function, but using the class keyword, and the class definition usually contains a number of class method definitions (a function in a class).\n\nEach class method should have an argument self as its first argument. This object is a self-reference.\nSome class method names have special meaning, for example:\n\n__init__: The name of the method that is invoked when the object is first created.\n__str__ : A method that is invoked when a simple string representation of the class is needed, as for example when printed.\nThere are many more, see http://docs.python.org/2/reference/datamodel.html#special-method-names\n\n\n\nclass Point:\n    \"\"\"\n    Simple class for representing a point in a Cartesian coordinate system.\n    \"\"\"\n    \n    def __init__(self, x, y):\n        \"\"\"\n        Create a new Point at x, y.\n        \"\"\"\n        self.x = x\n        self.y = y\n        \n    def translate(self, dx, dy):\n        \"\"\"\n        Translate the point by dx and dy in the x and y direction.\n        \"\"\"\n        self.x += dx\n        self.y += dy\n        \n    def __str__(self):\n        return(\"Point at [%f, %f]\" % (self.x, self.y))\n\nTo create a new instance of a class:\n\np1 = Point(0, 0) # this will invoke the __init__ method in the Point class\n\nprint(p1)         # this will invoke the __str__ method\n\nPoint at [0.000000, 0.000000]\n\n\nTo invoke a class method in the class instance p:\n\np2 = Point(1, 1)\n\np1.translate(0.25, 1.5)\n\nprint(p1)\nprint(p2)\n\nPoint at [0.250000, 1.500000]\nPoint at [1.000000, 1.000000]\n\n\nNote that calling class methods can modifiy the state of that particular class instance, but does not effect other class instances or any global variables.\nThat is one of the nice things about object-oriented design: code such as functions and related variables are grouped in separate and independent entities.\n\n\n\nOne of the most important concepts in good programming is to reuse code and avoid repetitions.\nThe idea is to write functions and classes with a well-defined purpose and scope, and reuse these instead of repeating similar code in different part of a program (modular programming). The result is usually that readability and maintainability of a program is greatly improved. What this means in practice is that our programs have fewer bugs, are easier to extend and debug/troubleshoot.\nPython supports modular programming at different levels. Functions and classes are examples of tools for low-level modular programming. Python modules are a higher-level modular programming construct, where we can collect related variables, functions and classes in a module. A python module is defined in a python file (with file-ending .py), and it can be made accessible to other Python modules and programs using the import statement.\nConsider the following example: the file mymodule.py contains simple example implementations of a variable, function and a class:\n\n%%file mymodule.py\n\"\"\"\nExample of a python module. Contains a variable called my_variable,\na function called my_function, and a class called MyClass.\n\"\"\"\n\nmy_variable = 0\n\ndef my_function():\n    \"\"\"\n    Example function\n    \"\"\"\n    return my_variable\n    \nclass MyClass:\n    \"\"\"\n    Example class.\n    \"\"\"\n\n    def __init__(self):\n        self.variable = my_variable\n        \n    def set_variable(self, new_value):\n        \"\"\"\n        Set self.variable to a new value\n        \"\"\"\n        self.variable = new_value\n        \n    def get_variable(self):\n        return self.variable\n\nWriting mymodule.py\n\n\nWe can import the module mymodule into our Python program using import:\n\nimport mymodule\n\nUse help(module) to get a summary of what the module provides:\n\nhelp(mymodule)\n\nHelp on module mymodule:\n\nNAME\n    mymodule\n\nFILE\n    /Users/rob/Desktop/scientific-python-lectures/mymodule.py\n\nDESCRIPTION\n    Example of a python module. Contains a variable called my_variable,\n    a function called my_function, and a class called MyClass.\n\nCLASSES\n    MyClass\n    \n    class MyClass\n     |  Example class.\n     |  \n     |  Methods defined here:\n     |  \n     |  __init__(self)\n     |  \n     |  get_variable(self)\n     |  \n     |  set_variable(self, new_value)\n     |      Set self.variable to a new value\n\nFUNCTIONS\n    my_function()\n        Example function\n\nDATA\n    my_variable = 0\n\n\n\n\n\nmymodule.my_variable\n\n0\n\n\n\nmymodule.my_function() \n\n0\n\n\n\nmy_class = mymodule.MyClass() \nmy_class.set_variable(10)\nmy_class.get_variable()\n\n10\n\n\nIf we make changes to the code in mymodule.py, we need to reload it using reload:\n\nreload(mymodule)  # works only in python 2\n\n&lt;module 'mymodule' from 'mymodule.pyc'&gt;\n\n\n\n\n\nIn Python errors are managed with a special language construct called “Exceptions”. When errors occur exceptions can be raised, which interrupts the normal program flow and fallback to somewhere else in the code where the closest try-except statement is defined.\nTo generate an exception we can use the raise statement, which takes an argument that must be an instance of the class BaseException or a class derived from it.\n\nraise Exception(\"description of the error\")\n\nException: description of the error\n\n\nA typical use of exceptions is to abort functions when some error condition occurs, for example:\ndef my_function(arguments):\n\n    if not verify(arguments):\n        raise Exception(\"Invalid arguments\")\n    \n    # rest of the code goes here\nTo gracefully catch errors that are generated by functions and class methods, or by the Python interpreter itself, use the try and except statements:\ntry:\n    # normal code goes here\nexcept:\n    # code for error handling goes here\n    # this code is not executed unless the code\n    # above generated an error\nFor example:\n\ntry:\n    print(\"test\")\n    # generate an error: the variable test is not defined\n    print(test)\nexcept:\n    print(\"Caught an exception\")\n\ntest\nCaught an exception\n\n\nTo get information about the error, we can access the Exception class instance that describes the exception by using for example:\nexcept Exception as e:\n\ntry:\n    print(\"test\")\n    # generate an error: the variable test is not defined\n    print(test)\nexcept Exception as e:\n    print(\"Caught an exception:\" + str(e))\n\ntest\nCaught an exception:name 'test' is not defined\n\n\n\n\n\n\nhttp://www.python.org - The official web page of the Python programming language.\nhttp://www.python.org/dev/peps/pep-0008 - Style guide for Python programming. Highly recommended.\nhttp://www.greenteapress.com/thinkpython/ - A free book on Python programming.\nPython Essential Reference - A good reference book on Python programming.",
    "crumbs": [
      "Introduction to Python programming"
    ]
  },
  {
    "objectID": "Crash-Course-Python/Exercise 1 - Python.html#python-program-files",
    "href": "Crash-Course-Python/Exercise 1 - Python.html#python-program-files",
    "title": "Introduction to Python programming",
    "section": "",
    "text": "Python code is usually stored in text files with the file ending “.py”:\n\nmyprogram.py\n\nEvery line in a Python program file is assumed to be a Python statement, or part thereof.\n\nThe only exception is comment lines, which start with the character # (optionally preceded by an arbitrary number of white-space characters, i.e., tabs or spaces). Comment lines are usually ignored by the Python interpreter.\n\nTo run our Python program from the command line we use:\n\n$ python myprogram.py\n\nOn UNIX systems it is common to define the path to the interpreter on the first line of the program (note that this is a comment line as far as the Python interpreter is concerned):\n\n#!/usr/bin/env python\nIf we do, and if we additionally set the file script to be executable, we can run the program like this:\n$ myprogram.py",
    "crumbs": [
      "Introduction to Python programming"
    ]
  },
  {
    "objectID": "Crash-Course-Python/Exercise 1 - Python.html#ipython-notebooks-a.k.a-jupyter-notebooks",
    "href": "Crash-Course-Python/Exercise 1 - Python.html#ipython-notebooks-a.k.a-jupyter-notebooks",
    "title": "Introduction to Python programming",
    "section": "",
    "text": "This file - an IPython notebook - does not follow the standard pattern with Python code in a text file. Instead, an IPython notebook is stored as a file in the JSON format. The advantage is that we can mix formatted text, Python code and code output. It requires the IPython notebook server to run it though, and therefore isn’t a stand-alone Python program as described above. Other than that, there is no difference between the Python code that goes into a program file or an IPython notebook.",
    "crumbs": [
      "Introduction to Python programming"
    ]
  },
  {
    "objectID": "Crash-Course-Python/Exercise 1 - Python.html#modules",
    "href": "Crash-Course-Python/Exercise 1 - Python.html#modules",
    "title": "Introduction to Python programming",
    "section": "",
    "text": "Most of the functionality in Python is provided by modules. The Python Standard Library is a large collection of modules that provides cross-platform implementations of common facilities such as access to the operating system, file I/O, string management, network communication, and much more.\n\n\n\nThe Python Language Reference: http://docs.python.org/2/reference/index.html\nThe Python Standard Library: http://docs.python.org/2/library/\n\nTo use a module in a Python program it first has to be imported. A module can be imported using the import statement. For example, to import the module math, which contains many standard mathematical functions, we can do:\n\nimport math\n\nThis includes the whole module and makes it available for use later in the program. For example, we can do:\n\nimport math\n\nx = math.cos(2 * math.pi)\n\nprint(x)\n\n1.0\n\n\nAlternatively, we can chose to import all symbols (functions and variables) in a module to the current namespace (so that we don’t need to use the prefix “math.” every time we use something from the math module:\n\nfrom math import *\n\nx = cos(2 * pi)\n\nprint(x)\n\n1.0\n\n\nThis pattern can be very convenient, but in large programs that include many modules it is often a good idea to keep the symbols from each module in their own namespaces, by using the import math pattern. This would elminate potentially confusing problems with name space collisions.\nAs a third alternative, we can chose to import only a few selected symbols from a module by explicitly listing which ones we want to import instead of using the wildcard character *:\n\nfrom math import cos, pi\n\nx = cos(2 * pi)\n\nprint(x)\n\n1.0\n\n\n\n\n\nOnce a module is imported, we can list the symbols it provides using the dir function:\n\nimport math\n\nprint(dir(math))\n\n['__doc__', '__file__', '__name__', '__package__', 'acos', 'acosh', 'asin', 'asinh', 'atan', 'atan2', 'atanh', 'ceil', 'copysign', 'cos', 'cosh', 'degrees', 'e', 'erf', 'erfc', 'exp', 'expm1', 'fabs', 'factorial', 'floor', 'fmod', 'frexp', 'fsum', 'gamma', 'hypot', 'isinf', 'isnan', 'ldexp', 'lgamma', 'log', 'log10', 'log1p', 'modf', 'pi', 'pow', 'radians', 'sin', 'sinh', 'sqrt', 'tan', 'tanh', 'trunc']\n\n\nAnd using the function help we can get a description of each function (almost .. not all functions have docstrings, as they are technically called, but the vast majority of functions are documented this way).\n\nhelp(math.log)\n\nHelp on built-in function log in module math:\n\nlog(...)\n    log(x[, base])\n    \n    Return the logarithm of x to the given base.\n    If the base not specified, returns the natural logarithm (base e) of x.\n\n\n\n\nlog(10)\n\n2.302585092994046\n\n\n\nlog(10, 2)\n\n3.3219280948873626\n\n\nWe can also use the help function directly on modules: Try\nhelp(math) \nSome very useful modules form the Python standard library are os, sys, math, shutil, re, subprocess, multiprocessing, threading.\nA complete lists of standard modules for Python 2 and Python 3 are available at http://docs.python.org/2/library/ and http://docs.python.org/3/library/, respectively.",
    "crumbs": [
      "Introduction to Python programming"
    ]
  },
  {
    "objectID": "Crash-Course-Python/Exercise 1 - Python.html#variables-and-types",
    "href": "Crash-Course-Python/Exercise 1 - Python.html#variables-and-types",
    "title": "Introduction to Python programming",
    "section": "",
    "text": "Variable names in Python can contain alphanumerical characters a-z, A-Z, 0-9 and some special characters such as _. Normal variable names must start with a letter.\nBy convention, variable names start with a lower-case letter, and Class names start with a capital letter.\nIn addition, there are a number of Python keywords that cannot be used as variable names. These keywords are:\nand, as, assert, break, class, continue, def, del, elif, else, except, \nexec, finally, for, from, global, if, import, in, is, lambda, not, or,\npass, print, raise, return, try, while, with, yield\nNote: Be aware of the keyword lambda, which could easily be a natural variable name in a scientific program. But being a keyword, it cannot be used as a variable name.\n\n\n\nThe assignment operator in Python is =. Python is a dynamically typed language, so we do not need to specify the type of a variable when we create one.\nAssigning a value to a new variable creates the variable:\n\n# variable assignments\nx = 1.0\nmy_variable = 12.2\n\nAlthough not explicitly specified, a variable does have a type associated with it. The type is derived from the value that was assigned to it.\n\ntype(x)\n\nfloat\n\n\nIf we assign a new value to a variable, its type can change.\n\nx = 1\n\n\ntype(x)\n\nint\n\n\nIf we try to use a variable that has not yet been defined we get an NameError:\n\nprint(y)\n\nNameError: name 'y' is not defined\n\n\n\n\n\n\n# integers\nx = 1\ntype(x)\n\nint\n\n\nNOTE: In a normal python file, you would have to give print(type(x)). You can skip the print command in ann IPython notebook IF it’s the last statement in that cell\n\n# float\nx = 1.0\ntype(x)\n\nfloat\n\n\n\n# boolean\nb1 = True\nb2 = False\n\ntype(b1)\n\nbool\n\n\n\n# complex numbers: note the use of `j` to specify the imaginary part\nx = 1.0 - 1.0j\ntype(x)\n\ncomplex\n\n\n\nprint(x)\n\n(1-1j)\n\n\n\nprint(x.real, x.imag)\n\n(1.0, -1.0)\n\n\n\n\n\nThe module types contains a number of type name definitions that can be used to test if variables are of certain types:\n\nimport types\n\n# print all types defined in the `types` module\nprint(dir(types))\n\n['BooleanType', 'BufferType', 'BuiltinFunctionType', 'BuiltinMethodType', 'ClassType', 'CodeType', 'ComplexType', 'DictProxyType', 'DictType', 'DictionaryType', 'EllipsisType', 'FileType', 'FloatType', 'FrameType', 'FunctionType', 'GeneratorType', 'GetSetDescriptorType', 'InstanceType', 'IntType', 'LambdaType', 'ListType', 'LongType', 'MemberDescriptorType', 'MethodType', 'ModuleType', 'NoneType', 'NotImplementedType', 'ObjectType', 'SliceType', 'StringType', 'StringTypes', 'TracebackType', 'TupleType', 'TypeType', 'UnboundMethodType', 'UnicodeType', 'XRangeType', '__all__', '__builtins__', '__doc__', '__file__', '__name__', '__package__']\n\n\n\nx = 1.0\n\n# check if the variable x is a float\ntype(x) is float\n\nTrue\n\n\n\n# check if the variable x is an int\ntype(x) is int\n\nFalse\n\n\nWe can also use the isinstance method for testing types of variables:\n\nisinstance(x, float)\n\nTrue\n\n\n\n\n\n\nx = 1.5\n\nprint(x, type(x))\n\n(1.5, &lt;type 'float'&gt;)\n\n\n\nx = int(x)\n\nprint(x, type(x))\n\n(1, &lt;type 'int'&gt;)\n\n\n\nz = complex(x)\n\nprint(z, type(z))\n\n((1+0j), &lt;type 'complex'&gt;)\n\n\n\nx = float(z)\n\nTypeError: can't convert complex to float\n\n\nComplex variables cannot be cast to floats or integers. We need to use z.real or z.imag to extract the part of the complex number we want:\n\ny = bool(z.real)\n\nprint(z.real, \" -&gt; \", y, type(y))\n\ny = bool(z.imag)\n\nprint(z.imag, \" -&gt; \", y, type(y))\n\n(1.0, ' -&gt; ', True, &lt;type 'bool'&gt;)\n(0.0, ' -&gt; ', False, &lt;type 'bool'&gt;)",
    "crumbs": [
      "Introduction to Python programming"
    ]
  },
  {
    "objectID": "Crash-Course-Python/Exercise 1 - Python.html#operators-and-comparisons",
    "href": "Crash-Course-Python/Exercise 1 - Python.html#operators-and-comparisons",
    "title": "Introduction to Python programming",
    "section": "",
    "text": "Most operators and comparisons in Python work as one would expect:\n\nArithmetic operators +, -, *, /, // (integer division), ’**’ power\n\n\n1 + 2, 1 - 2, 1 * 2, 1 / 2\n\n(3, -1, 2, 0)\n\n\n\n1.0 + 2.0, 1.0 - 2.0, 1.0 * 2.0, 1.0 / 2.0\n\n(3.0, -1.0, 2.0, 0.5)\n\n\n\n# Integer division of float numbers\n3.0 // 2.0\n\n1.0\n\n\n\n# Note! The power operators in python isn't ^, but **\n2 ** 2\n\n4\n\n\nNote: The / operator always performs a floating point division in Python 3.x. This is not true in Python 2.x, where the result of / is always an integer if the operands are integers. to be more specific, 1/2 = 0.5 (float) in Python 3.x, and 1/2 = 0 (int) in Python 2.x (but 1.0/2 = 0.5 in Python 2.x).\n\nThe boolean operators are spelled out as the words and, not, or.\n\n\nTrue and False\n\nFalse\n\n\n\nnot False\n\nTrue\n\n\n\nTrue or False\n\nTrue\n\n\n\nComparison operators &gt;, &lt;, &gt;= (greater or equal), &lt;= (less or equal), == equality, is identical.\n\n\n2 &gt; 1, 2 &lt; 1\n\n(True, False)\n\n\n\n2 &gt; 2, 2 &lt; 2\n\n(False, False)\n\n\n\n2 &gt;= 2, 2 &lt;= 2\n\n(True, True)\n\n\n\n# equality\n[1,2] == [1,2]\n\nTrue\n\n\n\n# objects identical?\nl1 = l2 = [1,2]\n\nl1 is l2\n\nTrue",
    "crumbs": [
      "Introduction to Python programming"
    ]
  },
  {
    "objectID": "Crash-Course-Python/Exercise 1 - Python.html#compound-types-strings-list-and-dictionaries",
    "href": "Crash-Course-Python/Exercise 1 - Python.html#compound-types-strings-list-and-dictionaries",
    "title": "Introduction to Python programming",
    "section": "",
    "text": "Strings are the variable type that is used for storing text messages.\n\ns = \"Hello world\"\ntype(s)\n\nstr\n\n\n\n# length of the string: the number of characters\nlen(s)\n\n11\n\n\n\n# replace a substring in a string with something else\ns2 = s.replace(\"world\", \"test\")\nprint(s2)\n\nHello test\n\n\nWe can index a character in a string using []:\n\ns[0]\n\n'H'\n\n\nHeads up MATLAB users: Indexing start at 0!\nWe can extract a part of a string using the syntax [start:stop], which extracts characters between index start and stop -1 (the character at index stop is not included):\n\ns[0:5]\n\n'Hello'\n\n\n\ns[4:5]\n\n'o'\n\n\nIf we omit either (or both) of start or stop from [start:stop], the default is the beginning and the end of the string, respectively:\n\ns[:5]\n\n'Hello'\n\n\n\ns[6:]\n\n'world'\n\n\n\ns[:]\n\n'Hello world'\n\n\nWe can also define the step size using the syntax [start:end:step] (the default value for step is 1, as we saw above):\n\ns[::1]\n\n'Hello world'\n\n\n\ns[::2]\n\n'Hlowrd'\n\n\nThis technique is called slicing. Read more about the syntax here: http://docs.python.org/release/2.7.3/library/functions.html?highlight=slice#slice\nPython has a very rich set of functions for text processing. See for example http://docs.python.org/2/library/string.html for more information.\n\n\n\nprint(\"str1\", \"str2\", \"str3\")  # The print statement concatenates strings with a space\n\n('str1', 'str2', 'str3')\n\n\n\nprint(\"str1\", 1.0, False, -1j)  # The print statements converts all arguments to strings\n\n('str1', 1.0, False, -1j)\n\n\n\nprint(\"str1\" + \"str2\" + \"str3\") # strings added with + are concatenated without space\n\nstr1str2str3\n\n\n\nprint(\"value = %f\" % 1.0)       # we can use C-style string formatting\n\nvalue = 1.000000\n\n\n\n# this formatting creates a string\ns2 = \"value1 = %.2f. value2 = %d\" % (3.1415, 1.5)\n\nprint(s2)\n\nvalue1 = 3.14. value2 = 1\n\n\n\n# alternative, more intuitive way of formatting a string \ns3 = 'value1 = {0}, value2 = {1}'.format(3.1415, 1.5)\n\nprint(s3)\n\nvalue1 = 3.1415, value2 = 1.5\n\n\n\n\n\n\nLists are very similar to strings, except that each element can be of any type.\nThe syntax for creating lists in Python is [...]:\n\nl = [1,2,3,4]\n\nprint(type(l))\nprint(l)\n\n&lt;type 'list'&gt;\n[1, 2, 3, 4]\n\n\nWe can use the same slicing techniques to manipulate lists as we could use on strings:\n\nprint(l)\n\nprint(l[1:3])\n\nprint(l[::2])\n\n[1, 2, 3, 4]\n[2, 3]\n[1, 3]\n\n\nHeads up MATLAB users: Indexing starts at 0!\n\nl[0]\n\n1\n\n\nElements in a list do not all have to be of the same type:\n\nl = [1, 'a', 1.0, 1-1j]\n\nprint(l)\n\n[1, 'a', 1.0, (1-1j)]\n\n\nPython lists can be inhomogeneous and arbitrarily nested:\n\nnested_list = [1, [2, [3, [4, [5]]]]]\n\nnested_list\n\n[1, [2, [3, [4, [5]]]]]\n\n\nLists play a very important role in Python. For example they are used in loops and other flow control structures (discussed below). There are a number of convenient functions for generating lists of various types, for example the range function:\n\nstart = 10\nstop = 30\nstep = 2\n\nrange(start, stop, step)\n\n[10, 12, 14, 16, 18, 20, 22, 24, 26, 28]\n\n\n\n# in python 3 range generates an interator, which can be converted to a list using 'list(...)'.\n# It has no effect in python 2\nlist(range(start, stop, step))\n\n[10, 12, 14, 16, 18, 20, 22, 24, 26, 28]\n\n\n\nlist(range(-10, 10))\n\n[-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\ns\n\n'Hello world'\n\n\n\n# convert a string to a list by type casting:\ns2 = list(s)\n\ns2\n\n['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n\n\n\n# sorting lists\ns2.sort()\n\nprint(s2)\n\n[' ', 'H', 'd', 'e', 'l', 'l', 'l', 'o', 'o', 'r', 'w']\n\n\n\n\n\n# create a new empty list\nl = []\n\n# add an elements using `append`\nl.append(\"A\")\nl.append(\"d\")\nl.append(\"d\")\n\nprint(l)\n\n['A', 'd', 'd']\n\n\nWe can modify lists by assigning new values to elements in the list. In technical jargon, lists are mutable.\n\nl[1] = \"p\"\nl[2] = \"p\"\n\nprint(l)\n\n['A', 'p', 'p']\n\n\n\nl[1:3] = [\"d\", \"d\"]\n\nprint(l)\n\n['A', 'd', 'd']\n\n\nInsert an element at an specific index using insert\n\nl.insert(0, \"i\")\nl.insert(1, \"n\")\nl.insert(2, \"s\")\nl.insert(3, \"e\")\nl.insert(4, \"r\")\nl.insert(5, \"t\")\n\nprint(l)\n\n['i', 'n', 's', 'e', 'r', 't', 'A', 'd', 'd']\n\n\nRemove first element with specific value using ‘remove’\n\nl.remove(\"A\")\n\nprint(l)\n\n['i', 'n', 's', 'e', 'r', 't', 'd', 'd']\n\n\nRemove an element at a specific location using del:\n\ndel l[7]\ndel l[6]\n\nprint(l)\n\n['i', 'n', 's', 'e', 'r', 't']\n\n\nSee help(list) for more details, or read the online documentation\n\n\n\n\nTuples are like lists, except that they cannot be modified once created, that is they are immutable.\nIn Python, tuples are created using the syntax (..., ..., ...), or even ..., ...:\n\npoint = (10, 20)\n\nprint(point, type(point))\n\n((10, 20), &lt;type 'tuple'&gt;)\n\n\n\npoint = 10, 20\n\nprint(point, type(point))\n\n((10, 20), &lt;type 'tuple'&gt;)\n\n\nWe can unpack a tuple by assigning it to a comma-separated list of variables:\n\nx, y = point\n\nprint(\"x =\", x)\nprint(\"y =\", y)\n\n('x =', 10)\n('y =', 20)\n\n\nIf we try to assign a new value to an element in a tuple we get an error:\n\npoint[0] = 20\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n\n\nDictionaries are also like lists, except that each element is a key-value pair. The syntax for dictionaries is {key1 : value1, ...}:\n\nparams = {\"parameter1\" : 1.0,\n          \"parameter2\" : 2.0,\n          \"parameter3\" : 3.0,}\n\nprint(type(params))\nprint(params)\n\n&lt;class 'dict'&gt;\n{'parameter1': 1.0, 'parameter2': 2.0, 'parameter3': 3.0}\n\n\n\nprint(\"parameter1 = \" + str(params[\"parameter1\"]))\nprint(\"parameter2 = \" + str(params[\"parameter2\"]))\nprint(\"parameter3 = \" + str(params[\"parameter3\"]))\n\nparameter1 = 1.0\nparameter2 = 2.0\nparameter3 = 3.0\n\n\n\nparams[\"parameter1\"] = \"A\"\nparams[\"parameter2\"] = \"B\"\n\n# add a new entry\nparams[\"parameter4\"] = \"D\"\n\nprint(\"parameter1 = \" + str(params[\"parameter1\"]))\nprint(\"parameter2 = \" + str(params[\"parameter2\"]))\nprint(\"parameter3 = \" + str(params[\"parameter3\"]))\nprint(\"parameter4 = \" + str(params[\"parameter4\"]))\n\nparameter1 = A\nparameter2 = B\nparameter3 = 3.0\nparameter4 = D",
    "crumbs": [
      "Introduction to Python programming"
    ]
  },
  {
    "objectID": "Crash-Course-Python/Exercise 1 - Python.html#control-flow",
    "href": "Crash-Course-Python/Exercise 1 - Python.html#control-flow",
    "title": "Introduction to Python programming",
    "section": "",
    "text": "The Python syntax for conditional execution of code uses the keywords if, elif (else if), else:\n\nstatement1 = False\nstatement2 = False\n\nif statement1:\n    print(\"statement1 is True\")\n    \nelif statement2:\n    print(\"statement2 is True\")\n    \nelse:\n    print(\"statement1 and statement2 are False\")\n\nstatement1 and statement2 are False\n\n\nFor the first time, here we encounted a peculiar and unusual aspect of the Python programming language: Program blocks are defined by their indentation level.\nCompare to the equivalent C code:\nif (statement1)\n{\n    printf(\"statement1 is True\\n\");\n}\nelse if (statement2)\n{\n    printf(\"statement2 is True\\n\");\n}\nelse\n{\n    printf(\"statement1 and statement2 are False\\n\");\n}\nIn C blocks are defined by the enclosing curly brakets { and }. And the level of indentation (white space before the code statements) does not matter (completely optional).\nBut in Python, the extent of a code block is defined by the indentation level (usually a tab or say four white spaces). This means that we have to be careful to indent our code correctly, or else we will get syntax errors.\n\n\n\nstatement1 = statement2 = True\n\nif statement1:\n    if statement2:\n        print(\"both statement1 and statement2 are True\")\n\nboth statement1 and statement2 are True\n\n\n\n# Bad indentation!\nif statement1:\n    if statement2:\n    print(\"both statement1 and statement2 are True\")  # this line is not properly indented\n\nIndentationError: expected an indented block (&lt;ipython-input-8-ac4109c9123a&gt;, line 4)\n\n\n\nstatement1 = False \n\nif statement1:\n    print(\"printed if statement1 is True\")\n    \n    print(\"still inside the if block\")\n\n\nif statement1:\n    print(\"printed if statement1 is True\")\n    \nprint(\"now outside the if block\")\n\nnow outside the if block",
    "crumbs": [
      "Introduction to Python programming"
    ]
  },
  {
    "objectID": "Crash-Course-Python/Exercise 1 - Python.html#loops",
    "href": "Crash-Course-Python/Exercise 1 - Python.html#loops",
    "title": "Introduction to Python programming",
    "section": "",
    "text": "In Python, loops can be programmed in a number of different ways. The most common is the for loop, which is used together with iterable objects, such as lists. The basic syntax is:\n\n\n\nfor x in [1,2,3]:\n    print(x)\n\n1\n2\n3\n\n\nThe for loop iterates over the elements of the supplied list, and executes the containing block once for each element. Any kind of list can be used in the for loop. For example:\n\nfor x in range(4): # by default range start at 0\n    print(x)\n\n0\n1\n2\n3\n\n\nNote: range(4) does not include 4 !\n\nfor x in range(-3,3):\n    print(x)\n\n-3\n-2\n-1\n0\n1\n2\n\n\n\nfor word in [\"scientific\", \"computing\", \"with\", \"python\"]:\n    print(word)\n\nscientific\ncomputing\nwith\npython\n\n\nTo iterate over key-value pairs of a dictionary:\n\nfor key, value in params.items():\n    print(key + \" = \" + str(value))\n\nparameter4 = D\nparameter1 = A\nparameter3 = 3.0\nparameter2 = B\n\n\n\nparams.items()\n\ndict_items([('parameter1', 'A'), ('parameter2', 'B'), ('parameter3', 3.0), ('parameter4', 'D')])\n\n\nSometimes it is useful to have access to the indices of the values when iterating over a list. We can use the enumerate function for this:\n\nfor idx, x in enumerate(range(-3,3)):\n    print(idx, x)\n\n(0, -3)\n(1, -2)\n(2, -1)\n(3, 0)\n(4, 1)\n(5, 2)\n\n\n\n\n\nA convenient and compact way to initialize lists:\n\nl1 = [x**2 for x in range(0,5)]\n\nprint(l1)\n\n[0, 1, 4, 9, 16]\n\n\n\n\n\n\ni = 0\n\nwhile i &lt; 5:\n    print(i)\n    \n    i = i + 1\n    \nprint(\"done\")\n\n0\n1\n2\n3\n4\ndone\n\n\nNote that the print(\"done\") statement is not part of the while loop body because of the difference in indentation.",
    "crumbs": [
      "Introduction to Python programming"
    ]
  },
  {
    "objectID": "Crash-Course-Python/Exercise 1 - Python.html#functions",
    "href": "Crash-Course-Python/Exercise 1 - Python.html#functions",
    "title": "Introduction to Python programming",
    "section": "",
    "text": "A function in Python is defined using the keyword def, followed by a function name, a signature within parentheses (), and a colon :. The following code, with one additional level of indentation, is the function body.\n\ndef func0():   \n    print(\"test\")\n\n\nfunc0()\n\ntest\n\n\nOptionally, but highly recommended, we can define a so called “docstring”, which is a description of the functions purpose and behaivor. The docstring should follow directly after the function definition, before the code in the function body.\n\ndef func1(s):\n    \"\"\"\n    Print a string 's' and tell how many characters it has    \n    \"\"\"\n    \n    print(s + \" has \" + str(len(s)) + \" characters\")\n\n\nhelp(func1)\n\nHelp on function func1 in module __main__:\n\nfunc1(s)\n    Print a string 's' and tell how many characters it has\n\n\n\n\nfunc1(\"test\")\n\ntest has 4 characters\n\n\nFunctions that returns a value use the return keyword:\n\ndef square(x):\n    \"\"\"\n    Return the square of x.\n    \"\"\"\n    return x ** 2\n\n\nsquare(4)\n\n16\n\n\nWe can return multiple values from a function using tuples (see above):\n\ndef powers(x):\n    \"\"\"\n    Return a few powers of x.\n    \"\"\"\n    return x ** 2, x ** 3, x ** 4\n\n\npowers(3)\n\n(9, 27, 81)\n\n\n\nx2, x3, x4 = powers(3)\n\nprint(x3)\n\n27\n\n\n\n\nIn a definition of a function, we can give default values to the arguments the function takes:\n\ndef myfunc(x, p=2, debug=False):\n    if debug:\n        print(\"evaluating myfunc for x = \" + str(x) + \" using exponent p = \" + str(p))\n    return x**p\n\nIf we don’t provide a value of the debug argument when calling the the function myfunc it defaults to the value provided in the function definition:\n\nmyfunc(5)\n\n25\n\n\n\nmyfunc(5, debug=True)\n\nevaluating myfunc for x = 5 using exponent p = 2\n\n\n25\n\n\nIf we explicitly list the name of the arguments in the function calls, they do not need to come in the same order as in the function definition. This is called keyword arguments, and is often very useful in functions that takes a lot of optional arguments.\n\nmyfunc(p=3, debug=True, x=7)\n\nevaluating myfunc for x = 7 using exponent p = 3\n\n\n343\n\n\n\n\n\nIn Python we can also create unnamed functions, using the lambda keyword:\n\nf1 = lambda x: x**2\n    \n# is equivalent to \n\ndef f2(x):\n    return x**2\n\n\nf1(2), f2(2)\n\n(4, 4)\n\n\nThis technique is useful for example when we want to pass a simple function as an argument to another function, like this:\n\n# map is a built-in python function\nmap(lambda x: x**2, range(-3,4))\n\n[9, 4, 1, 0, 1, 4, 9]\n\n\n\n# in python 3 we can use `list(...)` to convert the iterator to an explicit list\nlist(map(lambda x: x**2, range(-3,4)))\n\n[9, 4, 1, 0, 1, 4, 9]",
    "crumbs": [
      "Introduction to Python programming"
    ]
  },
  {
    "objectID": "Crash-Course-Python/Exercise 1 - Python.html#classes",
    "href": "Crash-Course-Python/Exercise 1 - Python.html#classes",
    "title": "Introduction to Python programming",
    "section": "",
    "text": "Classes are the key features of object-oriented programming. A class is a structure for representing an object and the operations that can be performed on the object.\nIn Python a class can contain attributes (variables) and methods (functions).\nA class is defined almost like a function, but using the class keyword, and the class definition usually contains a number of class method definitions (a function in a class).\n\nEach class method should have an argument self as its first argument. This object is a self-reference.\nSome class method names have special meaning, for example:\n\n__init__: The name of the method that is invoked when the object is first created.\n__str__ : A method that is invoked when a simple string representation of the class is needed, as for example when printed.\nThere are many more, see http://docs.python.org/2/reference/datamodel.html#special-method-names\n\n\n\nclass Point:\n    \"\"\"\n    Simple class for representing a point in a Cartesian coordinate system.\n    \"\"\"\n    \n    def __init__(self, x, y):\n        \"\"\"\n        Create a new Point at x, y.\n        \"\"\"\n        self.x = x\n        self.y = y\n        \n    def translate(self, dx, dy):\n        \"\"\"\n        Translate the point by dx and dy in the x and y direction.\n        \"\"\"\n        self.x += dx\n        self.y += dy\n        \n    def __str__(self):\n        return(\"Point at [%f, %f]\" % (self.x, self.y))\n\nTo create a new instance of a class:\n\np1 = Point(0, 0) # this will invoke the __init__ method in the Point class\n\nprint(p1)         # this will invoke the __str__ method\n\nPoint at [0.000000, 0.000000]\n\n\nTo invoke a class method in the class instance p:\n\np2 = Point(1, 1)\n\np1.translate(0.25, 1.5)\n\nprint(p1)\nprint(p2)\n\nPoint at [0.250000, 1.500000]\nPoint at [1.000000, 1.000000]\n\n\nNote that calling class methods can modifiy the state of that particular class instance, but does not effect other class instances or any global variables.\nThat is one of the nice things about object-oriented design: code such as functions and related variables are grouped in separate and independent entities.",
    "crumbs": [
      "Introduction to Python programming"
    ]
  },
  {
    "objectID": "Crash-Course-Python/Exercise 1 - Python.html#modules-1",
    "href": "Crash-Course-Python/Exercise 1 - Python.html#modules-1",
    "title": "Introduction to Python programming",
    "section": "",
    "text": "One of the most important concepts in good programming is to reuse code and avoid repetitions.\nThe idea is to write functions and classes with a well-defined purpose and scope, and reuse these instead of repeating similar code in different part of a program (modular programming). The result is usually that readability and maintainability of a program is greatly improved. What this means in practice is that our programs have fewer bugs, are easier to extend and debug/troubleshoot.\nPython supports modular programming at different levels. Functions and classes are examples of tools for low-level modular programming. Python modules are a higher-level modular programming construct, where we can collect related variables, functions and classes in a module. A python module is defined in a python file (with file-ending .py), and it can be made accessible to other Python modules and programs using the import statement.\nConsider the following example: the file mymodule.py contains simple example implementations of a variable, function and a class:\n\n%%file mymodule.py\n\"\"\"\nExample of a python module. Contains a variable called my_variable,\na function called my_function, and a class called MyClass.\n\"\"\"\n\nmy_variable = 0\n\ndef my_function():\n    \"\"\"\n    Example function\n    \"\"\"\n    return my_variable\n    \nclass MyClass:\n    \"\"\"\n    Example class.\n    \"\"\"\n\n    def __init__(self):\n        self.variable = my_variable\n        \n    def set_variable(self, new_value):\n        \"\"\"\n        Set self.variable to a new value\n        \"\"\"\n        self.variable = new_value\n        \n    def get_variable(self):\n        return self.variable\n\nWriting mymodule.py\n\n\nWe can import the module mymodule into our Python program using import:\n\nimport mymodule\n\nUse help(module) to get a summary of what the module provides:\n\nhelp(mymodule)\n\nHelp on module mymodule:\n\nNAME\n    mymodule\n\nFILE\n    /Users/rob/Desktop/scientific-python-lectures/mymodule.py\n\nDESCRIPTION\n    Example of a python module. Contains a variable called my_variable,\n    a function called my_function, and a class called MyClass.\n\nCLASSES\n    MyClass\n    \n    class MyClass\n     |  Example class.\n     |  \n     |  Methods defined here:\n     |  \n     |  __init__(self)\n     |  \n     |  get_variable(self)\n     |  \n     |  set_variable(self, new_value)\n     |      Set self.variable to a new value\n\nFUNCTIONS\n    my_function()\n        Example function\n\nDATA\n    my_variable = 0\n\n\n\n\n\nmymodule.my_variable\n\n0\n\n\n\nmymodule.my_function() \n\n0\n\n\n\nmy_class = mymodule.MyClass() \nmy_class.set_variable(10)\nmy_class.get_variable()\n\n10\n\n\nIf we make changes to the code in mymodule.py, we need to reload it using reload:\n\nreload(mymodule)  # works only in python 2\n\n&lt;module 'mymodule' from 'mymodule.pyc'&gt;",
    "crumbs": [
      "Introduction to Python programming"
    ]
  },
  {
    "objectID": "Crash-Course-Python/Exercise 1 - Python.html#exceptions",
    "href": "Crash-Course-Python/Exercise 1 - Python.html#exceptions",
    "title": "Introduction to Python programming",
    "section": "",
    "text": "In Python errors are managed with a special language construct called “Exceptions”. When errors occur exceptions can be raised, which interrupts the normal program flow and fallback to somewhere else in the code where the closest try-except statement is defined.\nTo generate an exception we can use the raise statement, which takes an argument that must be an instance of the class BaseException or a class derived from it.\n\nraise Exception(\"description of the error\")\n\nException: description of the error\n\n\nA typical use of exceptions is to abort functions when some error condition occurs, for example:\ndef my_function(arguments):\n\n    if not verify(arguments):\n        raise Exception(\"Invalid arguments\")\n    \n    # rest of the code goes here\nTo gracefully catch errors that are generated by functions and class methods, or by the Python interpreter itself, use the try and except statements:\ntry:\n    # normal code goes here\nexcept:\n    # code for error handling goes here\n    # this code is not executed unless the code\n    # above generated an error\nFor example:\n\ntry:\n    print(\"test\")\n    # generate an error: the variable test is not defined\n    print(test)\nexcept:\n    print(\"Caught an exception\")\n\ntest\nCaught an exception\n\n\nTo get information about the error, we can access the Exception class instance that describes the exception by using for example:\nexcept Exception as e:\n\ntry:\n    print(\"test\")\n    # generate an error: the variable test is not defined\n    print(test)\nexcept Exception as e:\n    print(\"Caught an exception:\" + str(e))\n\ntest\nCaught an exception:name 'test' is not defined",
    "crumbs": [
      "Introduction to Python programming"
    ]
  },
  {
    "objectID": "Crash-Course-Python/Exercise 1 - Python.html#further-reading",
    "href": "Crash-Course-Python/Exercise 1 - Python.html#further-reading",
    "title": "Introduction to Python programming",
    "section": "",
    "text": "http://www.python.org - The official web page of the Python programming language.\nhttp://www.python.org/dev/peps/pep-0008 - Style guide for Python programming. Highly recommended.\nhttp://www.greenteapress.com/thinkpython/ - A free book on Python programming.\nPython Essential Reference - A good reference book on Python programming.",
    "crumbs": [
      "Introduction to Python programming"
    ]
  },
  {
    "objectID": "r-snippet/create_package.html",
    "href": "r-snippet/create_package.html",
    "title": "Hướng dẫn tạo package trên R",
    "section": "",
    "text": "Cài đặt những packages cần thiết\ninstall.packages(c(\"devtools\", \"roxygen2\", \"testthat\", \"knitr\", \"usethis\"))\n\n\nTạo package mới\nusethis::create_package(\"D:/R/how.to.create.package\")\nSau khi dùng lệnh này thì 1 project được tạo ra gồm:\n\nfolder: R dùng để lưu tất cả các hàm\nfile: DESCRIPTION và NAMESPACE: 2 file này dùng để mô tả về package và dependencies\n\n\n\nMột làm ví dụ\n# R/performance.R\n# Sensitivity\n#' @title sensitivity\n#' @description Calculate the sensitivity for a given logit model.\n#' @details For a given binary response actuals and predicted probability scores, sensitivity is defined as number of observations with the event AND predicted to have the event divided by the number of observations with the event. It can be used as an indicator to gauge how sensitive is your model in detecting the occurence of events, especially when you are not so concerned about predicting the non-events as true.\n#' @author Selva Prabhakaran \\email{selva86@@gmail.com}\n#' @export sensitivity\n#' @param actuals The actual binary flags for the response variable. It can take a numeric vector containing values of either 1 or 0, where 1 represents the 'Good' or 'Events' while 0 represents 'Bad' or 'Non-Events'.\n#' @param predictedScores The prediction probability scores for each observation. If your classification model gives the 1/0 predcitions, convert it to a numeric vector of 1's and 0's.\n#' @param threshold If predicted value is above the threshold, it will be considered as an event (1), else it will be a non-event (0). Defaults to 0.5.\n#' @return The sensitivity of the given binary response actuals and predicted probability scores, which is, the number of observations with the event AND predicted to have the event divided by the nummber of observations with the event.\n#' @examples\n#' data('ActualsAndScores')\n#' sensitivity(actuals=ActualsAndScores$Actuals, predictedScores=ActualsAndScores$PredictedScores)\nsensitivity &lt;- function(actuals, predictedScores, threshold = 0.5) {\n  predicted_dir &lt;- ifelse(predictedScores &lt; threshold, 0, 1)\n  actual_dir &lt;- actuals\n  no_with_and_predicted_to_have_event &lt;- sum(actual_dir == 1 & predicted_dir == 1, na.rm = T)\n  no_with_event &lt;- sum(actual_dir == 1, na.rm = T)\n  return(no_with_and_predicted_to_have_event/no_with_event)\n}\n\n\nChỉnh sửa DESCRIPTION\n\nChỉnh sửa thông tin như tác giả, đồng tác giả, email, License, mô tả tổng quát về package …\nThêm thông tin DESCRIPTION\n\nusethis::use_package('ggplot2', type = 'import', min_version = TRUE)\nusethis::use_package('scorecard', type = 'suggests', min_version = TRUE)\n\nlapply(c('dplyr','ggplot2', 'pROC', 'ROCR', 'tidyr', 'purrr', 'magrittr', 'tibble', 'knitr'),\n       function(pkg) usethis::use_package(pkg, type = 'suggests', min_version = TRUE))\n\n\nChỉnh sửa NAMESPACE\n\nTạo RD file\nSau khi sử dụng lệnh này thì sẽ có folder man được tạo ra, folder này chứa các hướng dẫn sử dụng các hàm trong package\n\ndevtools::document()\nroxygen2::roxygenise() # cách khác\n\n\nThêm hướng dẫn sử dụng (vignettes)\ndevtools::clean_vignettes() # NOTE: backup data before run these commands\nusethis::use_vignette(\"mba_past\")\n\nbuild vignettes\n\ndevtools::build_vignettes()\n\nSau khi sử dụng lệnh này thì sẽ có thêm folder doc chứa Rcode và các file đã được knit\n\n\n\nThêm data vào package\n\nVới dữ liệu nhỏ &lt; 1Mb thì có thể thêm luôn vào package\n\nhmeq &lt;- read.csv('D:/tmp/hmeq-data/hmeq.csv')\nnames(hmeq) &lt;- tolower(names(hmeq))\nusethis::use_data_raw(\"hmeq\")\nusethis::use_data(hmeq)\n\n#' hmeq\n#' @docType data\n#' @name hmeq\n#' @usage data(hmeq)\n#' @author Nguyen Ngoc Binh \\email{nguyenngocbinhneu@@gmail.com}\n#' @references \\url{https://codeload.github.com/Carl-Lejerskar/HMEQ/zip/master}\n#' @keywords datasets\n#' @examples\n#' data(hmeq)\nNULL\n\n\nBuild package\ndevtools::check()\ndevtools::build()",
    "crumbs": [
      "R snippet",
      "Hướng dẫn tạo package trên R"
    ]
  },
  {
    "objectID": "r-snippet/mamba.html",
    "href": "r-snippet/mamba.html",
    "title": "Create an R environment using Mamba (a faster alternative to Conda)",
    "section": "",
    "text": "Install Mamba: If you haven’t already installed Mamba, you can do so by running the following command in your terminal or command prompt:\nconda install mamba -c conda-forge\nCreate a YAML File: Create a YAML file (e.g., r_environment.yml) to specify the packages and dependencies for your R environment. Here’s an example YAML file:\nname: r_env\nchannels:\n  - r\n  - conda-forge\ndependencies:\n  - r=4.1\n  - r-ggplot2\n  - r-dplyr\n  - r-tidymodels\n  - r-mlr3\n  - r-lightgbm\n  - r-pROC\n  # Add more R packages as needed\nIn this example, we’ve included R version 4.1 along with several R packages such as ggplot2, dplyr, tidymodels, mlr3, lightgbm, and pROC. Adjust the list of packages to match your specific requirements.\nCreate the Environment: To create the R environment using Mamba and the YAML file, run the following command:\nmamba env create -f r_environment.yml\nThis will create a new Mamba environment named r_env and install the specified R packages and their dependencies.\nActivate the Environment: Activate the newly created environment with the following command:\nconda activate r_env\nNow, you’re working within the r_env environment.\nInstall Additional R Packages: You can install additional R packages within the Mamba environment using the R console, just like you would in a regular R environment. For example:\ninstall.packages(\"new_package\")\nDeactivate the Environment: When you’re finished working in the R environment, deactivate it by running:\nconda deactivate\nThis will return you to the base Conda environment or system environment.\n\nBy following these steps, you can create and manage an R environment using Mamba, which offers improved speed and efficiency compared to Conda for package and environment management.",
    "crumbs": [
      "R snippet",
      "Create an R environment using Mamba (a faster alternative to Conda)"
    ]
  },
  {
    "objectID": "r-snippet/database.html",
    "href": "r-snippet/database.html",
    "title": "database",
    "section": "",
    "text": "odbc::odbcListDrivers()\n\n\n\nlibrary(DBI)\ncon &lt;- DBI::dbConnect(odbc::odbc(), \"BILIVE\", UID=\"JUMP_B\", PWD=\"******\")\n\n\n\ndbListTables(con, schema = \"JUMP_B\")\ndbListTables(con, schema = \"SB_BI\")\n\n\n\ndbWriteTable(con, \"mtcars\", mtcars[1:5, ])",
    "crumbs": [
      "R snippet",
      "database"
    ]
  },
  {
    "objectID": "r-snippet/database.html#dbplyr",
    "href": "r-snippet/database.html#dbplyr",
    "title": "database",
    "section": "",
    "text": "odbc::odbcListDrivers()\n\n\n\nlibrary(DBI)\ncon &lt;- DBI::dbConnect(odbc::odbc(), \"BILIVE\", UID=\"JUMP_B\", PWD=\"******\")\n\n\n\ndbListTables(con, schema = \"JUMP_B\")\ndbListTables(con, schema = \"SB_BI\")\n\n\n\ndbWriteTable(con, \"mtcars\", mtcars[1:5, ])",
    "crumbs": [
      "R snippet",
      "database"
    ]
  },
  {
    "objectID": "r-snippet/database.html#rodbc",
    "href": "r-snippet/database.html#rodbc",
    "title": "database",
    "section": "RODBC",
    "text": "RODBC\n\nConnect to DB\nlibrary(RODBC)\nchannel &lt;- odbcConnect('BILIVE',\n                   uid = 'JUMP_B',\n                   pwd = '******')\n\n\nGet data\ncus &lt;- sqlQuery(con, \"select /*+ PARALLEL(11) */ * from nnb_cus\") \nThis function load all rows",
    "crumbs": [
      "R snippet",
      "database"
    ]
  },
  {
    "objectID": "r-snippet/treemap.html",
    "href": "r-snippet/treemap.html",
    "title": "tree map",
    "section": "",
    "text": "library(ggplot2)\nlibrary(treemapify)\n\nif(!require(treemapify)) install.packages(\"treemapify\")\n\ndata(G20)\n\nggplot(G20, aes(area = gdp_mil_usd, fill = hdi)) +\n  geom_treemap()\n\nggplot(G20, aes(area = gdp_mil_usd, fill = hdi, label = country)) +\n  geom_treemap() +\n  geom_treemap_text(fontface = \"italic\", colour = \"white\", place = \"centre\",\n                    grow = TRUE)\n\n\nggplot(G20, aes(area = gdp_mil_usd, fill = hdi, label = country,\n                subgroup = region)) +\n  geom_treemap() +\n  geom_treemap_subgroup_border() +\n  geom_treemap_subgroup_text(place = \"centre\", grow = T, alpha = 0.5, colour =\n                               \"black\", fontface = \"italic\", min.size = 0) +\n  geom_treemap_text(colour = \"white\", place = \"topleft\", reflow = T)\n\nggplot(G20, aes(area = gdp_mil_usd, fill = region, label = country, subgroup = region)) +\n  geom_treemap() +\n  geom_treemap_text(grow = T, reflow = T, colour = \"black\") +\n  facet_wrap( ~ hemisphere) +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(legend.position = \"bottom\") +\n  labs(\n    title = \"The G-20 major economies by hemisphere\",\n    caption = \"The area of each tile represents the country's GDP as a\n    proportion of all countries in that hemisphere\",\n    fill = \"Region\"\n  )",
    "crumbs": [
      "R snippet",
      "tree map"
    ]
  },
  {
    "objectID": "r-snippet/ggplot2.html",
    "href": "r-snippet/ggplot2.html",
    "title": "ggplot2",
    "section": "",
    "text": "Pie chart\nPie chart with other approach this by defining another variable (call pos) in df that calculates the position of text labels.\n# https://stackoverflow.com/questions/24803460/r-ggplot2-add-labels-on-facet-pie-chart\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndf &lt;- df %&gt;% group_by(year) %&gt;% mutate(pos = cumsum(quantity)- quantity/2)\n\nggplot(data=df, aes(x=factor(1), y=quantity, fill=factor(prod))) +\n  geom_bar(stat=\"identity\") +\n  geom_text(aes(x= factor(1), y=pos, label = quantity), size=10) +  # note y = pos\n  facet_grid(facets = .~year, labeller = label_value) +\n  coord_polar(theta = \"y\")\n\n\nDonut chart\ncount.data &lt;- data.frame(\n  class = c(\"1st\", \"2nd\", \"3rd\", \"Crew\"),\n  n = c(325, 285, 706, 885),\n  prop = c(14.8, 12.9, 32.1, 40.2)\n)\ncount.data\n\n# Add label position\ncount.data &lt;- count.data %&gt;%\n  arrange(desc(class)) %&gt;%\n  mutate(lab.ypos = cumsum(prop) - 0.5*prop)\ncount.data\n\nmycols &lt;- c(\"#0073C2FF\", \"#EFC000FF\", \"#868686FF\", \"#CD534CFF\")\n\nggplot(count.data, aes(x = \"\", y = prop, fill = class)) +\n  geom_bar(width = 1, stat = \"identity\", color = \"white\") +\n  coord_polar(\"y\", start = 0)+\n  geom_text(aes(y = lab.ypos, label = prop), color = \"white\")+\n  scale_fill_manual(values = mycols) +\n  theme_void()\n\n# Donut chart\nggplot(count.data, aes(x = 2, y = prop, fill = class)) +\n  geom_bar(stat = \"identity\", color = \"white\") +\n  coord_polar(theta = \"y\", start = 0)+\n  geom_text(aes(y = lab.ypos, label = prop), color = \"white\")+\n  scale_fill_manual(values = mycols) +\n  theme_void()+\n  xlim(0.5, 2.5)\n\ngganimate .gif\nhttps://github.com/thomasp85/gganimate\n\n\nhrbrtheme\n\nformat date\n\nscale_y_continuous(label=scales::comma)+\n  scale_x_date(date_breaks = \"2 month\",\n               date_labels = \"%m-%Y\")+\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\",\n        axis.text.x = element_text(angle = 30))\n\nhistogram\n\nggplot(aes(x = cnt))+\n  geom_histogram(bins = 18)+ # bins = 24\n  labs(title = \"Tong thoi gian\",\n       subtitle = \"don vi\",\n       x = \"So luong\",\n       y = \"So quan sat\")+\n  scale_y_comma()+\n  theme_ipsum_rc()+\n  facet_wrap(~ bi_card_group, nrow = 2)\n\ntheme\n\ntheme(axis.text.x = element_text(size = 14), axis.title.x = element_text(size = 16),\n      axis.text.y = element_text(size = 14), axis.title.y = element_text(size = 16),\n      plot.title = element_text(size = 20, face = \"bold\", color = \"darkgreen\"))\n\n\nvietnamese text\nplot(1:4,rep(1,4),pch=c(\"\\u0111\",\"\\u01B0\",\"\\u01A1\",\"\\u0103\"),cex=4)\n\n\nArea plot\nmu &lt;- wdata %&gt;%\n  group_by(sex) %&gt;%\n  summarise(grp.mean = mean(weight))\n\n# Area plot\na &lt;- ggplot(economics, aes(x=date)) +\n  geom_area(aes(y=psavert), fill = \"#999999\",\n            color = \"#999999\", alpha=0.5) +\n  geom_area(aes(y=uempmed), fill = \"#E69F00\",\n            color = \"#E69F00\", alpha=0.5) +\n  theme_minimal()\n\na + geom_density(aes(color = sex), alpha=0.4)+\n  geom_vline(data = mu, aes(xintercept = grp.mean, color=sex),\n             linetype=\"dashed\")\n\n\nDensity\na + geom_density(aes(color = sex), alpha=0.4)+\n  geom_vline(data = mu, aes(xintercept = grp.mean, color=sex),\n             linetype=\"dashed\")\n\n\nHistogram\n# Basic plot\na + geom_histogram()\n# Change the number of bins\na + geom_histogram(bins = 50)\n# Position adjustment: \"identity\" (overlaid)\na + geom_histogram(aes(color = sex), fill = \"white\", alpha = 0.6,\n                   position=\"identity\")\n\n\nHistogram with density plot\n# Color by groups\na + geom_histogram(aes(y=..density.., color = sex, fill = sex),\n                   alpha=0.5, position=\"identity\")+\n  geom_density(aes(color = sex), size = 1)\n\n\nBox plot with mean points\ne + geom_boxplot() +\n  stat_summary(fun.y = mean, geom = \"point\",\n               shape = 18, size = 4, color = \"blue\")\n\n\nChange the default order of items\ne + geom_boxplot() +\n  scale_x_discrete(limits=c(\"2\", \"0.5\", \"1\"))\n\n\nAdd mean points +/- SD\n# Use geom = \"pointrange\" or geom = \"crossbar\"\ne + geom_violin(trim = FALSE) +\n  stat_summary(fun.data=\"mean_sdl\", fun.args = list(mult=1),\n               geom=\"pointrange\", color = \"red\")\n# Combine with box plot to add median and quartiles\ne + geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.2)\n\n\ngeom_line\n# Change line types, point shapes and colors\np + geom_line(aes(linetype=supp, color = supp))+\n  geom_point(aes(shape=supp, color = supp))\n\n\ngeom_bar with labels\nrequire(plyr)\n# Sort by dose and supp\ndf_sorted &lt;- arrange(df2, dose, supp)\n\n# Calculate the cumulative sum of len for each dose\ndf_cumsum &lt;- ddply(df_sorted, \"dose\", transform,\n                   label_ypos=cumsum(len))\n\n# Create the bar plot\nggplot(data=df_cumsum, aes(x = dose, y = len, fill = supp)) +\n  geom_bar(stat = \"identity\")+\n  geom_text(aes(y = label_ypos, label = len), vjust=1.6,\n            color = \"white\", size = 3.5)\n\n\nPie charts\n# Basic pie charts\ndf &lt;- data.frame(\n  group = c(\"Male\", \"Female\", \"Child\"),\n  value = c(25, 25, 50))\n\np &lt;- ggplot(df, aes(x=\"\", y = value, fill=group)) +\n  geom_bar(width = 1, stat = \"identity\") +\n  coord_polar(\"y\", start=0)\n\nblank_theme &lt;- theme_minimal()+\n  theme(\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.x=element_blank(),\n    panel.border = element_blank(),\n    panel.grid=element_blank(),\n    axis.ticks = element_blank(),\n    plot.title=element_text(size=14, face=\"bold\")\n  )\n\nrequire(scales)\np + scale_fill_brewer(\"Blues\") + blank_theme +\n  geom_text(aes(y = value/3 + c(0, cumsum(value)[-length(value)]),\n                label = percent(value/100)), size=5)",
    "crumbs": [
      "R snippet",
      "ggplot2"
    ]
  },
  {
    "objectID": "r-snippet/index.html",
    "href": "r-snippet/index.html",
    "title": "R snippet",
    "section": "",
    "text": "install.packages(\"spAddins\")\ninstall.packages(\"remedy\")",
    "crumbs": [
      "R snippet"
    ]
  },
  {
    "objectID": "r-snippet/index.html#awesome-ggplot2",
    "href": "r-snippet/index.html#awesome-ggplot2",
    "title": "R snippet",
    "section": "Awesome ggplot2",
    "text": "Awesome ggplot2\n\nThemes and aesthetics\n\ngghighlight{Eg: gghighlight(max(value) &gt; 19)}\nggsci {Eg: scale_color_palname(), scale_fill_palname()}\nggfittext {Eg. geom_fit_text(reflow = TRUE, grow = TRUE)}\nhrbrtheme {Eg: scale_y_comma(), theme_ipsum_rc()}\nbbplot {https://bbc.github.io/rcookbook/, Eg: bbc_style(), finalise_plot()}\nggthemr {https://github.com/cttobin/ggthemr}\n\n\n\nPresentation, composition and scales\n\ntagger",
    "crumbs": [
      "R snippet"
    ]
  },
  {
    "objectID": "r-snippet/index.html#format-axis",
    "href": "r-snippet/index.html#format-axis",
    "title": "R snippet",
    "section": "format axis",
    "text": "format axis\nscale_y_continuous(label=scales::comma)\n\nscale_x_date(date_breaks = \"2 month\", date_labels = \"%m-%Y\")\n\ntheme(legend.title = element_blank(),\n        legend.position = \"bottom\",\n        axis.text.x = element_text(angle = 30))",
    "crumbs": [
      "R snippet"
    ]
  },
  {
    "objectID": "r-snippet/index.html#format-axis-text",
    "href": "r-snippet/index.html#format-axis-text",
    "title": "R snippet",
    "section": "format axis text",
    "text": "format axis text\ntheme(axis.text.x = element_text(size = 14), axis.title.x = element_text(size = 16),\n      axis.text.y = element_text(size = 14), axis.title.y = element_text(size = 16),\n      plot.title = element_text(size = 20, face = \"bold\", color = \"darkgreen\"))\n      \ntheme(axis.ticks.length.y = unit(nc * 0.15,\"cm\"))",
    "crumbs": [
      "R snippet"
    ]
  },
  {
    "objectID": "r-snippet/index.html#geom_line-with-x-is-factor",
    "href": "r-snippet/index.html#geom_line-with-x-is-factor",
    "title": "R snippet",
    "section": "geom_line with x is factor",
    "text": "geom_line with x is factor\ngeom_line(aes(group = 1))",
    "crumbs": [
      "R snippet"
    ]
  },
  {
    "objectID": "r-snippet/index.html#combine-graphs",
    "href": "r-snippet/index.html#combine-graphs",
    "title": "R snippet",
    "section": "combine graphs",
    "text": "combine graphs\nhttps://cran.r-project.org/web/packages/patchwork/vignettes/patchwork.html",
    "crumbs": [
      "R snippet"
    ]
  },
  {
    "objectID": "r-snippet/index.html#vietnamese-font",
    "href": "r-snippet/index.html#vietnamese-font",
    "title": "R snippet",
    "section": "vietnamese font",
    "text": "vietnamese font\nSys.setlocale(category = \"LC_ALL\", locale = \"vietnamese\")\n\neval(parse(\"R/graph_pqr_201911.R\", encoding = \"UTF-8\"))\n\nplot(1:4,rep(1,4), pch=c(\"\\u0111\",\"\\u01B0\",\"\\u01A1\",\"\\u0103\"),cex=4)\n# Uppercase\nplot(1:4,rep(1,4), pch=c(\"\\U0110\",\"\\u01AF\",\"\\u01A0\",\"\\u0102\"),cex=4)",
    "crumbs": [
      "R snippet"
    ]
  },
  {
    "objectID": "r-snippet/index.html#top_n_by-function",
    "href": "r-snippet/index.html#top_n_by-function",
    "title": "R snippet",
    "section": "top_n_by function",
    "text": "top_n_by function\nf_top_n &lt;- function(df, n, top_by){\n  top_by &lt;- enquo(top_by)\n  \n  top_val &lt;- df %&gt;% \n    filter(is.finite(!!top_by)) %&gt;% \n    pull(!!top_by) %&gt;% \n    unique() %&gt;% \n    sort(decreasing = TRUE) %&gt;% \n    head(n) \n  \n  filter(df, !!top_by %in% top_val) %&gt;% return()\n}",
    "crumbs": [
      "R snippet"
    ]
  },
  {
    "objectID": "r-snippet/index.html#bỏ-dấu",
    "href": "r-snippet/index.html#bỏ-dấu",
    "title": "R snippet",
    "section": "Bỏ dấu",
    "text": "Bỏ dấu\nstringi::stri_trans_general('Nguyễn Ngọc Bình', \"latin-ascii\" )",
    "crumbs": [
      "R snippet"
    ]
  },
  {
    "objectID": "r-snippet/index.html#compose",
    "href": "r-snippet/index.html#compose",
    "title": "R snippet",
    "section": "compose",
    "text": "compose\ntidy_lm &lt;- compose(tidy, lm)\ntidy_lm(Sepal.Length ~ Species, data = iris)",
    "crumbs": [
      "R snippet"
    ]
  },
  {
    "objectID": "r-snippet/index.html#partial",
    "href": "r-snippet/index.html#partial",
    "title": "R snippet",
    "section": "partial",
    "text": "partial\nmean_na_rm &lt;- partial(mean, na.rm = TRUE)",
    "crumbs": [
      "R snippet"
    ]
  },
  {
    "objectID": "r-snippet/index.html#reduce",
    "href": "r-snippet/index.html#reduce",
    "title": "R snippet",
    "section": "reduce",
    "text": "reduce\ndfs &lt;- list(\n  age = tibble(name = \"John\", age = 30),\n  sex = tibble(name = c(\"John\", \"Mary\"), sex = c(\"M\", \"F\")),\n  trt = tibble(name = \"Mary\", treatment = \"A\")\n)\n\ndfs %&gt;% reduce(full_join)",
    "crumbs": [
      "R snippet"
    ]
  },
  {
    "objectID": "r-snippet/index.html#rpub",
    "href": "r-snippet/index.html#rpub",
    "title": "R snippet",
    "section": "rpub",
    "text": "rpub\n---\ntitle: \"Correlation analysis\"\nauthor: \"Nguyễn Ngọc Bình\"\ndate: \"`r format(Sys.Date(),'%Y-%m-%d')`\"\noutput:\n  html_document:\n    code_download: true\n    code_folding: show\n    number_sections: yes\n    theme: \"default\"\n    toc: TRUE\n    toc_float: TRUE\n    dev: 'svg'\neditor_options:\n  chunk_output_type: console\n---",
    "crumbs": [
      "R snippet"
    ]
  },
  {
    "objectID": "r-snippet/index.html#image",
    "href": "r-snippet/index.html#image",
    "title": "R snippet",
    "section": "image",
    "text": "image\n![](../figures/d_i_d_graph.png)\nbookdown::html_document2, bookdown::word_document2\n![(\\#fig:nnet2)Một mạng nơ-ron với bốn đầu vào và một lớp ẩn với ba nơ-ron ẩn.](images/nnet2.png)\n\\@ref(fig:nnet2)\nor\nknitr::opts_chunk$set(echo = FALSE, fig.height = 5, fig.width = 7, out.width = \"70%\")\nknitr::include_graphics(\"figures/d_i_d_graph.png\")\n\nFormat number in rmarkdown\nfnc_kbl &lt;- function(df) {\n    df %&gt;%\n    mutate_if(\n      is.numeric,\n      format,\n      digits = 2,\n      nsmall = 2,\n      big.mark = \".\",\n      decimal.mark = \",\"\n    ) %&gt;%\n    kbl() %&gt;%\n    kable_classic(full_width = F)\n}\n\n\nFormat table in word\nfnc_print_tbl_df &lt;- function(tbl_name) {\n  out &lt;- tbl_name %&gt;%\n    ungroup() %&gt;% \n    mutate_if(is.numeric, round, 2) %&gt;%\n    flextable() %&gt;%\n    autofit() %&gt;% \n    theme_zebra(odd_header = '#8064A2') %&gt;% \n    font(fontname = 'Tahoma', part = 'all') %&gt;% \n    fontsize(size = 10, part = 'all')%&gt;% \n    border(border = officer::fp_border(color = \"#8064A2\")) \n  return(out)\n}\n\n\nimport data\nrio::import(xml2::xml2_example(\"cd_catalog.xml\"))\n\n\nReticulate\nfile.edit(file.path(\"~\", \".Rprofile\"))\n# RETICULATE_PYTHON=\"C/Users/nguye/anaconda3\"",
    "crumbs": [
      "R snippet"
    ]
  },
  {
    "objectID": "comparision/Index.html",
    "href": "comparision/Index.html",
    "title": "Comparision",
    "section": "",
    "text": "dplyr vs. pandas\nggplot2 vs. matplotlib\nPl/sql vs. db2\nDb2 vs. mssql",
    "crumbs": [
      "Comparision"
    ]
  },
  {
    "objectID": "comparision/ggplot2-matplotlib.html",
    "href": "comparision/ggplot2-matplotlib.html",
    "title": "ggplot2 vs. matplotlib",
    "section": "",
    "text": "Below is a markdown table comparing the features of ggplot2 (R) and matplotlib (Python) for creating visualizations:\n\nPlot basics\n\n\n\n\n\n\n\n\nggplot2 (R)\nFeature\nmatplotlib (Python)\n\n\n\n\naes()\nUsed inside ggplot() to map variables to aesthetics\nNot used; column names are specified directly in plot()\n\n\n+() %&gt;%\nUsed to add layers and modify the plot\nNot applicable\n\n\nggsave()\nSave the plot to a file\nNot applicable; plots are saved using plt.savefig() in Matplotlib\n\n\nquickplot()\nSimple, intuitive function for quick plots\nNot available\n\n\n\nLayers\n\n\n\n\n\n\n\n\nggplot2 (R)\nFeature\nmatplotlib (Python)\n\n\n\n\ngeom_abline()\nAdd an arbitrary line to the plot\naxhline() and axvline() in Matplotlib\n\n\ngeom_hline()\nAdd horizontal lines to the plot\naxhline() in Matplotlib\n\n\ngeom_vline()\nAdd vertical lines to the plot\naxvline() in Matplotlib\n\n\ngeom_bar()\nCreate bar charts\nkind='bar' in plot()\n\n\ngeom_col()\nCreate column charts\nkind='bar' with position='identity' in plot()\n\n\nstat_count()\nCreate bar charts with automatic counting\nkind='bar' with position='identity' in plot()\n\n\ngeom_boxplot()\nCreate boxplots\nkind='box' in plot()\n\n\nstat_boxplot()\nCreate boxplots with statistical summaries\nkind='box' in plot()\n\n\ngeom_map()\nPlot spatial data on maps\nNot available\n\n\ngeom_point()\nCreate scatter plots\nkind='scatter' in plot()\n\n\ngeom_label()\nAdd text labels to points\nNot available\n\n\ngeom_text()\nAdd text annotations to the plot\ntext() in Matplotlib\n\n\ngeom_violin()\nCreate violin plots\nNot available\n\n\nstat_ydensity()\nCompute density for violin plots\nNot available\n\n\n\nPosition adjustment\n\n\n\n\n\n\n\n\nggplot2 (R)\nFeature\nmatplotlib (Python)\n\n\n\n\nposition_dodge()\nDodge overlapping elements\nNot available\n\n\n\nAnnotations\n\n\n\n\n\n\n\n\nggplot2 (R)\nFeature\nmatplotlib (Python)\n\n\n\n\nannotate()\nAdd annotations to the plot\nNot available\n\n\n\nScales\n\n\n\n\n\n\n\n\nggplot2 (R)\nFeature\nmatplotlib (Python)\n\n\n\n\nlabs()\nModify plot labels and titles\nNot available\n\n\nxlab()\nModify the x-axis label\nset_xlabel() in Matplotlib\n\n\nylab()\nModify the y-axis label\nset_ylabel() in Matplotlib\n\n\nggtitle()\nAdd a plot title\nset_title() in Matplotlib\n\n\nlims()\nSet plot limits\nset_xlim() and set_ylim() in Matplotlib\n\n\nxlim()\nSet x-axis limits\nset_xlim() in Matplotlib\n\n\nylim()\nSet y-axis limits\nset_ylim() in Matplotlib\n\n\nscale_x_continuous()\nModify x-axis scales\nset_xscale() in Matplotlib\n\n\nscale_y_continuous()\nModify y-axis scales\nset_yscale() in Matplotlib\n\n\nscale_x_date()\nModify x-axis scales for date data\nNot available\n\n\nscale_y_date()\nModify y-axis scales for date data\nNot available\n\n\nscale_x_discrete()\nModify x-axis scales for discrete data\nNot available\n\n\nscale_y_discrete()\nModify y-axis scales for discrete data\nNot available\n\n\n\nFacetting\n\n\n\n\n\n\n\n\nggplot2 (R)\nFeature\nmatplotlib (Python)\n\n\n\n\nfacet_wrap()\nCreate small multiples in a wrap layout\nsubplots=True with multiple plots in Pandas\n\n\nfacet_grid()\nCreate small multiples in a grid layout\nsubplots=True with multiple plots in Pandas\n\n\ncoord_flip()\nFlip the x and y-axis\nNot available\n\n\n\nThemes\n\n\n\n\n\n\n\n\nggplot2 (R)\nFeature\nmatplotlib (Python)\n\n\n\n\nelement_blank()\nRemove an element from the plot\nNot available\n\n\nelement_rect()\nModify rectangle elements in the plot\nNot available\n\n\nelement_line()\nModify line elements in the plot\nNot available\n\n\nelement_text()\nModify text elements in the plot\nNot available\n\n\n\nautoplot\n\n\n\n\n\n\n\n\nggplot2 (R)\nFeature\nmatplotlib (Python)\n\n\n\n\nautoplot()\nCreate basic plots automatically\nNot available\n\n\n\n\nPlease note that ggplot2 and matplotlib are both powerful visualization libraries, but they have different philosophies and strengths. The syntax for creating visualizations can differ significantly between the two libraries.",
    "crumbs": [
      "Comparision",
      "`ggplot2` vs. `matplotlib`"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/02-NumPy-Operations.html",
    "href": "Crash-Course-Numpy/02-NumPy-Operations.html",
    "title": "NumPy Operations",
    "section": "",
    "text": "You can easily perform array with array arithmetic, or scalar with array arithmetic. Let’s see some examples:\n\nimport numpy as np\narr = np.arange(0,10)\narr\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\narr + arr\n\narray([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])\n\n\n\narr * arr\n\narray([ 0,  1,  4,  9, 16, 25, 36, 49, 64, 81])\n\n\n\narr - arr\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\n# This will raise a Warning on division by zero, but not an error!\n# It just fills the spot with nan\narr/arr\n\nC:\\Anaconda3\\envs\\tsa_course\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n  This is separate from the ipykernel package so we can avoid doing imports until\n\n\narray([nan,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])\n\n\n\n# Also a warning (but not an error) relating to infinity\n1/arr\n\nC:\\Anaconda3\\envs\\tsa_course\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in true_divide\n  \n\n\narray([       inf, 1.        , 0.5       , 0.33333333, 0.25      ,\n       0.2       , 0.16666667, 0.14285714, 0.125     , 0.11111111])\n\n\n\narr**3\n\narray([  0,   1,   8,  27,  64, 125, 216, 343, 512, 729], dtype=int32)\n\n\n\n\n\nNumPy comes with many universal array functions, or ufuncs, which are essentially just mathematical operations that can be applied across the array.Let’s show some common ones:\n\n# Taking Square Roots\nnp.sqrt(arr)\n\narray([0.        , 1.        , 1.41421356, 1.73205081, 2.        ,\n       2.23606798, 2.44948974, 2.64575131, 2.82842712, 3.        ])\n\n\n\n# Calculating exponential (e^)\nnp.exp(arr)\n\narray([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01,\n       5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03,\n       2.98095799e+03, 8.10308393e+03])\n\n\n\n# Trigonometric Functions like sine\nnp.sin(arr)\n\narray([ 0.        ,  0.84147098,  0.90929743,  0.14112001, -0.7568025 ,\n       -0.95892427, -0.2794155 ,  0.6569866 ,  0.98935825,  0.41211849])\n\n\n\n# Taking the Natural Logarithm\nnp.log(arr)\n\nC:\\Anaconda3\\envs\\tsa_course\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n  \n\n\narray([      -inf, 0.        , 0.69314718, 1.09861229, 1.38629436,\n       1.60943791, 1.79175947, 1.94591015, 2.07944154, 2.19722458])\n\n\n\n\n\nNumPy also offers common summary statistics like sum, mean and max. You would call these as methods on an array.\n\narr = np.arange(0,10)\narr\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\narr.sum()\n\n45\n\n\n\narr.mean()\n\n4.5\n\n\n\narr.max()\n\n9\n\n\nOther summary statistics include:\narr.min() returns 0                   minimum\narr.var() returns 8.25                variance\narr.std() returns 2.8722813232690143  standard deviation\n\n\n\n\nWhen working with 2-dimensional arrays (matrices) we have to consider rows and columns. This becomes very important when we get to the section on pandas. In array terms, axis 0 (zero) is the vertical axis (rows), and axis 1 is the horizonal axis (columns). These values (0,1) correspond to the order in which arr.shape values are returned.\nLet’s see how this affects our summary statistic calculations from above.\n\narr_2d = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])\narr_2d\n\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n\n\n\narr_2d.sum(axis=0)\n\narray([15, 18, 21, 24])\n\n\nBy passing in axis=0, we’re returning an array of sums along the vertical axis, essentially [(1+5+9), (2+6+10), (3+7+11), (4+8+12)]\n\n\narr_2d.shape\n\n(3, 4)\n\n\nThis tells us that arr_2d has 3 rows and 4 columns.\nIn arr_2d.sum(axis=0) above, the first element in each row was summed, then the second element, and so forth.\nSo what should arr_2d.sum(axis=1) return?\n\n# THINK ABOUT WHAT THIS WILL RETURN BEFORE RUNNING THE CELL!\narr_2d.sum(axis=1)",
    "crumbs": [
      "NumPy",
      "NumPy Operations"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/02-NumPy-Operations.html#arithmetic",
    "href": "Crash-Course-Numpy/02-NumPy-Operations.html#arithmetic",
    "title": "NumPy Operations",
    "section": "",
    "text": "You can easily perform array with array arithmetic, or scalar with array arithmetic. Let’s see some examples:\n\nimport numpy as np\narr = np.arange(0,10)\narr\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\narr + arr\n\narray([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])\n\n\n\narr * arr\n\narray([ 0,  1,  4,  9, 16, 25, 36, 49, 64, 81])\n\n\n\narr - arr\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\n# This will raise a Warning on division by zero, but not an error!\n# It just fills the spot with nan\narr/arr\n\nC:\\Anaconda3\\envs\\tsa_course\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n  This is separate from the ipykernel package so we can avoid doing imports until\n\n\narray([nan,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])\n\n\n\n# Also a warning (but not an error) relating to infinity\n1/arr\n\nC:\\Anaconda3\\envs\\tsa_course\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in true_divide\n  \n\n\narray([       inf, 1.        , 0.5       , 0.33333333, 0.25      ,\n       0.2       , 0.16666667, 0.14285714, 0.125     , 0.11111111])\n\n\n\narr**3\n\narray([  0,   1,   8,  27,  64, 125, 216, 343, 512, 729], dtype=int32)",
    "crumbs": [
      "NumPy",
      "NumPy Operations"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/02-NumPy-Operations.html#universal-array-functions",
    "href": "Crash-Course-Numpy/02-NumPy-Operations.html#universal-array-functions",
    "title": "NumPy Operations",
    "section": "",
    "text": "NumPy comes with many universal array functions, or ufuncs, which are essentially just mathematical operations that can be applied across the array.Let’s show some common ones:\n\n# Taking Square Roots\nnp.sqrt(arr)\n\narray([0.        , 1.        , 1.41421356, 1.73205081, 2.        ,\n       2.23606798, 2.44948974, 2.64575131, 2.82842712, 3.        ])\n\n\n\n# Calculating exponential (e^)\nnp.exp(arr)\n\narray([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01,\n       5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03,\n       2.98095799e+03, 8.10308393e+03])\n\n\n\n# Trigonometric Functions like sine\nnp.sin(arr)\n\narray([ 0.        ,  0.84147098,  0.90929743,  0.14112001, -0.7568025 ,\n       -0.95892427, -0.2794155 ,  0.6569866 ,  0.98935825,  0.41211849])\n\n\n\n# Taking the Natural Logarithm\nnp.log(arr)\n\nC:\\Anaconda3\\envs\\tsa_course\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n  \n\n\narray([      -inf, 0.        , 0.69314718, 1.09861229, 1.38629436,\n       1.60943791, 1.79175947, 1.94591015, 2.07944154, 2.19722458])",
    "crumbs": [
      "NumPy",
      "NumPy Operations"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/02-NumPy-Operations.html#summary-statistics-on-arrays",
    "href": "Crash-Course-Numpy/02-NumPy-Operations.html#summary-statistics-on-arrays",
    "title": "NumPy Operations",
    "section": "",
    "text": "NumPy also offers common summary statistics like sum, mean and max. You would call these as methods on an array.\n\narr = np.arange(0,10)\narr\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\narr.sum()\n\n45\n\n\n\narr.mean()\n\n4.5\n\n\n\narr.max()\n\n9\n\n\nOther summary statistics include:\narr.min() returns 0                   minimum\narr.var() returns 8.25                variance\narr.std() returns 2.8722813232690143  standard deviation",
    "crumbs": [
      "NumPy",
      "NumPy Operations"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/02-NumPy-Operations.html#axis-logic",
    "href": "Crash-Course-Numpy/02-NumPy-Operations.html#axis-logic",
    "title": "NumPy Operations",
    "section": "",
    "text": "When working with 2-dimensional arrays (matrices) we have to consider rows and columns. This becomes very important when we get to the section on pandas. In array terms, axis 0 (zero) is the vertical axis (rows), and axis 1 is the horizonal axis (columns). These values (0,1) correspond to the order in which arr.shape values are returned.\nLet’s see how this affects our summary statistic calculations from above.\n\narr_2d = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])\narr_2d\n\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n\n\n\narr_2d.sum(axis=0)\n\narray([15, 18, 21, 24])\n\n\nBy passing in axis=0, we’re returning an array of sums along the vertical axis, essentially [(1+5+9), (2+6+10), (3+7+11), (4+8+12)]\n\n\narr_2d.shape\n\n(3, 4)\n\n\nThis tells us that arr_2d has 3 rows and 4 columns.\nIn arr_2d.sum(axis=0) above, the first element in each row was summed, then the second element, and so forth.\nSo what should arr_2d.sum(axis=1) return?\n\n# THINK ABOUT WHAT THIS WILL RETURN BEFORE RUNNING THE CELL!\narr_2d.sum(axis=1)",
    "crumbs": [
      "NumPy",
      "NumPy Operations"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/00-NumPy-Arrays.html",
    "href": "Crash-Course-Numpy/00-NumPy-Arrays.html",
    "title": "NumPy",
    "section": "",
    "text": "NumPy is a powerful linear algebra library for Python. What makes it so important is that almost all of the libraries in the PyData ecosystem (pandas, scipy, scikit-learn, etc.) rely on NumPy as one of their main building blocks. Plus we will use it to generate data for our analysis examples later on!\nNumPy is also incredibly fast, as it has bindings to C libraries. For more info on why you would want to use arrays instead of lists, check out this great StackOverflow post.\nWe will only learn the basics of NumPy. To get started we need to install it!\n\n\n\n\n\n\n\nIt is highly recommended you install Python using the Anaconda distribution to make sure all underlying dependencies (such as Linear Algebra libraries) all sync up with the use of a conda install. If you have Anaconda, install NumPy by going to your terminal or command prompt and typing:\npip install numpy\n\n\n\n\n\n\nOnce you’ve installed NumPy you can import it as a library:\n\nimport numpy as np\n\nNumPy has many built-in functions and capabilities. We won’t cover them all but instead we will focus on some of the most important aspects of NumPy: vectors, arrays, matrices and number generation. Let’s start by discussing arrays.",
    "crumbs": [
      "NumPy"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/00-NumPy-Arrays.html#installation-instructions",
    "href": "Crash-Course-Numpy/00-NumPy-Arrays.html#installation-instructions",
    "title": "NumPy",
    "section": "",
    "text": "It is highly recommended you install Python using the Anaconda distribution to make sure all underlying dependencies (such as Linear Algebra libraries) all sync up with the use of a conda install. If you have Anaconda, install NumPy by going to your terminal or command prompt and typing:\npip install numpy",
    "crumbs": [
      "NumPy"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/00-NumPy-Arrays.html#using-numpy",
    "href": "Crash-Course-Numpy/00-NumPy-Arrays.html#using-numpy",
    "title": "NumPy",
    "section": "",
    "text": "Once you’ve installed NumPy you can import it as a library:\n\nimport numpy as np\n\nNumPy has many built-in functions and capabilities. We won’t cover them all but instead we will focus on some of the most important aspects of NumPy: vectors, arrays, matrices and number generation. Let’s start by discussing arrays.",
    "crumbs": [
      "NumPy"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/00-NumPy-Arrays.html#creating-numpy-arrays",
    "href": "Crash-Course-Numpy/00-NumPy-Arrays.html#creating-numpy-arrays",
    "title": "NumPy",
    "section": "Creating NumPy Arrays",
    "text": "Creating NumPy Arrays\n\nFrom a Python List\nWe can create an array by directly converting a list or list of lists:\n\nmy_list = [1,2,3]\nmy_list\n\n[1, 2, 3]\n\n\n\nnp.array(my_list)\n\narray([1, 2, 3])\n\n\n\nmy_matrix = [[1,2,3],[4,5,6],[7,8,9]]\nmy_matrix\n\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n\n\nnp.array(my_matrix)\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])",
    "crumbs": [
      "NumPy"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/00-NumPy-Arrays.html#built-in-methods",
    "href": "Crash-Course-Numpy/00-NumPy-Arrays.html#built-in-methods",
    "title": "NumPy",
    "section": "Built-in Methods",
    "text": "Built-in Methods\nThere are lots of built-in ways to generate arrays.\n\narange\nReturn evenly spaced values within a given interval. [reference]\n\nnp.arange(0,10)\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\nnp.arange(0,11,2)\n\narray([ 0,  2,  4,  6,  8, 10])\n\n\n\n\nzeros and ones\nGenerate arrays of zeros or ones. [reference]\n\nnp.zeros(3)\n\narray([0., 0., 0.])\n\n\n\nnp.zeros((5,5))\n\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]])\n\n\n\nnp.ones(3)\n\narray([1., 1., 1.])\n\n\n\nnp.ones((3,3))\n\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n\n\n\n\nlinspace\nReturn evenly spaced numbers over a specified interval. [reference]\n\nnp.linspace(0,10,3)\n\narray([ 0.,  5., 10.])\n\n\n\nnp.linspace(0,5,20)\n\narray([0.        , 0.26315789, 0.52631579, 0.78947368, 1.05263158,\n       1.31578947, 1.57894737, 1.84210526, 2.10526316, 2.36842105,\n       2.63157895, 2.89473684, 3.15789474, 3.42105263, 3.68421053,\n       3.94736842, 4.21052632, 4.47368421, 4.73684211, 5.        ])\n\n\nNote that .linspace() includes the stop value. To obtain an array of common fractions, increase the number of items:\n\nnp.linspace(0,5,21)\n\narray([0.  , 0.25, 0.5 , 0.75, 1.  , 1.25, 1.5 , 1.75, 2.  , 2.25, 2.5 ,\n       2.75, 3.  , 3.25, 3.5 , 3.75, 4.  , 4.25, 4.5 , 4.75, 5.  ])\n\n\n\n\neye\nCreates an identity matrix [reference]\n\nnp.eye(4)\n\narray([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])",
    "crumbs": [
      "NumPy"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/00-NumPy-Arrays.html#random",
    "href": "Crash-Course-Numpy/00-NumPy-Arrays.html#random",
    "title": "NumPy",
    "section": "Random",
    "text": "Random\nNumpy also has lots of ways to create random number arrays:\n\nrand\nCreates an array of the given shape and populates it with random samples from a uniform distribution over [0, 1). [reference]\n\nnp.random.rand(2)\n\narray([0.37065108, 0.89813878])\n\n\n\nnp.random.rand(5,5)\n\narray([[0.03932992, 0.80719137, 0.50145497, 0.68816102, 0.1216304 ],\n       [0.44966851, 0.92572848, 0.70802042, 0.10461719, 0.53768331],\n       [0.12201904, 0.5940684 , 0.89979774, 0.3424078 , 0.77421593],\n       [0.53191409, 0.0112285 , 0.3989947 , 0.8946967 , 0.2497392 ],\n       [0.5814085 , 0.37563686, 0.15266028, 0.42948309, 0.26434141]])\n\n\n\n\nrandn\nReturns a sample (or samples) from the “standard normal” distribution [σ = 1]. Unlike rand which is uniform, values closer to zero are more likely to appear. [reference]\n\nnp.random.randn(2)\n\narray([-0.36633217, -1.40298731])\n\n\n\nnp.random.randn(5,5)\n\narray([[-0.45241033,  1.07491082,  1.95698188,  0.40660223, -1.50445807],\n       [ 0.31434506, -2.16912609, -0.51237235,  0.78663583, -0.61824678],\n       [-0.17569928, -2.39139828,  0.30905559,  0.1616695 ,  0.33783857],\n       [-0.2206597 , -0.05768918,  0.74882883, -1.01241629, -1.81729966],\n       [-0.74891671,  0.88934796,  1.32275912, -0.71605188,  0.0450718 ]])\n\n\n\n\nrandint\nReturns random integers from low (inclusive) to high (exclusive). [reference]\n\nnp.random.randint(1,100)\n\n61\n\n\n\nnp.random.randint(1,100,10)\n\narray([39, 50, 72, 18, 27, 59, 15, 97, 11, 14])\n\n\n\n\nseed\nCan be used to set the random state, so that the same “random” results can be reproduced. [reference]\n\nnp.random.seed(42)\nnp.random.rand(4)\n\narray([0.37454012, 0.95071431, 0.73199394, 0.59865848])\n\n\n\nnp.random.seed(42)\nnp.random.rand(4)\n\narray([0.37454012, 0.95071431, 0.73199394, 0.59865848])",
    "crumbs": [
      "NumPy"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/00-NumPy-Arrays.html#array-attributes-and-methods",
    "href": "Crash-Course-Numpy/00-NumPy-Arrays.html#array-attributes-and-methods",
    "title": "NumPy",
    "section": "Array Attributes and Methods",
    "text": "Array Attributes and Methods\nLet’s discuss some useful attributes and methods for an array:\n\narr = np.arange(25)\nranarr = np.random.randint(0,50,10)\n\n\narr\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24])\n\n\n\nranarr\n\narray([38, 18, 22, 10, 10, 23, 35, 39, 23,  2])",
    "crumbs": [
      "NumPy"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/00-NumPy-Arrays.html#reshape",
    "href": "Crash-Course-Numpy/00-NumPy-Arrays.html#reshape",
    "title": "NumPy",
    "section": "Reshape",
    "text": "Reshape\nReturns an array containing the same data with a new shape. [reference]\n\narr.reshape(5,5)\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14],\n       [15, 16, 17, 18, 19],\n       [20, 21, 22, 23, 24]])\n\n\n\nmax, min, argmax, argmin\nThese are useful methods for finding max or min values. Or to find their index locations using argmin or argmax\n\nranarr\n\narray([38, 18, 22, 10, 10, 23, 35, 39, 23,  2])\n\n\n\nranarr.max()\n\n39\n\n\n\nranarr.argmax()\n\n7\n\n\n\nranarr.min()\n\n2\n\n\n\nranarr.argmin()\n\n9",
    "crumbs": [
      "NumPy"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/00-NumPy-Arrays.html#shape",
    "href": "Crash-Course-Numpy/00-NumPy-Arrays.html#shape",
    "title": "NumPy",
    "section": "Shape",
    "text": "Shape\nShape is an attribute that arrays have (not a method): [reference]\n\n# Vector\narr.shape\n\n(25,)\n\n\n\n# Notice the two sets of brackets\narr.reshape(1,25)\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n        16, 17, 18, 19, 20, 21, 22, 23, 24]])\n\n\n\narr.reshape(1,25).shape\n\n(1, 25)\n\n\n\narr.reshape(25,1)\n\narray([[ 0],\n       [ 1],\n       [ 2],\n       [ 3],\n       [ 4],\n       [ 5],\n       [ 6],\n       [ 7],\n       [ 8],\n       [ 9],\n       [10],\n       [11],\n       [12],\n       [13],\n       [14],\n       [15],\n       [16],\n       [17],\n       [18],\n       [19],\n       [20],\n       [21],\n       [22],\n       [23],\n       [24]])\n\n\n\narr.reshape(25,1).shape\n\n(25, 1)\n\n\n\ndtype\nYou can also grab the data type of the object in the array: [reference]\n\narr.dtype\n\ndtype('int32')\n\n\n\narr2 = np.array([1.2, 3.4, 5.6])\narr2.dtype\n\ndtype('float64')",
    "crumbs": [
      "NumPy"
    ]
  },
  {
    "objectID": "sqlserver-snippet/Index.html#schemas",
    "href": "sqlserver-snippet/Index.html#schemas",
    "title": "SQL-Server Snippet",
    "section": "Schemas",
    "text": "Schemas",
    "crumbs": [
      "SQL-Server Snippet"
    ]
  },
  {
    "objectID": "sqlserver-snippet/Index.html#optimize",
    "href": "sqlserver-snippet/Index.html#optimize",
    "title": "SQL-Server Snippet",
    "section": "Optimize",
    "text": "Optimize",
    "crumbs": [
      "SQL-Server Snippet"
    ]
  },
  {
    "objectID": "sqlserver-snippet/Index.html#references",
    "href": "sqlserver-snippet/Index.html#references",
    "title": "SQL-Server Snippet",
    "section": "References",
    "text": "References\n\n[SHORTCUT] (https://manhng.com/blog/sql-query/)\n[SYNONYM] (https://www.sqlservertutorial.net/sql-server-basics/sql-server-synonym/)\n[STORE PROCEDURE] (https://comdy.vn/sql-server/stored-procedure-trong-sql-server/)",
    "crumbs": [
      "SQL-Server Snippet"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/06-Data-Input-and-Output.html",
    "href": "Crash-Course-Pandas/06-Data-Input-and-Output.html",
    "title": "Data Input and Output",
    "section": "",
    "text": "NOTE: Typically we will just be either reading csv files directly or using pandas-datareader to pull data from the web. Consider this lecture just a quick overview of what is possible with pandas (we won’t be working with SQL or Excel files in this course)",
    "crumbs": [
      "Introduction to Pandas",
      "Data Input and Output"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/06-Data-Input-and-Output.html#csv",
    "href": "Crash-Course-Pandas/06-Data-Input-and-Output.html#csv",
    "title": "Data Input and Output",
    "section": "CSV",
    "text": "CSV\nComma Separated Values files are text files that use commas as field delimeters. Unless you’re running the virtual environment included with the course, you may need to install xlrd and openpyxl. In your terminal/command prompt run:\nconda install xlrd\nconda install openpyxl\nThen restart Jupyter Notebook. (or use pip install if you aren’t using the Anaconda Distribution)\n\nCSV Input\n\ndf = pd.read_csv('example.csv')\ndf\n\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n0\n1\n2\n3\n\n\n1\n4\n5\n6\n7\n\n\n2\n8\n9\n10\n11\n\n\n3\n12\n13\n14\n15\n\n\n\n\n\n\n\n\n\n\nCSV Output\n\ndf.to_csv('example.csv',index=False)",
    "crumbs": [
      "Introduction to Pandas",
      "Data Input and Output"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/06-Data-Input-and-Output.html#excel",
    "href": "Crash-Course-Pandas/06-Data-Input-and-Output.html#excel",
    "title": "Data Input and Output",
    "section": "Excel",
    "text": "Excel\nPandas can read and write MS Excel files. However, this only imports data, not formulas or images. A file that contains images or macros may cause the .read_excel()method to crash.\n\nExcel Input\n\npd.read_excel('Excel_Sample.xlsx',sheet_name='Sheet1')\n\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n0\n1\n2\n3\n\n\n1\n4\n5\n6\n7\n\n\n2\n8\n9\n10\n11\n\n\n3\n12\n13\n14\n15\n\n\n\n\n\n\n\n\n\n\nExcel Output\n\ndf.to_excel('Excel_Sample.xlsx',sheet_name='Sheet1')",
    "crumbs": [
      "Introduction to Pandas",
      "Data Input and Output"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/06-Data-Input-and-Output.html#html",
    "href": "Crash-Course-Pandas/06-Data-Input-and-Output.html#html",
    "title": "Data Input and Output",
    "section": "HTML",
    "text": "HTML\nPandas can read table tabs off of HTML. Unless you’re running the virtual environment included with the course, you may need to install lxml, htmllib5, and BeautifulSoup4. In your terminal/command prompt run:\nconda install lxml\nconda install html5lib\nconda install beautifulsoup4\nThen restart Jupyter Notebook. (or use pip install if you aren’t using the Anaconda Distribution)\n\nHTML Input\nPandas read_html function will read tables off of a webpage and return a list of DataFrame objects:\n\ndf = pd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html')\n\n\ndf[0].head()\n\n\n\n\n\n\n\n\n\nBank Name\nCity\nST\nCERT\nAcquiring Institution\nClosing Date\nUpdated Date\n\n\n\n\n0\nWashington Federal Bank for Savings\nChicago\nIL\n30570\nRoyal Savings Bank\nDecember 15, 2017\nFebruary 21, 2018\n\n\n1\nThe Farmers and Merchants State Bank of Argonia\nArgonia\nKS\n17719\nConway Bank\nOctober 13, 2017\nFebruary 21, 2018\n\n\n2\nFayette County Bank\nSaint Elmo\nIL\n1802\nUnited Fidelity Bank, fsb\nMay 26, 2017\nJuly 26, 2017\n\n\n3\nGuaranty Bank, (d/b/a BestBank in Georgia & Mi...\nMilwaukee\nWI\n30003\nFirst-Citizens Bank & Trust Company\nMay 5, 2017\nMarch 22, 2018\n\n\n4\nFirst NBC Bank\nNew Orleans\nLA\n58302\nWhitney Bank\nApril 28, 2017\nDecember 5, 2017",
    "crumbs": [
      "Introduction to Pandas",
      "Data Input and Output"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/02-DataFrames.html",
    "href": "Crash-Course-Pandas/02-DataFrames.html",
    "title": "DataFrames",
    "section": "",
    "text": "DataFrames are the workhorse of pandas and are directly inspired by the R programming language. We can think of a DataFrame as a bunch of Series objects put together to share the same index. Let’s use pandas to explore this topic!\n\nimport pandas as pd\nimport numpy as np\n\n\nfrom numpy.random import randn\nnp.random.seed(101)\n\n\ndf = pd.DataFrame(randn(5,4),index='A B C D E'.split(),columns='W X Y Z'.split())\n\n\ndf\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\n\n\nLet’s learn the various methods to grab data from a DataFrame\n\ndf['W']\n\nA    2.706850\nB    0.651118\nC   -2.018168\nD    0.188695\nE    0.190794\nName: W, dtype: float64\n\n\n\n# Pass a list of column names\ndf[['W','Z']]\n\n\n\n\n\n\n\n\n\nW\nZ\n\n\n\n\nA\n2.706850\n0.503826\n\n\nB\n0.651118\n0.605965\n\n\nC\n-2.018168\n-0.589001\n\n\nD\n0.188695\n0.955057\n\n\nE\n0.190794\n0.683509\n\n\n\n\n\n\n\n\n\n# SQL Syntax (NOT RECOMMENDED!)\ndf.W\n\nA    2.706850\nB    0.651118\nC   -2.018168\nD    0.188695\nE    0.190794\nName: W, dtype: float64\n\n\nDataFrame Columns are just Series\n\ntype(df['W'])\n\npandas.core.series.Series\n\n\n\n\n\ndf['new'] = df['W'] + df['Y']\n\n\ndf\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\nnew\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n3.614819\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n-0.196959\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\n-1.489355\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n-0.744542\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\n2.796762\n\n\n\n\n\n\n\n\n\n\n\n\ndf.drop('new',axis=1)\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\n\n# Not inplace unless specified!\ndf\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\nnew\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n3.614819\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n-0.196959\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\n-1.489355\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n-0.744542\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\n2.796762\n\n\n\n\n\n\n\n\n\ndf.drop('new',axis=1,inplace=True)\n\n\ndf\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\nCan also drop rows this way:\n\ndf.drop('E',axis=0)\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\n\n\n\n\n\n\n\n\n\n\ndf.loc['A']\n\nW    2.706850\nX    0.628133\nY    0.907969\nZ    0.503826\nName: A, dtype: float64\n\n\nOr select based off of position instead of label\n\ndf.iloc[2]\n\nW   -2.018168\nX    0.740122\nY    0.528813\nZ   -0.589001\nName: C, dtype: float64\n\n\n\n\n\n\ndf.loc['B','Y']\n\n-0.8480769834036315\n\n\n\ndf.loc[['A','B'],['W','Y']]\n\n\n\n\n\n\n\n\n\nW\nY\n\n\n\n\nA\n2.706850\n0.907969\n\n\nB\n0.651118\n-0.848077\n\n\n\n\n\n\n\n\n\n\n\nAn important feature of pandas is conditional selection using bracket notation, very similar to numpy:\n\ndf\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\n\ndf&gt;0\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nA\nTrue\nTrue\nTrue\nTrue\n\n\nB\nTrue\nFalse\nFalse\nTrue\n\n\nC\nFalse\nTrue\nTrue\nFalse\n\n\nD\nTrue\nFalse\nFalse\nTrue\n\n\nE\nTrue\nTrue\nTrue\nTrue\n\n\n\n\n\n\n\n\n\ndf[df&gt;0]\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nB\n0.651118\nNaN\nNaN\n0.605965\n\n\nC\nNaN\n0.740122\n0.528813\nNaN\n\n\nD\n0.188695\nNaN\nNaN\n0.955057\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\n\ndf[df['W']&gt;0]\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\n\ndf[df['W']&gt;0]['Y']\n\nA    0.907969\nB   -0.848077\nD   -0.933237\nE    2.605967\nName: Y, dtype: float64\n\n\n\ndf[df['W']&gt;0][['Y','X']]\n\n\n\n\n\n\n\n\n\nY\nX\n\n\n\n\nA\n0.907969\n0.628133\n\n\nB\n-0.848077\n-0.319318\n\n\nD\n-0.933237\n-0.758872\n\n\nE\n2.605967\n1.978757\n\n\n\n\n\n\n\n\nFor two conditions you can use | and & with parenthesis:\n\ndf[(df['W']&gt;0) & (df['Y'] &gt; 1)]\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s discuss some more features of indexing, including resetting the index or setting it something else. We’ll also talk about index hierarchy!\n\ndf\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\n\n# Reset to default 0,1...n index\ndf.reset_index()\n\n\n\n\n\n\n\n\n\nindex\nW\nX\nY\nZ\n\n\n\n\n0\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\n1\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\n2\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\n\n\n3\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\n4\nE\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\n\nnewind = 'CA NY WY OR CO'.split()\n\n\ndf['States'] = newind\n\n\ndf\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\nStates\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\nCA\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\nNY\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\nWY\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\nOR\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\nCO\n\n\n\n\n\n\n\n\n\ndf.set_index('States')\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\nStates\n\n\n\n\n\n\n\n\nCA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nNY\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\nWY\n-2.018168\n0.740122\n0.528813\n-0.589001\n\n\nOR\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\nCO\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\n\ndf\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\nStates\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\nCA\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\nNY\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\nWY\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\nOR\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\nCO\n\n\n\n\n\n\n\n\n\ndf.set_index('States',inplace=True)\n\n\ndf\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\nStates\n\n\n\n\n\n\n\n\nCA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nNY\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\nWY\n-2.018168\n0.740122\n0.528813\n-0.589001\n\n\nOR\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\nCO\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\n\n\n\nThere are a couple of ways to obtain summary data on DataFrames. df.describe() provides summary statistics on all numerical columns. df.info and df.dtypes displays the data type of all columns.\n\ndf.describe()\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\ncount\n5.000000\n5.000000\n5.000000\n5.000000\n\n\nmean\n0.343858\n0.453764\n0.452287\n0.431871\n\n\nstd\n1.681131\n1.061385\n1.454516\n0.594708\n\n\nmin\n-2.018168\n-0.758872\n-0.933237\n-0.589001\n\n\n25%\n0.188695\n-0.319318\n-0.848077\n0.503826\n\n\n50%\n0.190794\n0.628133\n0.528813\n0.605965\n\n\n75%\n0.651118\n0.740122\n0.907969\n0.683509\n\n\nmax\n2.706850\n1.978757\n2.605967\n0.955057\n\n\n\n\n\n\n\n\n\ndf.dtypes\n\nW    float64\nX    float64\nY    float64\nZ    float64\ndtype: object\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 5 entries, CA to CO\nData columns (total 4 columns):\nW    5 non-null float64\nX    5 non-null float64\nY    5 non-null float64\nZ    5 non-null float64\ndtypes: float64(4)\nmemory usage: 200.0+ bytes",
    "crumbs": [
      "Introduction to Pandas",
      "DataFrames"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/02-DataFrames.html#selection-and-indexing",
    "href": "Crash-Course-Pandas/02-DataFrames.html#selection-and-indexing",
    "title": "DataFrames",
    "section": "",
    "text": "Let’s learn the various methods to grab data from a DataFrame\n\ndf['W']\n\nA    2.706850\nB    0.651118\nC   -2.018168\nD    0.188695\nE    0.190794\nName: W, dtype: float64\n\n\n\n# Pass a list of column names\ndf[['W','Z']]\n\n\n\n\n\n\n\n\n\nW\nZ\n\n\n\n\nA\n2.706850\n0.503826\n\n\nB\n0.651118\n0.605965\n\n\nC\n-2.018168\n-0.589001\n\n\nD\n0.188695\n0.955057\n\n\nE\n0.190794\n0.683509\n\n\n\n\n\n\n\n\n\n# SQL Syntax (NOT RECOMMENDED!)\ndf.W\n\nA    2.706850\nB    0.651118\nC   -2.018168\nD    0.188695\nE    0.190794\nName: W, dtype: float64\n\n\nDataFrame Columns are just Series\n\ntype(df['W'])\n\npandas.core.series.Series\n\n\n\n\n\ndf['new'] = df['W'] + df['Y']\n\n\ndf\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\nnew\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n3.614819\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n-0.196959\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\n-1.489355\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n-0.744542\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\n2.796762\n\n\n\n\n\n\n\n\n\n\n\n\ndf.drop('new',axis=1)\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\n\n# Not inplace unless specified!\ndf\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\nnew\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n3.614819\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n-0.196959\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\n-1.489355\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n-0.744542\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\n2.796762\n\n\n\n\n\n\n\n\n\ndf.drop('new',axis=1,inplace=True)\n\n\ndf\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\nCan also drop rows this way:\n\ndf.drop('E',axis=0)\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\n\n\n\n\n\n\n\n\n\n\ndf.loc['A']\n\nW    2.706850\nX    0.628133\nY    0.907969\nZ    0.503826\nName: A, dtype: float64\n\n\nOr select based off of position instead of label\n\ndf.iloc[2]\n\nW   -2.018168\nX    0.740122\nY    0.528813\nZ   -0.589001\nName: C, dtype: float64\n\n\n\n\n\n\ndf.loc['B','Y']\n\n-0.8480769834036315\n\n\n\ndf.loc[['A','B'],['W','Y']]\n\n\n\n\n\n\n\n\n\nW\nY\n\n\n\n\nA\n2.706850\n0.907969\n\n\nB\n0.651118\n-0.848077\n\n\n\n\n\n\n\n\n\n\n\nAn important feature of pandas is conditional selection using bracket notation, very similar to numpy:\n\ndf\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\n\ndf&gt;0\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nA\nTrue\nTrue\nTrue\nTrue\n\n\nB\nTrue\nFalse\nFalse\nTrue\n\n\nC\nFalse\nTrue\nTrue\nFalse\n\n\nD\nTrue\nFalse\nFalse\nTrue\n\n\nE\nTrue\nTrue\nTrue\nTrue\n\n\n\n\n\n\n\n\n\ndf[df&gt;0]\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nB\n0.651118\nNaN\nNaN\n0.605965\n\n\nC\nNaN\n0.740122\n0.528813\nNaN\n\n\nD\n0.188695\nNaN\nNaN\n0.955057\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\n\ndf[df['W']&gt;0]\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\n\ndf[df['W']&gt;0]['Y']\n\nA    0.907969\nB   -0.848077\nD   -0.933237\nE    2.605967\nName: Y, dtype: float64\n\n\n\ndf[df['W']&gt;0][['Y','X']]\n\n\n\n\n\n\n\n\n\nY\nX\n\n\n\n\nA\n0.907969\n0.628133\n\n\nB\n-0.848077\n-0.319318\n\n\nD\n-0.933237\n-0.758872\n\n\nE\n2.605967\n1.978757\n\n\n\n\n\n\n\n\nFor two conditions you can use | and & with parenthesis:\n\ndf[(df['W']&gt;0) & (df['Y'] &gt; 1)]\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509",
    "crumbs": [
      "Introduction to Pandas",
      "DataFrames"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/02-DataFrames.html#more-index-details",
    "href": "Crash-Course-Pandas/02-DataFrames.html#more-index-details",
    "title": "DataFrames",
    "section": "",
    "text": "Let’s discuss some more features of indexing, including resetting the index or setting it something else. We’ll also talk about index hierarchy!\n\ndf\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\n\n# Reset to default 0,1...n index\ndf.reset_index()\n\n\n\n\n\n\n\n\n\nindex\nW\nX\nY\nZ\n\n\n\n\n0\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\n1\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\n2\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\n\n\n3\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\n4\nE\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\n\nnewind = 'CA NY WY OR CO'.split()\n\n\ndf['States'] = newind\n\n\ndf\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\nStates\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\nCA\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\nNY\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\nWY\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\nOR\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\nCO\n\n\n\n\n\n\n\n\n\ndf.set_index('States')\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\nStates\n\n\n\n\n\n\n\n\nCA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nNY\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\nWY\n-2.018168\n0.740122\n0.528813\n-0.589001\n\n\nOR\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\nCO\n0.190794\n1.978757\n2.605967\n0.683509\n\n\n\n\n\n\n\n\n\ndf\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\nStates\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\nCA\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\nNY\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\nWY\n\n\nD\n0.188695\n-0.758872\n-0.933237\n0.955057\nOR\n\n\nE\n0.190794\n1.978757\n2.605967\n0.683509\nCO\n\n\n\n\n\n\n\n\n\ndf.set_index('States',inplace=True)\n\n\ndf\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\nStates\n\n\n\n\n\n\n\n\nCA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nNY\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\nWY\n-2.018168\n0.740122\n0.528813\n-0.589001\n\n\nOR\n0.188695\n-0.758872\n-0.933237\n0.955057\n\n\nCO\n0.190794\n1.978757\n2.605967\n0.683509",
    "crumbs": [
      "Introduction to Pandas",
      "DataFrames"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/02-DataFrames.html#dataframe-summaries",
    "href": "Crash-Course-Pandas/02-DataFrames.html#dataframe-summaries",
    "title": "DataFrames",
    "section": "",
    "text": "There are a couple of ways to obtain summary data on DataFrames. df.describe() provides summary statistics on all numerical columns. df.info and df.dtypes displays the data type of all columns.\n\ndf.describe()\n\n\n\n\n\n\n\n\n\nW\nX\nY\nZ\n\n\n\n\ncount\n5.000000\n5.000000\n5.000000\n5.000000\n\n\nmean\n0.343858\n0.453764\n0.452287\n0.431871\n\n\nstd\n1.681131\n1.061385\n1.454516\n0.594708\n\n\nmin\n-2.018168\n-0.758872\n-0.933237\n-0.589001\n\n\n25%\n0.188695\n-0.319318\n-0.848077\n0.503826\n\n\n50%\n0.190794\n0.628133\n0.528813\n0.605965\n\n\n75%\n0.651118\n0.740122\n0.907969\n0.683509\n\n\nmax\n2.706850\n1.978757\n2.605967\n0.955057\n\n\n\n\n\n\n\n\n\ndf.dtypes\n\nW    float64\nX    float64\nY    float64\nZ    float64\ndtype: object\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 5 entries, CA to CO\nData columns (total 4 columns):\nW    5 non-null float64\nX    5 non-null float64\nY    5 non-null float64\nZ    5 non-null float64\ndtypes: float64(4)\nmemory usage: 200.0+ bytes",
    "crumbs": [
      "Introduction to Pandas",
      "DataFrames"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/05-Operations.html",
    "href": "Crash-Course-Pandas/05-Operations.html",
    "title": "Operations",
    "section": "",
    "text": "Operations\nThere are lots of operations with pandas that will be really useful to you, but don’t fall into any distinct category. Let’s show them here in this lecture:\n\nimport pandas as pd\ndf = pd.DataFrame({'col1':[1,2,3,4],'col2':[444,555,666,444],'col3':['abc','def','ghi','xyz']})\ndf.head()\n\n\n\n\n\n\n\n\ncol1\ncol2\ncol3\n\n\n\n\n0\n1\n444\nabc\n\n\n1\n2\n555\ndef\n\n\n2\n3\n666\nghi\n\n\n3\n4\n444\nxyz\n\n\n\n\n\n\n\n\n\nInfo on Unique Values\n\ndf['col2'].unique()\n\narray([444, 555, 666])\n\n\n\ndf['col2'].nunique()\n\n3\n\n\n\ndf['col2'].value_counts()\n\n444    2\n555    1\n666    1\nName: col2, dtype: int64\n\n\n\n\nSelecting Data\n\n#Select from DataFrame using criteria from multiple columns\nnewdf = df[(df['col1']&gt;2) & (df['col2']==444)]\n\n\nnewdf\n\n\n\n\n\n\n\n\ncol1\ncol2\ncol3\n\n\n\n\n3\n4\n444\nxyz\n\n\n\n\n\n\n\n\n\n\nApplying Functions\n\ndef times2(x):\n    return x*2\n\n\ndf['col1'].apply(times2)\n\n0    2\n1    4\n2    6\n3    8\nName: col1, dtype: int64\n\n\n\ndf['col3'].apply(len)\n\n0    3\n1    3\n2    3\n3    3\nName: col3, dtype: int64\n\n\n\ndf['col1'].sum()\n\n10\n\n\n\n\nPermanently Removing a Column\n\ndel df['col1']\n\n\ndf\n\n\n\n\n\n\n\n\ncol2\ncol3\n\n\n\n\n0\n444\nabc\n\n\n1\n555\ndef\n\n\n2\n666\nghi\n\n\n3\n444\nxyz\n\n\n\n\n\n\n\n\n\n\nGet column and index names:\n\ndf.columns\n\nIndex(['col2', 'col3'], dtype='object')\n\n\n\ndf.index\n\nRangeIndex(start=0, stop=4, step=1)\n\n\n\n\nSorting and Ordering a DataFrame:\n\ndf\n\n\n\n\n\n\n\n\ncol2\ncol3\n\n\n\n\n0\n444\nabc\n\n\n1\n555\ndef\n\n\n2\n666\nghi\n\n\n3\n444\nxyz\n\n\n\n\n\n\n\n\n\ndf.sort_values(by='col2') #inplace=False by default\n\n\n\n\n\n\n\n\ncol2\ncol3\n\n\n\n\n0\n444\nabc\n\n\n3\n444\nxyz\n\n\n1\n555\ndef\n\n\n2\n666\nghi\n\n\n\n\n\n\n\n\n\n\n\nGreat Job!",
    "crumbs": [
      "Introduction to Pandas",
      "Operations"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/00-Intro-to-Pandas.html",
    "href": "Crash-Course-Pandas/00-Intro-to-Pandas.html",
    "title": "Introduction to Pandas",
    "section": "",
    "text": "Introduction to Pandas\nIn this section of the course we will learn how to use pandas for data analysis. You can think of pandas as an extremely powerful version of Excel, with a lot more features. In this section of the course, you should go through the notebooks in this order:\n\nIntroduction to Pandas\nSeries\nDataFrames\nMissing Data\nGroupBy\nMerging, Joining and Concatenating\nOperations\nData Input and Output",
    "crumbs": [
      "Introduction to Pandas"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/04-Groupby.html",
    "href": "Crash-Course-Pandas/04-Groupby.html",
    "title": "Groupby",
    "section": "",
    "text": "Groupby\nThe groupby method allows you to group rows of data together and call aggregate functions\n\nimport pandas as pd\n# Create dataframe\ndata = {'Company':['GOOG','GOOG','MSFT','MSFT','FB','FB'],\n       'Person':['Sam','Charlie','Amy','Vanessa','Carl','Sarah'],\n       'Sales':[200,120,340,124,243,350]}\n\n\ndf = pd.DataFrame(data)\n\n\ndf\n\n\n\n\n\n\n\n\nCompany\nPerson\nSales\n\n\n\n\n0\nGOOG\nSam\n200\n\n\n1\nGOOG\nCharlie\n120\n\n\n2\nMSFT\nAmy\n340\n\n\n3\nMSFT\nVanessa\n124\n\n\n4\nFB\nCarl\n243\n\n\n5\nFB\nSarah\n350\n\n\n\n\n\n\n\n\nNow you can use the .groupby() method to group rows together based off of a column name.For instance let’s group based off of Company. This will create a DataFrameGroupBy object:\n\ndf.groupby('Company')\n\n&lt;pandas.core.groupby.DataFrameGroupBy object at 0x113014128&gt;\n\n\nYou can save this object as a new variable:\n\nby_comp = df.groupby(\"Company\")\n\nAnd then call aggregate methods off the object:\n\nby_comp.mean()\n\n\n\n\n\n\n\n\nSales\n\n\nCompany\n\n\n\n\n\nFB\n296.5\n\n\nGOOG\n160.0\n\n\nMSFT\n232.0\n\n\n\n\n\n\n\n\n\ndf.groupby('Company').mean()\n\n\n\n\n\n\n\n\nSales\n\n\nCompany\n\n\n\n\n\nFB\n296.5\n\n\nGOOG\n160.0\n\n\nMSFT\n232.0\n\n\n\n\n\n\n\n\nMore examples of aggregate methods:\n\nby_comp.std()\n\n\n\n\n\n\n\n\nSales\n\n\nCompany\n\n\n\n\n\nFB\n75.660426\n\n\nGOOG\n56.568542\n\n\nMSFT\n152.735065\n\n\n\n\n\n\n\n\n\nby_comp.min()\n\n\n\n\n\n\n\n\nPerson\nSales\n\n\nCompany\n\n\n\n\n\n\nFB\nCarl\n243\n\n\nGOOG\nCharlie\n120\n\n\nMSFT\nAmy\n124\n\n\n\n\n\n\n\n\n\nby_comp.max()\n\n\n\n\n\n\n\n\nPerson\nSales\n\n\nCompany\n\n\n\n\n\n\nFB\nSarah\n350\n\n\nGOOG\nSam\n200\n\n\nMSFT\nVanessa\n340\n\n\n\n\n\n\n\n\n\nby_comp.count()\n\n\n\n\n\n\n\n\nPerson\nSales\n\n\nCompany\n\n\n\n\n\n\nFB\n2\n2\n\n\nGOOG\n2\n2\n\n\nMSFT\n2\n2\n\n\n\n\n\n\n\n\n\nby_comp.describe()\n\n\n\n\n\n\n\n\n\nSales\n\n\nCompany\n\n\n\n\n\n\nFB\ncount\n2.000000\n\n\nmean\n296.500000\n\n\nstd\n75.660426\n\n\nmin\n243.000000\n\n\n25%\n269.750000\n\n\n50%\n296.500000\n\n\n75%\n323.250000\n\n\nmax\n350.000000\n\n\nGOOG\ncount\n2.000000\n\n\nmean\n160.000000\n\n\nstd\n56.568542\n\n\nmin\n120.000000\n\n\n25%\n140.000000\n\n\n50%\n160.000000\n\n\n75%\n180.000000\n\n\nmax\n200.000000\n\n\nMSFT\ncount\n2.000000\n\n\nmean\n232.000000\n\n\nstd\n152.735065\n\n\nmin\n124.000000\n\n\n25%\n178.000000\n\n\n50%\n232.000000\n\n\n75%\n286.000000\n\n\nmax\n340.000000\n\n\n\n\n\n\n\n\n\nby_comp.describe().transpose()\n\n\n\n\n\n\n\nCompany\nFB\nGOOG\nMSFT\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\ncount\nmean\n...\n75%\nmax\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nSales\n2.0\n296.5\n75.660426\n243.0\n269.75\n296.5\n323.25\n350.0\n2.0\n160.0\n...\n180.0\n200.0\n2.0\n232.0\n152.735065\n124.0\n178.0\n232.0\n286.0\n340.0\n\n\n\n\n1 rows × 24 columns\n\n\n\n\n\nby_comp.describe().transpose()['GOOG']\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nSales\n2.0\n160.0\n56.568542\n120.0\n140.0\n160.0\n180.0\n200.0\n\n\n\n\n\n\n\n\n\n\nGreat Job!",
    "crumbs": [
      "Introduction to Pandas",
      "Groupby"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html",
    "href": "machine-learning/transformer.html",
    "title": "Transformer",
    "section": "",
    "text": "Kiến trúc Transformer là một mô hình học sâu phổ biến được giới thiệu trong bài báo “Attention is All You Need” của Vaswani và đồng nghiệp (2017). Nó đã cách mạng hóa nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) và trở thành cơ sở cho các mô hình tiên tiến như BERT và GPT.\nKiến trúc Transformer dựa trên khái niệm self-attention, cho phép mô hình tập trung vào các phần khác nhau của chuỗi đầu vào khi xử lý từng phần tử. Dưới đây là một cái nhìn tổng quan về kiến trúc Transformer:\n\n\n\nKiến trúc Transformer\n\n\n\nNhúng Đầu Vào (Input Embeddings):\n\nChuỗi đầu vào được nhúng thành các biểu diễn liên tục được gọi là nhúng đầu vào.\nMỗi từ hoặc thành phần trong chuỗi đầu vào được biểu diễn bằng một vector mật độ.\n\nMã Hóa Vị Trí (Positional Encoding):\n\nMã hóa vị trí được thêm vào nhúng đầu vào để cung cấp thông tin về thứ tự hoặc vị trí của các từ trong chuỗi.\nĐiều này cho phép mô hình bắt chước thông tin tuần tự, vì kiến trúc Transformer không có kết nối lặp lại.\n\nBộ Mã Hóa (Encoder):\n\nTransformer bao gồm một cấu trúc mã hóa-giải mã (encoder-decoder), nhưng để đơn giản, chúng ta tập trung vào bộ mã hóa.\nBộ mã hóa bao gồm nhiều lớp giống nhau, mỗi lớp bao gồm hai lớp con:\n\nMulti-head Self-Attention:\n\nSelf-attention cho phép mô hình đánh trọng số sự quan trọng của các từ khác nhau trong chuỗi đầu vào khi xử lý một từ cụ thể.\nNó tính toán tổng trọng số của các nhúng từ tất cả các từ, trong đó trọng số được xác định bởi sự tương đồng giữa các từ.\n\nFeed-Forward Neural Network:\n\nA simple feed-forward neural network được áp dụng cho từng vị trí trong chuỗi độc lập.\nNó giúp bắt chước mối quan hệ phi tuyến giữa các từ trong chuỗi.\n\n\n\nBộ Giải Mã (Decoder):\n\nBộ giải mã tương tự như bộ mã hóa nhưng có cơ chế attention bổ sung.\nNó tạo ra chuỗi đầu ra bằng cách chú ý đến các biểu diễn đầu ra của bộ mã hóa và các từ đã được tạo ra trước đó.\n\nCơ Chế Che (Masking):\n\nTrong quá trình huấn luyện, cơ chế che được sử dụng để ngăn mô hình chú ý đến các vị trí trong tương lai.\nĐiều này đảm bảo rằng mỗi vị trí chỉ có thể chú ý đến các vị trí trước đó trong chuỗi đầu vào.\n\nTạo Ra Đầu Ra:\n\nLớp cuối cùng của bộ giải mã tạo ra xác suất đầu ra cho mỗi vị trí trong chuỗi đầu ra.\nCác xác suất này thường được tính bằng cách sử dụng hàm kích hoạt softmax.\n\n\nKiến trúc Transformer có một số lợi thế, như khả năng song song hóa, khả năng bắt chước các phụ thuộc xa, và khả năng tổng quát tốt hơn nhờ self-attention. Nó đã đạt được kết quả tiên tiến trên nhiều nhiệm vụ NLP, bao gồm dịch máy, tóm tắt văn bản và hiểu ngôn ngữ.\n\n\n\n\n\nSelf-attention, còn được gọi là intra-attention hay scaled dot-product attention (tích vô hướng giữa 2 vector), là một cơ chế tính toán trọng số chú ý cho các phần tử trong cùng một chuỗi đầu vào. Nó cho phép mô hình đánh giá mức độ quan trọng của mỗi phần tử trong chuỗi dựa trên mức liên quan của nó đối với các phần tử khác.\n\n\n\nTrong self-attention, mỗi từ trong chuỗi đầu vào được biểu diễn bằng một vector, thông thường được gọi là nhúng hay vector nhúng (embedding vector). Giả sử nhúng từ cho chuỗi đầu vào như sau:\nVí dụ câu như sau: “Con mèo ngồi trên chiếc thảm.”\nĐể áp dụng self-attention, chúng ta trước tiên biểu diễn mỗi từ trong câu thành một vector nhúng. Hãy giả sử chúng ta có các vector nhúng từ sau đây:\nNhúng từ:\n- \"Con\": [0.2, 0.5, -0.3]\n- \"mèo\": [-0.1, 0.7, 0.2]\n- \"ngồi\": [0.4, -0.2, 0.6]\n- \"trên\": [-0.5, 0.3, -0.1]\n- \"chiếc\": [0.2, 0.5, -0.3]\n- \"thảm\": [0.3, 0.1, 0.8]\n- \".\": [0.0, 0.0, 0.0]\nBây giờ, chúng ta tính toán trọng số self-attention cho mỗi từ trong câu. Trong self-attention, mỗi từ được so sánh với tất cả các từ khác để xác định độ quan trọng hoặc liên quan của nó. Quá trình so sánh được thực hiện bằng cách tính tích vô hướng giữa nhúng từ, sau đó áp dụng phép softmax để thu được trọng số chú ý.\nVí dụ, đối với từ “Con,” tích vô hướng được tính toán với nhúng từ của tất cả các từ khác:\nTrọng số Chú ý cho “Con”:\n- \"Con\" so sánh với \"Con\": 0.3\n- \"Con\" so sánh với \"mèo\": 0.1\n- \"Con\" so sánh với \"ngồi\": 0.2\n- \"Con\" so sánh với \"trên\": 0.05\n- \"Con\" so sánh với \"chiếc\": 0.35\n- \"Con\" so sánh với \"thảm\": 0.0\n- \"Con\" so sánh với \".\": 0.0\nCác trọng số chú ý phản ánh sự quan trọng của mỗi từ đối với “Con” dựa trên sự tương đồng ngữ nghĩa hoặc liên quan ngữ cảnh của chúng. Phép softmax đảm bảo rằng các trọng số tổng cộng bằng 1.\nChúng ta lặp lại quá trình này cho mỗi từ trong câu để thu được trọng số self-attention cho toàn bộ câu. Những trọng số chú ý này sau đó có thể được sử dụng để tổng hợp thông tin từ các từ khác nhau và bắt chước các mối quan hệ quan trọng trong câu để tiếp tục xử lý trong mô hình.\n\n\n\n\n\n\nMulti-Head Attention là một phần mở rộng của self-attention cho phép mô hình tập trung vào các khía cạnh khác nhau của chuỗi đầu vào cùng một lúc. Đó là việc áp dụng self-attention nhiều lần song song, mỗi attention head tập trung vào một biểu diễn khác nhau của đầu vào. Bằng cách sử dụng nhiều attention head, mô hình có thể nắm bắt các thông tin khác nhau và học các mẫu đa dạng, nâng cao khả năng hiểu và xử lý các chuỗi phức tạp một cách hiệu quả.\n\n\n\n\nXem xét một câu tiếng Anh: “I love cats.” -&gt; Và chúng ta muốn dịch nó sang tiếng Pháp: “J’adore les chats.”\nGiả sử cả hai câu tiếng Anh và tiếng Pháp đã được nhúng thành các vector như sau:\nNhúng từ tiếng Anh:\n\n“I”: [0.1, 0.2, 0.3]\n“love”: [0.4, 0.5, 0.6]\n“cats”: [0.7, 0.8, 0.9]\n\nNhúng từ tiếng Pháp:\n\n“J’adore”: [0.9, 0.8, 0.7]\n“les”: [0.6, 0.5, 0.4]\n“chats”: [0.3, 0.2, 0.1]\n\nTính toán trọng số\n\nGiả sử chúng ta đang sử dụng cơ chế multi-head attention với 2 attention head. Mỗi head sẽ tập trung vào chuỗi đầu vào tiếng Anh (nguồn) và chuỗi đầu vào tiếng Pháp (đích) riêng biệt.\nVới mỗi attention head, chúng ta sẽ tính toán trọng số attention cho cả chuỗi đầu vào tiếng Anh và tiếng Pháp. Các trọng số attention xác định mức độ quan trọng của mỗi từ trong chuỗi nguồn liên quan đến các từ trong chuỗi đích.\nVí dụ,\n\nVới attention head đầu tiên, chúng ta tính toán trọng số attention cho chuỗi đầu vào tiếng Anh:\nAttention_scores_source_head1 = softmax(dot_product(english_source_embeddings, [french_target_embedding1, french_target_embedding2, french_target_embedding3]))\nTương tự, chúng ta tính toán trọng số attention cho chuỗi đích tiếng Pháp:\nAttention_scores_target_head1 = softmax(dot_product(french_target_embeddings, [english_source_embedding1, english_source_embedding2, english_source_embedding3]))\nLặp lại các bước trên cho attention head tiếp theo, kết quả sẽ được các attention score cho cả chuỗi nguồn và chuỗi đích.\nSau khi có các trọng số attention, chúng ta áp dụng chúng vào nhúng tương ứng để có tổng có trọng số cho mỗi chuỗi.\nVí dụ, đối với chuỗi nguồn tiếng Anh và attention head đầu tiên:\nWeighted_sum_source_head1 = attention_score1 * english_source_embedding1 + attention_score2 * english_source_embedding2 + attention_score3 * english_source_embedding3\nTương tự, đối với chuỗi đích tiếng Pháp và attention head đầu tiên:\nWeighted_sum_target_head1 = attention_score1 * french_target_embedding1 + attention_score2 * french_target_embedding2 + attention_score3 * french_target_embedding3\nChúng ta lặp lại các bước trên cho attention head thứ hai.\nCuối cùng, chúng ta nối các đầu ra từ mỗi attention head và thông qua một phép biến đổi tuyến tính để có đầu ra multi-head attention cuối cùng.\nNhư vậy, cơ chế multi-head attention cho phép mô hình nắm bắt các thông tin và mối quan hệ khác nhau giữa chuỗi nguồn và chuỗi đích, tăng khả năng dịch giữa các ngôn ngữ một cách chính xác.\n\n\n\n\nimport torch\nimport torch.nn as nn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.query_projection = nn.Linear(embed_dim, embed_dim)\n        self.key_projection = nn.Linear(embed_dim, embed_dim)\n        self.value_projection = nn.Linear(embed_dim, embed_dim)\n        self.output_projection = nn.Linear(embed_dim, embed_dim)\n        \n    def forward(self, query, key, value):\n        batch_size = query.size(0)\n        \n        # Apply linear projections for query, key, and value\n        query = self.query_projection(query)\n        key = self.key_projection(key)\n        value = self.value_projection(value)\n        \n        # Reshape query, key, and value to split heads\n        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # Compute attention scores\n        scores = torch.matmul(query, key.transpose(-2, -1))\n        scores = scores / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n        attention_weights = torch.softmax(scores, dim=-1)\n        \n        # Apply attention weights to value\n        attended_values = torch.matmul(attention_weights, value)\n        \n        # Reshape and concatenate attended values\n        attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n        \n        # Apply linear projection for output\n        output = self.output_projection(attended_values)\n        \n        return output\n\n\n\n\n\n\n\nPhép mã hóa vị trí (Positional Encoding) là một kỹ thuật được sử dụng trong các mô hình dựa trên chuỗi, đặc biệt là trong kiến trúc Transformer, để đưa thông tin về vị trí tương đối của các yếu tố trong chuỗi đầu vào. Nó giải quyết vấn đề rằng tự chú ý (self-attention) và các cơ chế chú ý khác trong các mô hình này không ngầm định mã hóa thông tin về vị trí.\nTrong các nhiệm vụ xử lý ngôn ngữ tự nhiên, thứ tự của các từ trong một câu mang ý nghĩa và ngữ cảnh quan trọng. Tuy nhiên, mô hình Transformer xử lý chuỗi đầu vào theo cách song song, không rõ ràng ghi nhận thứ tự tuần tự. Phép mã hóa vị trí giúp mô hình phân biệt giữa các yếu tố ở các vị trí khác nhau trong chuỗi.\nMã hóa vị trí được thêm vào nhúng đầu vào của chuỗi trước khi đưa chúng vào mô hình Transformer. Thông thường, điều này được thực hiện bằng cách kết hợp các hàm sin-cos với các nhúng đầu vào. Mỗi chiều của mã hóa vị trí đại diện cho một vị trí cụ thể trong chuỗi.\nSự lựa chọn các hàm sin-cos cho phép mô hình dễ dàng mở rộng cho các độ dài chuỗi dài hơn so với những gì mô hình đã được huấn luyện. Tần số của các hàm sin-cos tuân theo một cấp số học, cho phép mô hình học để chú ý đến các vị trí khác nhau với các tỷ lệ khác nhau.\nBằng cách thêm mã hóa vị trí vào nhúng đầu vào, mô hình Transformer có thể hiệu quả nắm bắt thứ tự và vị trí tương đối của các yếu tố trong chuỗi. Điều này giúp mô hình hiểu các phụ thuộc và mối quan hệ giữa các vị trí khác nhau và cho phép thực hiện các nhiệm vụ yêu cầu xử lý thông tin tuần tự, chẳng hạn như dịch máy hoặc hiểu ngôn ngữ.\n\n\n\n\nGiả sử chúng ta có một chuỗi đầu vào gồm ba từ: [“Tôi”, “yêu”, “mèo”].\nMỗi từ sẽ được biểu diễn bằng một vector nhúng từ, nắm bắt ý nghĩa ngữ nghĩa của nó. Hãy giả định các vector nhúng từ như sau:\n\n“Tôi”: [0.2, 0.3, -0.1]\n“yêu”: [-0.5, 0.7, 0.2]\n“mèo”: [0.7, 0.8, 0.9]\n\nĐể tích hợp thông tin về vị trí, chúng ta sử dụng mã hóa vị trí. Trong ví dụ này, chúng ta sẽ sử dụng các hàm sin và cos để tạo các vector mã hóa vị trí.\nGiả sử chiều nhúng là 3, các vector mã hóa vị trí sẽ được tính như sau:\nCho vị trí 0 (từ đầu tiên): - PE(0) = [sin(0/10000^0), cos(0/10000^0), sin(0/10000^0)]\nCho vị trí 1 (từ thứ hai): - PE(1) = [sin(1/10000^1), cos(1/10000^1), sin(1/10000^1)]\nCho vị trí 2 (từ thứ ba): - PE(2) = [sin(2/10000^0), cos(2/10000^0), sin(2/10000^0)]\nVì sin(0) = 0, sin(1) = 0.8415, sin(2) = 0.9093, cos(0) = 1, cos(1) = 0.5403, cos(2) = -0.4161.\nNên, các vector mã hóa vị trí là:\nPE(0) = [0, 1, 0] PE(1) = [0.8415, 0.5403, 0.8415] PE(2) = [0.9093, -0.4161, 0.9093]\nĐể tích hợp mã hóa vị trí, chúng ta cộng vector mã hóa vị trí tương ứng vào mỗi vector nhúng từ:\n\nTừ “Tôi” với mã hóa vị trí: [0.2, 0.3, -0.1] + [0, 1, 0] = [0.2, 1.3, -0.1]\nTừ “yêu” với mã hóa vị trí: [-0.5, 0.7, 0.2] + [0.8415, 0.5403, 0.8415] =\n\n[0.3415, 1.2403, 1.0415] - Từ “mèo” với mã hóa vị trí: [0.7, 0.8, 0.9] + [0.9093, -0.4161, 0.9093] = [1.6093, 0.3839, 1.8093]\nBằng cách cộng các vector mã hóa vị trí vào các vector nhúng từ, chúng ta giới thiệu thông tin về vị trí tương đối của các từ trong chuỗi đầu vào. Điều này cho phép mô hình Transformer nắm bắt thứ tự tuần tự và các phụ thuộc giữa các từ, điều quan trọng cho các nhiệm vụ như dịch máy hoặc phân tích cảm xúc.\n\n\n\n\nLayer Normalization là một kỹ thuật được sử dụng trong các mô hình học sâu để chuẩn hóa các kích hoạt (kết quả chuyển đổi đầu vào thành đầu ra bằng hàm kích hoạt - tạm gọi là các kích hoạt) của các đơn vị trong một lớp. Nó giải quyết vấn đề của sự thay đổi phân phối đầu vào của lớp trong quá trình huấn luyện, gây khó khăn cho quá trình học.\nCông thức cho Layer Normalization có thể được biểu diễn như sau:\noutput = scale * (input - mean) / sqrt(variance + epsilon) + bias\n\nỞ đây, đầu vào đại diện cho các kích hoạt đầu vào của một lớp, trung bình và phương sai đại diện cho giá trị trung bình và phương sai được tính theo chiều đầu vào, epsilon là một hằng số nhỏ được thêm vào để đảm bảo tính ổn định số học, và scale và bias là các tham số có thể học giúp mô hình điều chỉnh và dịch chuyển các kích hoạt đã được chuẩn hóa.\nLayer Normalization được áp dụng độc lập cho mỗi lô dữ liệu (batch), điều này làm cho nó phù hợp cho các nhiệm vụ với kích thước lô nhỏ. Nó đã được chứng minh là hiệu quả trong ổn định quá trình huấn luyện, tăng tốc quá trình hội tụ và cải thiện hiệu suất tổng quát của các mô hình học sâu.\nTổng quát, Layer Normalization giúp giảm sự phụ thuộc vào tỷ lệ của các kích hoạt đầu vào, thúc đẩy việc truyền gradient ổn định và cải thiện động học huấn luyện tổng thể (overall training dynamics - gồm learning rate, gradient, loss rate) của mạng neural. Nó đã được sử dụng rộng rãi trong các lĩnh vực khác nhau, bao gồm xử lý ngôn ngữ tự nhiên, thị giác máy tính và nhận dạng giọng nói.\n\n\n\nGiả sử chúng ta có một câu: “Tôi thích chơi bóng đá.”\nTrong mô hình ngôn ngữ, mục tiêu là dự đoán từ tiếp theo trong ngữ cảnh đã cho. Tuy nhiên, trong quá trình huấn luyện, chúng ta thường không muốn cho mô hình truy cập vào các từ trong tương lai vì nó có thể dẫn đến rò rỉ dữ liệu và dự đoán không thực tế. Vì vậy, chế che được áp dụng.\nĐể áp dụng cơ chế che, thường được ký hiệu là [MASK], để thay thế một số từ trong chuỗi đầu vào trong quá trình huấn luyện. Trong trường hợp này, chúng ta có thể ngẫu nhiên che một số từ trong câu, dẫn đến:\nCâu Đầu Vào (có cơ chế che): “Tôi thích chơi [MASK] bóng đá.”\nBây giờ, trong quá trình huấn luyện, mục tiêu của mô hình là dự đoán từ bị che dựa trên ngữ cảnh xung quanh. Nó học cách hiểu cấu trúc câu, ý nghĩa và sự phụ thuộc giữa các từ để đưa ra dự đoán chính xác. Mô hình nhận chuỗi đầu vào đã được chỉnh sửa (thêm [MASK]) và cố gắng dự đoán bản gốc, không bị che, ở vị trí đã được che.\nTrong quá trình huấn luyện hoặc đánh giá, cơ chế che không được sử dụng và mô hình được cung cấp với toàn bộ chuỗi đầu vào. Nó có thể tạo ra dự đoán cho từ tiếp theo trong chuỗi dựa trên kiến thức đã học từ quá trình huấn luyện.\nCơ chế che giúp mô hình tổng quát tốt và xử lý các trường hợp mà nó gặp phải các từ chưa được nhìn thấy hoặc thiếu. Nó khuyến khích mô hình dựa vào ngữ cảnh có sẵn để đưa ra dự đoán chính xác và cải thiện khả năng hiểu cấu trúc câu.",
    "crumbs": [
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html#transformer-architecture",
    "href": "machine-learning/transformer.html#transformer-architecture",
    "title": "Transformer",
    "section": "",
    "text": "Kiến trúc Transformer là một mô hình học sâu phổ biến được giới thiệu trong bài báo “Attention is All You Need” của Vaswani và đồng nghiệp (2017). Nó đã cách mạng hóa nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) và trở thành cơ sở cho các mô hình tiên tiến như BERT và GPT.\nKiến trúc Transformer dựa trên khái niệm self-attention, cho phép mô hình tập trung vào các phần khác nhau của chuỗi đầu vào khi xử lý từng phần tử. Dưới đây là một cái nhìn tổng quan về kiến trúc Transformer:\n\n\n\nKiến trúc Transformer\n\n\n\nNhúng Đầu Vào (Input Embeddings):\n\nChuỗi đầu vào được nhúng thành các biểu diễn liên tục được gọi là nhúng đầu vào.\nMỗi từ hoặc thành phần trong chuỗi đầu vào được biểu diễn bằng một vector mật độ.\n\nMã Hóa Vị Trí (Positional Encoding):\n\nMã hóa vị trí được thêm vào nhúng đầu vào để cung cấp thông tin về thứ tự hoặc vị trí của các từ trong chuỗi.\nĐiều này cho phép mô hình bắt chước thông tin tuần tự, vì kiến trúc Transformer không có kết nối lặp lại.\n\nBộ Mã Hóa (Encoder):\n\nTransformer bao gồm một cấu trúc mã hóa-giải mã (encoder-decoder), nhưng để đơn giản, chúng ta tập trung vào bộ mã hóa.\nBộ mã hóa bao gồm nhiều lớp giống nhau, mỗi lớp bao gồm hai lớp con:\n\nMulti-head Self-Attention:\n\nSelf-attention cho phép mô hình đánh trọng số sự quan trọng của các từ khác nhau trong chuỗi đầu vào khi xử lý một từ cụ thể.\nNó tính toán tổng trọng số của các nhúng từ tất cả các từ, trong đó trọng số được xác định bởi sự tương đồng giữa các từ.\n\nFeed-Forward Neural Network:\n\nA simple feed-forward neural network được áp dụng cho từng vị trí trong chuỗi độc lập.\nNó giúp bắt chước mối quan hệ phi tuyến giữa các từ trong chuỗi.\n\n\n\nBộ Giải Mã (Decoder):\n\nBộ giải mã tương tự như bộ mã hóa nhưng có cơ chế attention bổ sung.\nNó tạo ra chuỗi đầu ra bằng cách chú ý đến các biểu diễn đầu ra của bộ mã hóa và các từ đã được tạo ra trước đó.\n\nCơ Chế Che (Masking):\n\nTrong quá trình huấn luyện, cơ chế che được sử dụng để ngăn mô hình chú ý đến các vị trí trong tương lai.\nĐiều này đảm bảo rằng mỗi vị trí chỉ có thể chú ý đến các vị trí trước đó trong chuỗi đầu vào.\n\nTạo Ra Đầu Ra:\n\nLớp cuối cùng của bộ giải mã tạo ra xác suất đầu ra cho mỗi vị trí trong chuỗi đầu ra.\nCác xác suất này thường được tính bằng cách sử dụng hàm kích hoạt softmax.\n\n\nKiến trúc Transformer có một số lợi thế, như khả năng song song hóa, khả năng bắt chước các phụ thuộc xa, và khả năng tổng quát tốt hơn nhờ self-attention. Nó đã đạt được kết quả tiên tiến trên nhiều nhiệm vụ NLP, bao gồm dịch máy, tóm tắt văn bản và hiểu ngôn ngữ.",
    "crumbs": [
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html#self-attention",
    "href": "machine-learning/transformer.html#self-attention",
    "title": "Transformer",
    "section": "",
    "text": "Self-attention, còn được gọi là intra-attention hay scaled dot-product attention (tích vô hướng giữa 2 vector), là một cơ chế tính toán trọng số chú ý cho các phần tử trong cùng một chuỗi đầu vào. Nó cho phép mô hình đánh giá mức độ quan trọng của mỗi phần tử trong chuỗi dựa trên mức liên quan của nó đối với các phần tử khác.\n\n\n\nTrong self-attention, mỗi từ trong chuỗi đầu vào được biểu diễn bằng một vector, thông thường được gọi là nhúng hay vector nhúng (embedding vector). Giả sử nhúng từ cho chuỗi đầu vào như sau:\nVí dụ câu như sau: “Con mèo ngồi trên chiếc thảm.”\nĐể áp dụng self-attention, chúng ta trước tiên biểu diễn mỗi từ trong câu thành một vector nhúng. Hãy giả sử chúng ta có các vector nhúng từ sau đây:\nNhúng từ:\n- \"Con\": [0.2, 0.5, -0.3]\n- \"mèo\": [-0.1, 0.7, 0.2]\n- \"ngồi\": [0.4, -0.2, 0.6]\n- \"trên\": [-0.5, 0.3, -0.1]\n- \"chiếc\": [0.2, 0.5, -0.3]\n- \"thảm\": [0.3, 0.1, 0.8]\n- \".\": [0.0, 0.0, 0.0]\nBây giờ, chúng ta tính toán trọng số self-attention cho mỗi từ trong câu. Trong self-attention, mỗi từ được so sánh với tất cả các từ khác để xác định độ quan trọng hoặc liên quan của nó. Quá trình so sánh được thực hiện bằng cách tính tích vô hướng giữa nhúng từ, sau đó áp dụng phép softmax để thu được trọng số chú ý.\nVí dụ, đối với từ “Con,” tích vô hướng được tính toán với nhúng từ của tất cả các từ khác:\nTrọng số Chú ý cho “Con”:\n- \"Con\" so sánh với \"Con\": 0.3\n- \"Con\" so sánh với \"mèo\": 0.1\n- \"Con\" so sánh với \"ngồi\": 0.2\n- \"Con\" so sánh với \"trên\": 0.05\n- \"Con\" so sánh với \"chiếc\": 0.35\n- \"Con\" so sánh với \"thảm\": 0.0\n- \"Con\" so sánh với \".\": 0.0\nCác trọng số chú ý phản ánh sự quan trọng của mỗi từ đối với “Con” dựa trên sự tương đồng ngữ nghĩa hoặc liên quan ngữ cảnh của chúng. Phép softmax đảm bảo rằng các trọng số tổng cộng bằng 1.\nChúng ta lặp lại quá trình này cho mỗi từ trong câu để thu được trọng số self-attention cho toàn bộ câu. Những trọng số chú ý này sau đó có thể được sử dụng để tổng hợp thông tin từ các từ khác nhau và bắt chước các mối quan hệ quan trọng trong câu để tiếp tục xử lý trong mô hình.",
    "crumbs": [
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html#multi-head-attention",
    "href": "machine-learning/transformer.html#multi-head-attention",
    "title": "Transformer",
    "section": "",
    "text": "Multi-Head Attention là một phần mở rộng của self-attention cho phép mô hình tập trung vào các khía cạnh khác nhau của chuỗi đầu vào cùng một lúc. Đó là việc áp dụng self-attention nhiều lần song song, mỗi attention head tập trung vào một biểu diễn khác nhau của đầu vào. Bằng cách sử dụng nhiều attention head, mô hình có thể nắm bắt các thông tin khác nhau và học các mẫu đa dạng, nâng cao khả năng hiểu và xử lý các chuỗi phức tạp một cách hiệu quả.\n\n\n\n\nXem xét một câu tiếng Anh: “I love cats.” -&gt; Và chúng ta muốn dịch nó sang tiếng Pháp: “J’adore les chats.”\nGiả sử cả hai câu tiếng Anh và tiếng Pháp đã được nhúng thành các vector như sau:\nNhúng từ tiếng Anh:\n\n“I”: [0.1, 0.2, 0.3]\n“love”: [0.4, 0.5, 0.6]\n“cats”: [0.7, 0.8, 0.9]\n\nNhúng từ tiếng Pháp:\n\n“J’adore”: [0.9, 0.8, 0.7]\n“les”: [0.6, 0.5, 0.4]\n“chats”: [0.3, 0.2, 0.1]\n\nTính toán trọng số\n\nGiả sử chúng ta đang sử dụng cơ chế multi-head attention với 2 attention head. Mỗi head sẽ tập trung vào chuỗi đầu vào tiếng Anh (nguồn) và chuỗi đầu vào tiếng Pháp (đích) riêng biệt.\nVới mỗi attention head, chúng ta sẽ tính toán trọng số attention cho cả chuỗi đầu vào tiếng Anh và tiếng Pháp. Các trọng số attention xác định mức độ quan trọng của mỗi từ trong chuỗi nguồn liên quan đến các từ trong chuỗi đích.\nVí dụ,\n\nVới attention head đầu tiên, chúng ta tính toán trọng số attention cho chuỗi đầu vào tiếng Anh:\nAttention_scores_source_head1 = softmax(dot_product(english_source_embeddings, [french_target_embedding1, french_target_embedding2, french_target_embedding3]))\nTương tự, chúng ta tính toán trọng số attention cho chuỗi đích tiếng Pháp:\nAttention_scores_target_head1 = softmax(dot_product(french_target_embeddings, [english_source_embedding1, english_source_embedding2, english_source_embedding3]))\nLặp lại các bước trên cho attention head tiếp theo, kết quả sẽ được các attention score cho cả chuỗi nguồn và chuỗi đích.\nSau khi có các trọng số attention, chúng ta áp dụng chúng vào nhúng tương ứng để có tổng có trọng số cho mỗi chuỗi.\nVí dụ, đối với chuỗi nguồn tiếng Anh và attention head đầu tiên:\nWeighted_sum_source_head1 = attention_score1 * english_source_embedding1 + attention_score2 * english_source_embedding2 + attention_score3 * english_source_embedding3\nTương tự, đối với chuỗi đích tiếng Pháp và attention head đầu tiên:\nWeighted_sum_target_head1 = attention_score1 * french_target_embedding1 + attention_score2 * french_target_embedding2 + attention_score3 * french_target_embedding3\nChúng ta lặp lại các bước trên cho attention head thứ hai.\nCuối cùng, chúng ta nối các đầu ra từ mỗi attention head và thông qua một phép biến đổi tuyến tính để có đầu ra multi-head attention cuối cùng.\nNhư vậy, cơ chế multi-head attention cho phép mô hình nắm bắt các thông tin và mối quan hệ khác nhau giữa chuỗi nguồn và chuỗi đích, tăng khả năng dịch giữa các ngôn ngữ một cách chính xác.\n\n\n\n\nimport torch\nimport torch.nn as nn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.query_projection = nn.Linear(embed_dim, embed_dim)\n        self.key_projection = nn.Linear(embed_dim, embed_dim)\n        self.value_projection = nn.Linear(embed_dim, embed_dim)\n        self.output_projection = nn.Linear(embed_dim, embed_dim)\n        \n    def forward(self, query, key, value):\n        batch_size = query.size(0)\n        \n        # Apply linear projections for query, key, and value\n        query = self.query_projection(query)\n        key = self.key_projection(key)\n        value = self.value_projection(value)\n        \n        # Reshape query, key, and value to split heads\n        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # Compute attention scores\n        scores = torch.matmul(query, key.transpose(-2, -1))\n        scores = scores / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n        attention_weights = torch.softmax(scores, dim=-1)\n        \n        # Apply attention weights to value\n        attended_values = torch.matmul(attention_weights, value)\n        \n        # Reshape and concatenate attended values\n        attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n        \n        # Apply linear projection for output\n        output = self.output_projection(attended_values)\n        \n        return output",
    "crumbs": [
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html#positional-encoding",
    "href": "machine-learning/transformer.html#positional-encoding",
    "title": "Transformer",
    "section": "",
    "text": "Phép mã hóa vị trí (Positional Encoding) là một kỹ thuật được sử dụng trong các mô hình dựa trên chuỗi, đặc biệt là trong kiến trúc Transformer, để đưa thông tin về vị trí tương đối của các yếu tố trong chuỗi đầu vào. Nó giải quyết vấn đề rằng tự chú ý (self-attention) và các cơ chế chú ý khác trong các mô hình này không ngầm định mã hóa thông tin về vị trí.\nTrong các nhiệm vụ xử lý ngôn ngữ tự nhiên, thứ tự của các từ trong một câu mang ý nghĩa và ngữ cảnh quan trọng. Tuy nhiên, mô hình Transformer xử lý chuỗi đầu vào theo cách song song, không rõ ràng ghi nhận thứ tự tuần tự. Phép mã hóa vị trí giúp mô hình phân biệt giữa các yếu tố ở các vị trí khác nhau trong chuỗi.\nMã hóa vị trí được thêm vào nhúng đầu vào của chuỗi trước khi đưa chúng vào mô hình Transformer. Thông thường, điều này được thực hiện bằng cách kết hợp các hàm sin-cos với các nhúng đầu vào. Mỗi chiều của mã hóa vị trí đại diện cho một vị trí cụ thể trong chuỗi.\nSự lựa chọn các hàm sin-cos cho phép mô hình dễ dàng mở rộng cho các độ dài chuỗi dài hơn so với những gì mô hình đã được huấn luyện. Tần số của các hàm sin-cos tuân theo một cấp số học, cho phép mô hình học để chú ý đến các vị trí khác nhau với các tỷ lệ khác nhau.\nBằng cách thêm mã hóa vị trí vào nhúng đầu vào, mô hình Transformer có thể hiệu quả nắm bắt thứ tự và vị trí tương đối của các yếu tố trong chuỗi. Điều này giúp mô hình hiểu các phụ thuộc và mối quan hệ giữa các vị trí khác nhau và cho phép thực hiện các nhiệm vụ yêu cầu xử lý thông tin tuần tự, chẳng hạn như dịch máy hoặc hiểu ngôn ngữ.\n\n\n\n\nGiả sử chúng ta có một chuỗi đầu vào gồm ba từ: [“Tôi”, “yêu”, “mèo”].\nMỗi từ sẽ được biểu diễn bằng một vector nhúng từ, nắm bắt ý nghĩa ngữ nghĩa của nó. Hãy giả định các vector nhúng từ như sau:\n\n“Tôi”: [0.2, 0.3, -0.1]\n“yêu”: [-0.5, 0.7, 0.2]\n“mèo”: [0.7, 0.8, 0.9]\n\nĐể tích hợp thông tin về vị trí, chúng ta sử dụng mã hóa vị trí. Trong ví dụ này, chúng ta sẽ sử dụng các hàm sin và cos để tạo các vector mã hóa vị trí.\nGiả sử chiều nhúng là 3, các vector mã hóa vị trí sẽ được tính như sau:\nCho vị trí 0 (từ đầu tiên): - PE(0) = [sin(0/10000^0), cos(0/10000^0), sin(0/10000^0)]\nCho vị trí 1 (từ thứ hai): - PE(1) = [sin(1/10000^1), cos(1/10000^1), sin(1/10000^1)]\nCho vị trí 2 (từ thứ ba): - PE(2) = [sin(2/10000^0), cos(2/10000^0), sin(2/10000^0)]\nVì sin(0) = 0, sin(1) = 0.8415, sin(2) = 0.9093, cos(0) = 1, cos(1) = 0.5403, cos(2) = -0.4161.\nNên, các vector mã hóa vị trí là:\nPE(0) = [0, 1, 0] PE(1) = [0.8415, 0.5403, 0.8415] PE(2) = [0.9093, -0.4161, 0.9093]\nĐể tích hợp mã hóa vị trí, chúng ta cộng vector mã hóa vị trí tương ứng vào mỗi vector nhúng từ:\n\nTừ “Tôi” với mã hóa vị trí: [0.2, 0.3, -0.1] + [0, 1, 0] = [0.2, 1.3, -0.1]\nTừ “yêu” với mã hóa vị trí: [-0.5, 0.7, 0.2] + [0.8415, 0.5403, 0.8415] =\n\n[0.3415, 1.2403, 1.0415] - Từ “mèo” với mã hóa vị trí: [0.7, 0.8, 0.9] + [0.9093, -0.4161, 0.9093] = [1.6093, 0.3839, 1.8093]\nBằng cách cộng các vector mã hóa vị trí vào các vector nhúng từ, chúng ta giới thiệu thông tin về vị trí tương đối của các từ trong chuỗi đầu vào. Điều này cho phép mô hình Transformer nắm bắt thứ tự tuần tự và các phụ thuộc giữa các từ, điều quan trọng cho các nhiệm vụ như dịch máy hoặc phân tích cảm xúc.",
    "crumbs": [
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html#layer-normalization",
    "href": "machine-learning/transformer.html#layer-normalization",
    "title": "Transformer",
    "section": "",
    "text": "Layer Normalization là một kỹ thuật được sử dụng trong các mô hình học sâu để chuẩn hóa các kích hoạt (kết quả chuyển đổi đầu vào thành đầu ra bằng hàm kích hoạt - tạm gọi là các kích hoạt) của các đơn vị trong một lớp. Nó giải quyết vấn đề của sự thay đổi phân phối đầu vào của lớp trong quá trình huấn luyện, gây khó khăn cho quá trình học.\nCông thức cho Layer Normalization có thể được biểu diễn như sau:\noutput = scale * (input - mean) / sqrt(variance + epsilon) + bias\n\nỞ đây, đầu vào đại diện cho các kích hoạt đầu vào của một lớp, trung bình và phương sai đại diện cho giá trị trung bình và phương sai được tính theo chiều đầu vào, epsilon là một hằng số nhỏ được thêm vào để đảm bảo tính ổn định số học, và scale và bias là các tham số có thể học giúp mô hình điều chỉnh và dịch chuyển các kích hoạt đã được chuẩn hóa.\nLayer Normalization được áp dụng độc lập cho mỗi lô dữ liệu (batch), điều này làm cho nó phù hợp cho các nhiệm vụ với kích thước lô nhỏ. Nó đã được chứng minh là hiệu quả trong ổn định quá trình huấn luyện, tăng tốc quá trình hội tụ và cải thiện hiệu suất tổng quát của các mô hình học sâu.\nTổng quát, Layer Normalization giúp giảm sự phụ thuộc vào tỷ lệ của các kích hoạt đầu vào, thúc đẩy việc truyền gradient ổn định và cải thiện động học huấn luyện tổng thể (overall training dynamics - gồm learning rate, gradient, loss rate) của mạng neural. Nó đã được sử dụng rộng rãi trong các lĩnh vực khác nhau, bao gồm xử lý ngôn ngữ tự nhiên, thị giác máy tính và nhận dạng giọng nói.",
    "crumbs": [
      "Transformer"
    ]
  },
  {
    "objectID": "machine-learning/transformer.html#masking",
    "href": "machine-learning/transformer.html#masking",
    "title": "Transformer",
    "section": "",
    "text": "Giả sử chúng ta có một câu: “Tôi thích chơi bóng đá.”\nTrong mô hình ngôn ngữ, mục tiêu là dự đoán từ tiếp theo trong ngữ cảnh đã cho. Tuy nhiên, trong quá trình huấn luyện, chúng ta thường không muốn cho mô hình truy cập vào các từ trong tương lai vì nó có thể dẫn đến rò rỉ dữ liệu và dự đoán không thực tế. Vì vậy, chế che được áp dụng.\nĐể áp dụng cơ chế che, thường được ký hiệu là [MASK], để thay thế một số từ trong chuỗi đầu vào trong quá trình huấn luyện. Trong trường hợp này, chúng ta có thể ngẫu nhiên che một số từ trong câu, dẫn đến:\nCâu Đầu Vào (có cơ chế che): “Tôi thích chơi [MASK] bóng đá.”\nBây giờ, trong quá trình huấn luyện, mục tiêu của mô hình là dự đoán từ bị che dựa trên ngữ cảnh xung quanh. Nó học cách hiểu cấu trúc câu, ý nghĩa và sự phụ thuộc giữa các từ để đưa ra dự đoán chính xác. Mô hình nhận chuỗi đầu vào đã được chỉnh sửa (thêm [MASK]) và cố gắng dự đoán bản gốc, không bị che, ở vị trí đã được che.\nTrong quá trình huấn luyện hoặc đánh giá, cơ chế che không được sử dụng và mô hình được cung cấp với toàn bộ chuỗi đầu vào. Nó có thể tạo ra dự đoán cho từ tiếp theo trong chuỗi dựa trên kiến thức đã học từ quá trình huấn luyện.\nCơ chế che giúp mô hình tổng quát tốt và xử lý các trường hợp mà nó gặp phải các từ chưa được nhìn thấy hoặc thiếu. Nó khuyến khích mô hình dựa vào ngữ cảnh có sẵn để đưa ra dự đoán chính xác và cải thiện khả năng hiểu cấu trúc câu.",
    "crumbs": [
      "Transformer"
    ]
  },
  {
    "objectID": "python-snippet/Modelling.html",
    "href": "python-snippet/Modelling.html",
    "title": "Modelling",
    "section": "",
    "text": "Model explain\n\ndalex\nmodelStudio - R & Python examples",
    "crumbs": [
      "Python Snippet",
      "Modelling"
    ]
  },
  {
    "objectID": "python-snippet/Index.html#using-conda",
    "href": "python-snippet/Index.html#using-conda",
    "title": "Python Snippet",
    "section": "1.1. Using conda",
    "text": "1.1. Using conda\n\n1.1.1. Create conda environment\n\nCreate env by common command line\n\nconda create -n python38 python=3.8.5 pip=20.2.4 ipykernel notebook\nconda activate python38\n\nCreate env use environment.yaml file\n\nname: env_ascore\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.8\n  - pandas==1.4.4  \n  - joblib==1.1.0\n  - statsmodels==0.13.2\n  - ipykernel\n  - zipp  \n  - pip\n  - pip: \n    - optbinning==0.17.3\n    - ortools==9.4.1874\n# conda env create -f environment.yaml  \n# conda env remove -n env_ascore\n# set https_proxy=10.1.33.23:8080\n# set http_proxy=10.1.33.23:8080\n# conda install -n env_ascore ipykernel --update-deps --force-reinstall\n\nCreate env use requirements.txt file\n\nconda list --export &gt; requirements.txt\nconda install --file requirements.txt\n\n\n1.1.2. Conda common commands\n\nConda environment list\n\nconda info --env\n\nRemove conda environment\n\nconda deactivate\nconda env remove -n python38\n\nActivate conda environment\n\nconda activate python38\n\nClean unused library\n\nconda clean --all\npip cache remove *",
    "crumbs": [
      "Python Snippet"
    ]
  },
  {
    "objectID": "python-snippet/Index.html#create-environment-for-jupyter-notebook",
    "href": "python-snippet/Index.html#create-environment-for-jupyter-notebook",
    "title": "Python Snippet",
    "section": "1.2. Create environment for jupyter notebook",
    "text": "1.2. Create environment for jupyter notebook\n\n1.2.1. create\nconda activate python38\nipython kernel install --user --name=python38\n\n\n1.2.2. Remove jupyter notebook environment (require run as administrator)\njupyter kernelspec list\njupyter kernelspec uninstall python38",
    "crumbs": [
      "Python Snippet"
    ]
  },
  {
    "objectID": "python-snippet/Index.html#install-offline-packages-using-requirements.txt",
    "href": "python-snippet/Index.html#install-offline-packages-using-requirements.txt",
    "title": "Python Snippet",
    "section": "2.1. Install OFFLINE packages using requirements.txt",
    "text": "2.1. Install OFFLINE packages using requirements.txt\n\nStep 1: Input to requirements.txt file in current directory. Eg. content \"jupyter-contrib-nbextensions==0.5.1\"\nStep 2: Create folder wheel (Eg. D:)\nStep 3: Run following command to download dependencies packages to folder wheel\n\npip download -r requirements.txt -d wheel\n\nStep 4: Run following command to install\n\npip install -r requirements.txt --find-links=D:\\wheel --no-index",
    "crumbs": [
      "Python Snippet"
    ]
  },
  {
    "objectID": "python-snippet/Index.html#install-offline-linux-package",
    "href": "python-snippet/Index.html#install-offline-linux-package",
    "title": "Python Snippet",
    "section": "2.2. Install OFFLINE Linux package",
    "text": "2.2. Install OFFLINE Linux package\nActivate same version python (i.e 3.7.0) and type command following\npip download --platform manylinux1_x86_64 --only-binary=:all: --no-binary=:none: pandas",
    "crumbs": [
      "Python Snippet"
    ]
  },
  {
    "objectID": "python-snippet/Index.html#export-requirements",
    "href": "python-snippet/Index.html#export-requirements",
    "title": "Python Snippet",
    "section": "2.3. Export requirements",
    "text": "2.3. Export requirements\npip list --format=freeze &gt; requirements.txt",
    "crumbs": [
      "Python Snippet"
    ]
  },
  {
    "objectID": "python-snippet/Index.html#connect-to-database",
    "href": "python-snippet/Index.html#connect-to-database",
    "title": "Python Snippet",
    "section": "Connect to database",
    "text": "Connect to database\nimport pandas as pd\nimport pyodbc\nimport sqlalchemy as sa\nimport urllib\nfrom sqlalchemy import create_engine, event\nfrom sqlalchemy.engine.url import URL\n\nserver = 'VM-DC-JUMPSRV77\\\\IFRS9' \ndatabase = 'DATA' \nusername = 'username' \npassword = '@@@@@@' \n    \nparams = urllib.parse.quote_plus(\"DRIVER={SQL Server};\"\n                                     \"SERVER=\"+server+\";\"\n                                     \"DATABASE=\"+database+\";\"\n                                     \"UID=\"+username+\";\"\n                                     \"PWD=\"+password+\";\")\n    \nengine = sa.create_engine(\"mssql+pyodbc:///?odbc_connect={}\".format(params))",
    "crumbs": [
      "Python Snippet"
    ]
  },
  {
    "objectID": "python-snippet/Index.html#readpush-table-to-database",
    "href": "python-snippet/Index.html#readpush-table-to-database",
    "title": "Python Snippet",
    "section": "Read/push table to database",
    "text": "Read/push table to database\n# read data\ndf_test = pd.read_sql('SELECT top 5 * FROM b_tmp_cl020', engine)\n# push data\ndf_test.to_sql('binh_test', engine, if_exists = 'append')",
    "crumbs": [
      "Python Snippet"
    ]
  },
  {
    "objectID": "python-snippet/Index.html#sqlite3",
    "href": "python-snippet/Index.html#sqlite3",
    "title": "Python Snippet",
    "section": "sqlite3",
    "text": "sqlite3\ncon = sqlite3.connect(\"data/mydata.sqlite\")\n\ncur = con.cursor()\n\n# The result of a \"cursor.execute\" can be iterated over by row\nfor row in cur.execute('SELECT * FROM tbl;'):\n    print(row)\n\n# Be sure to close the connection\ncon.close()",
    "crumbs": [
      "Python Snippet"
    ]
  },
  {
    "objectID": "python-snippet/Code-optimize.html",
    "href": "python-snippet/Code-optimize.html",
    "title": "Code optimize",
    "section": "",
    "text": "import cProfile\nimport optuna\nimport time\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\n# Define the LSTMModel class and Trainer class here\n\n# Define the objective function for Optuna optimization\ndef objective(trial):\n    # ... (rest of the code)\n\n# Profile the objective function\nprofiler = cProfile.Profile()\nprofiler.enable()\n\n# Perform hyperparameter optimization with Optuna\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\nprofiler.disable()\nprofiler.print_stats(sort='cumulative')",
    "crumbs": [
      "Python Snippet",
      "Code optimize"
    ]
  },
  {
    "objectID": "python-snippet/Code-optimize.html#profiling",
    "href": "python-snippet/Code-optimize.html#profiling",
    "title": "Code optimize",
    "section": "",
    "text": "import cProfile\nimport optuna\nimport time\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\n# Define the LSTMModel class and Trainer class here\n\n# Define the objective function for Optuna optimization\ndef objective(trial):\n    # ... (rest of the code)\n\n# Profile the objective function\nprofiler = cProfile.Profile()\nprofiler.enable()\n\n# Perform hyperparameter optimization with Optuna\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\nprofiler.disable()\nprofiler.print_stats(sort='cumulative')",
    "crumbs": [
      "Python Snippet",
      "Code optimize"
    ]
  },
  {
    "objectID": "python-snippet/slopegraph.html",
    "href": "python-snippet/slopegraph.html",
    "title": "Slopegraphs",
    "section": "",
    "text": "Slopegraphs\nMột slopegraph là một loại biểu đồ trực quan hóa dữ liệu, hiển thị sự thay đổi giữa hai điểm thời gian hoặc giữa hai điều kiện. Edward Tufte đã giới thiệu khái niệm này trong cuốn sách của ông “The Visual Display of Quantitative Information” vào năm 1983. Slopegraphs đặc biệt hữu ích để trực quan hóa sự thay đổi tương đối giữa hai điểm dữ liệu qua nhiều hạng mục.\nDưới đây là một ý tưởng tổng quan về cách slopegraph hoạt động:\n\nHai trục: Có hai trục dọc, một cho điểm thời gian ban đầu (hoặc điều kiện) và một cho điểm thời gian cuối cùng (hoặc điều kiện). Hai trục này song song và thường có cùng một tỷ lệ.\nHạng mục: Nhiều hạng mục hoặc mục được liệt kê giữa hai trục. Đối với mỗi hạng mục, có một điểm dữ liệu trên mỗi trục.\nĐộ dốc: Các đường được vẽ nối các điểm dữ liệu cho mỗi hạng mục giữa hai trục. Độ dốc của các đường này cho thấy sự thay đổi giá trị cho mỗi hạng mục. Độ dốc dương cho thấy sự tăng lên, trong khi độ dốc âm cho thấy sự giảm đi.\nSo sánh: Người xem có thể dễ dàng so sánh sự thay đổi giá trị qua các hạng mục bằng cách nhìn vào độ dốc của các đường.\n\nNhững ưu điểm chính của slopegraphs là:\n\nĐơn giản: Chúng dễ hiểu, ngay cả đối với những người không quen thuộc với các kỹ thuật trực quan hóa dữ liệu.\nKhả năng so sánh: Tính chất hai trục đứng cạnh nhau giúp dễ dàng so sánh sự thay đổi giữa nhiều hạng mục chỉ qua một cái nhìn.\nNhấn mạnh sự thay đổi: Slopegraphs đặc biệt hiệu quả khi nêu bật sự khác biệt trong sự thay đổi, thay vì giá trị tuyệt đối.\n\nTuy nhiên, chúng có thể không phải là lựa chọn tốt nhất khi:\n\nCó quá nhiều hạng mục, làm cho biểu đồ bị rối và khó diễn giải.\nCác điểm dữ liệu quá gần nhau, dẫn đến việc các đường chồng lên nhau.\nCần trực quan hóa nhiều hơn hai điểm thời gian hoặc điều kiện.\n\n\nimport pandas as pd\nfrom lets_plot import *\nLetsPlot.setup_html()\n\n# Sample data\ncountries = ['Country A', 'Country B', 'Country C']\ngdp_2020 = [1000, 1200, 800]\ngdp_2023 = [1300, 1250, 820]\n\n# Create a dataframe\ndf_2020 = pd.DataFrame({'Year': '2020', 'Country': countries, 'GDP': gdp_2020})\ndf_2023 = pd.DataFrame({'Year': '2023', 'Country': countries, 'GDP': gdp_2023})\ndf = pd.concat([df_2020, df_2023])\n\n# Slopegraph\nslopegraph = ggplot(df, aes(x='Year', y='GDP', group='Country')) + \\\n    geom_line(aes(color='Country'), size=1.5) + \\\n    geom_point(aes(color='Country'), size=3) + \\\n    ggtitle(\"Slopegraph: GDP Change from 2020 to 2023\") + \\\n    coord_fixed(ratio=0.002) + \\\n    theme(axis_text_x=element_text(angle=45, hjust=1)) + \\\n    theme(legend_position='none')\n\nslopegraph\n\n\n            \n            \n            \n\n\n   \n   \n\n\n\nSo sánh: Slopegraph và Barchart\n\n\n\n\n\n\n\n\nYếu tố\nSlopegraph\nBarchart\n\n\n\n\nMục đích\nThiết kế để hiển thị sự thay đổi giữa hai điểm thời gian hoặc hai điều kiện. Hiệu quả khi trực quan hóa sự thay đổi tương đối qua nhiều hạng mục.\nĐa dạng và có thể biểu diễn số lượng cho dữ liệu danh mục. Hiển thị giá trị tuyệt đối cho các hạng mục khác nhau hoặc phân bố dữ liệu.\n\n\nYếu tố trực quan\nSử dụng hai trục dọc với các đường nối các điểm dữ liệu giữa các trục. Độ dốc của các đường này biểu thị sự thay đổi.\nSử dụng các thanh có chiều dài (hoặc chiều cao) biến thiên để biểu diễn giá trị. Chiều dài hoặc chiều cao của mỗi thanh tỷ lệ thuận với giá trị nó biểu diễn.\n\n\nƯu điểm\n- Nhấn mạnh sự thay đổi tương đối giữa hai điểm dữ liệu.  - So sánh rõ ràng cạnh nhau.  - Đơn giản và tối giản.\n- Dễ hiểu và được công nhận rộng rãi.  - Có thể biểu diễn nhiều hạng mục.  - Có thể sử dụng cho cả dữ liệu danh danh và thứ tự.\n\n\nHạn chế\n- Tốt nhất khi so sánh chỉ hai điểm.  - Có thể bị rối khi có nhiều hạng mục hoặc các điểm dữ liệu gần nhau.  - Không lý tưởng để hiển thị giá trị tuyệt đối.\n- Không nhấn mạnh sự thay đổi tương đối một cách hiệu quả.  - Có thể bị rối với nhiều hạng mục.  - Yêu cầu nhiều không gian hơn so với các loại biểu đồ khác.\n\n\n\n\n\n# Barchart\nbarchart = ggplot(df, aes(x='Country', y='GDP', fill='Year')) + \\\n    geom_bar(stat='identity', position='dodge') + \\\n    ggtitle(\"Barchart: GDP Values for 2020 and 2023\") + \\\n    theme(legend_position='top')\n\n# Display\nbarchart",
    "crumbs": [
      "Python Snippet",
      "Slopegraphs"
    ]
  },
  {
    "objectID": "python-snippet/convertLightGBMFromR2Python.html",
    "href": "python-snippet/convertLightGBMFromR2Python.html",
    "title": "Convert the MLR3 LightGBM model from R to Python",
    "section": "",
    "text": "Buile LightGBM model by the following steps:\n\nLoads the necessary libraries for mlr3, LightGBM, and data manipulation.\nSets the logging threshold for mlr3 to the warning level.\nLoads the German Credit dataset and creates a classification task.\nDefines a preprocessing pipeline with specific operations such as imputation, encoding, and feature filtering.\nSets parameter values for the preprocessing steps (e.g., filter fraction).\nDefines a LightGBM learner with a specified number of iterations.\nCombines the preprocessing and learner into a single pipeline.\nCreates a GraphLearner to encapsulate the pipeline.\nTrains the model on the classification task.\nMakes predictions on the task using the trained model.\nExtracts the LightGBM model from the pipeline.\nSpecifies a filename for saving the LightGBM model.\nSaves the LightGBM model to a file.\n\n# Load necessary libraries\nlibrary(\"mlr3verse\")\nlibrary(\"mlr3learners\")\nlibrary(\"mlr3tuning\")\nlibrary(\"data.table\")\nlibrary(\"ggplot2\")\n\n# Set logging threshold for mlr3 to warning level\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n\n# Load the German Credit dataset from rchallenge package\n# install rchallenge package if not install\ndata(\"german\", package = \"rchallenge\")\n\n# Create a classification task with target variable 'credit_risk'\ntask = as_task_classif(german, id = \"GermanCredit\", target = \"credit_risk\")\n\n# Define preprocessing steps as a pipeline\npreprocess &lt;- po(\"imputeoor\") %&gt;&gt;% \n  po(\"encodeimpact\", param_vals = list(impute_zero = T)) %&gt;&gt;%  \n  po(\"filter\", flt(\"auc\")) %&gt;&gt;%  \n  po(\"filter\", flt(\"find_correlation\", method = \"spearman\", use = \"na.or.complete\"))\n\n# Set parameter values for preprocessing steps\npreprocess$param_set$values$auc.filter.frac &lt;- 0.5\npreprocess$param_set$values$find_correlation.filter.frac &lt;- 0.5\n\n# Define the learner (LightGBM)\nlearner &lt;- lrn(\"classif.lightgbm\", num_iterations = 100)\n\n# Define the pipeline by combining preprocessing and the learner\npipeline &lt;- preprocess %&gt;&gt;% learner\n\n# Create a GraphLearner to encapsulate the pipeline\nmodel &lt;- GraphLearner$new(pipeline)\n\n# Train the model\nmodel$train(task)\n\n# Make predictions\npredictions &lt;- model$predict(task)",
    "crumbs": [
      "Python Snippet",
      "Convert the MLR3 LightGBM model from R to Python"
    ]
  },
  {
    "objectID": "python-snippet/convertLightGBMFromR2Python.html#step-1-extract-preprocessing",
    "href": "python-snippet/convertLightGBMFromR2Python.html#step-1-extract-preprocessing",
    "title": "Convert the MLR3 LightGBM model from R to Python",
    "section": "Step 1: Extract preprocessing",
    "text": "Step 1: Extract preprocessing\nExtract the results of tuning for imputeoor and encodeimpact steps from the model\nf_extract_impute &lt;- function(col){\n  val &lt;- model$state$model$imputeoor$model[col][[col]]\n  val\n}\nf_extract_encodeimpact &lt;- function(col) {\n  df &lt;- model$state$model$encodeimpact$impact[col]\n  df &lt;- as.data.frame(df)\n  df\n}\nNote: check final model in lightgbm model and only create preprocessing with them\n# Access the classif.lightgbm learner model\nmodel$state$model$classif.lightgbm\n (1000 x 10) * Target: credit_risk * Properties: twoclass * Features (9): - dbl (7): credit_history.good, employment_duration.good, housing.good, personal_status_sex.bad, purpose.good, savings.good, status.good - int (2): age, amount",
    "crumbs": [
      "Python Snippet",
      "Convert the MLR3 LightGBM model from R to Python"
    ]
  },
  {
    "objectID": "python-snippet/convertLightGBMFromR2Python.html#step-2-save-lightgbm-model",
    "href": "python-snippet/convertLightGBMFromR2Python.html#step-2-save-lightgbm-model",
    "title": "Convert the MLR3 LightGBM model from R to Python",
    "section": "Step 2: Save LightGBM model",
    "text": "Step 2: Save LightGBM model\n# Extract the trained LightGBM model from the pipeline\nlightgbm_model &lt;- model$state$model$classif.lightgbm$model\n\n# Specify the filename for saving the LightGBM model\nmodel_file &lt;- \"lightgbm_model.txt\"\n\n# Save the LightGBM model to a file\nlgb.save(lightgbm_model, model_file)",
    "crumbs": [
      "Python Snippet",
      "Convert the MLR3 LightGBM model from R to Python"
    ]
  },
  {
    "objectID": "python-snippet/convertLightGBMFromR2Python.html#step-3-create-preprocessing-function-in-python",
    "href": "python-snippet/convertLightGBMFromR2Python.html#step-3-create-preprocessing-function-in-python",
    "title": "Convert the MLR3 LightGBM model from R to Python",
    "section": "Step 3: Create preprocessing function in python",
    "text": "Step 3: Create preprocessing function in python\n# Function to impute values\ndef f_impute_values(missing_df):\n    # Select the desired columns in the specified order\n    sel_features = [\n          \"credit_history.good\", \"employment_duration.good\", \"housing.good\",\n            \"personal_status_sex.bad\", \"purpose.good\", \"savings.good\", \"status.good\",\n            \"age\", \"amount\"\n    ]\n    \n    # Sample data with feature names and impute values\n    impute_data = pd.DataFrame({\n        'featureName': [\"age\", \"amount\"],\n        'impute_value': [-38, -17925]\n    })\n\n    # Filter impute_data to include only feature names that exist in missing_df\n    impute_data = impute_data[impute_data['featureName'].isin(missing_df.columns)]\n\n    # Create a dictionary of feature names and their impute values\n    impute_dict = dict(zip(impute_data['featureName'], impute_data['impute_value']))\n\n    # Impute missing values in missing_df based on the impute_dict\n    missing_df.fillna(impute_dict, inplace=True)\n\n    # Select the desired columns in the specified order\n    missing_df = missing_df[sel_features]\n\n    return missing_df",
    "crumbs": [
      "Python Snippet",
      "Convert the MLR3 LightGBM model from R to Python"
    ]
  },
  {
    "objectID": "python-snippet/convertLightGBMFromR2Python.html#step-4-transfer-the-saved-model-file-lightgbm_model.txt-from-r-to-your-python-environment",
    "href": "python-snippet/convertLightGBMFromR2Python.html#step-4-transfer-the-saved-model-file-lightgbm_model.txt-from-r-to-your-python-environment",
    "title": "Convert the MLR3 LightGBM model from R to Python",
    "section": "Step 4: Transfer the saved model file (“lightgbm_model.txt”) from R to your Python environment",
    "text": "Step 4: Transfer the saved model file (“lightgbm_model.txt”) from R to your Python environment\nIn Python, use the LightGBM library to load the model from the saved file and make predictions. You’ll also need to load any necessary libraries and install LightGBM if you haven’t already:\nimport lightgbm as lgb\nimport pandas as pd\n\n# Load the saved LightGBM model from the file\nmodel = lgb.Booster(model_file='lightgbm_model.txt')\n\n# Load your new data for prediction as a pandas DataFrame\n# Replace 'new_data.csv' with the actual path to your data file\nnew_data = pd.read_csv('new_data.csv')\n\n# preprocessing\nimputed_df = f_impute_values(train_df)\n\n# Make predictions on the new data\npredictions = model.predict(imputed_df)\n\n# The 'predictions' variable now contains the model's predictions for the new data\nMake sure to replace 'new_data.csv' with the actual path to your new data file in the pd.read_csv line.\nWith these steps, you can load the MLR3 LightGBM model in Python, and then use it to make predictions on new data.",
    "crumbs": [
      "Python Snippet",
      "Convert the MLR3 LightGBM model from R to Python"
    ]
  },
  {
    "objectID": "python-snippet/JupyterNotebookFormat.html",
    "href": "python-snippet/JupyterNotebookFormat.html",
    "title": "Jupyter Notebook Python cell format",
    "section": "",
    "text": "import pandas as pd\nfrom IPython.display import HTML\nimport seaborn as sns\n\n# Load the Iris dataset\niris = sns.load_dataset(\"iris\")\n\n# Define a function to style the header cells and rows with accent color and Tahoma font\ndef style_dataframe(df, header_background_color=\"#5F497A\", header_text_color=\"white\", content_text_color=\"black\"):\n    styles = [\n        {\"selector\": \"thead th\", \"props\": [(\"background-color\", header_background_color), (\"color\", header_text_color), (\"font-weight\", \"bold\"), (\"font-family\", \"Tahoma, sans-serif\"), (\"font-size\", \"11px\")]},\n        {\"selector\": \"tbody tr:nth-child(odd)\", \"props\": [(\"background-color\", \"#f9f9f9\")]},\n        {\"selector\": \"tbody tr:nth-child(even)\", \"props\": [(\"background-color\", \"white\")]},\n        {\"selector\": \"tbody td\", \"props\": [(\"border\", \"1px solid lightgray\"), (\"font-family\", \"Tahoma, sans-serif\"), (\"font-size\", \"10px\"), (\"color\", content_text_color)]},\n    ]\n    return df.style.set_table_styles(styles).hide(axis=\"index\")\n\n# Apply the custom style function\nstyled_iris = style_dataframe(iris.head(5))\n\n# Display the styled DataFrame using the IPython display function\nstyled_iris\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n5.100000\n3.500000\n1.400000\n0.200000\nsetosa\n\n\n4.900000\n3.000000\n1.400000\n0.200000\nsetosa\n\n\n4.700000\n3.200000\n1.300000\n0.200000\nsetosa\n\n\n4.600000\n3.100000\n1.500000\n0.200000\nsetosa\n\n\n5.000000\n3.600000\n1.400000\n0.200000\nsetosa",
    "crumbs": [
      "Python Snippet",
      "Jupyter Notebook Python cell format"
    ]
  },
  {
    "objectID": "python-snippet/JupyterNotebookFormat.html#format-table",
    "href": "python-snippet/JupyterNotebookFormat.html#format-table",
    "title": "Jupyter Notebook Python cell format",
    "section": "",
    "text": "import pandas as pd\nfrom IPython.display import HTML\nimport seaborn as sns\n\n# Load the Iris dataset\niris = sns.load_dataset(\"iris\")\n\n# Define a function to style the header cells and rows with accent color and Tahoma font\ndef style_dataframe(df, header_background_color=\"#5F497A\", header_text_color=\"white\", content_text_color=\"black\"):\n    styles = [\n        {\"selector\": \"thead th\", \"props\": [(\"background-color\", header_background_color), (\"color\", header_text_color), (\"font-weight\", \"bold\"), (\"font-family\", \"Tahoma, sans-serif\"), (\"font-size\", \"11px\")]},\n        {\"selector\": \"tbody tr:nth-child(odd)\", \"props\": [(\"background-color\", \"#f9f9f9\")]},\n        {\"selector\": \"tbody tr:nth-child(even)\", \"props\": [(\"background-color\", \"white\")]},\n        {\"selector\": \"tbody td\", \"props\": [(\"border\", \"1px solid lightgray\"), (\"font-family\", \"Tahoma, sans-serif\"), (\"font-size\", \"10px\"), (\"color\", content_text_color)]},\n    ]\n    return df.style.set_table_styles(styles).hide(axis=\"index\")\n\n# Apply the custom style function\nstyled_iris = style_dataframe(iris.head(5))\n\n# Display the styled DataFrame using the IPython display function\nstyled_iris\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n5.100000\n3.500000\n1.400000\n0.200000\nsetosa\n\n\n4.900000\n3.000000\n1.400000\n0.200000\nsetosa\n\n\n4.700000\n3.200000\n1.300000\n0.200000\nsetosa\n\n\n4.600000\n3.100000\n1.500000\n0.200000\nsetosa\n\n\n5.000000\n3.600000\n1.400000\n0.200000\nsetosa",
    "crumbs": [
      "Python Snippet",
      "Jupyter Notebook Python cell format"
    ]
  },
  {
    "objectID": "python-snippet/Environment.html",
    "href": "python-snippet/Environment.html",
    "title": "Environment",
    "section": "",
    "text": "Environment setup\n\nUsing conda\n\nCreate conda environment\n\nQuick create env using conda create\nconda create -n env_rdm python=3.10 pip ipykernel notebook\nconda activate env_rdm\nCreate env use environment.yaml file\n\nFirst, create environment.yaml file with example content following\nSecond, run command conda env create -f environment.yaml\nOptional, remove environment if needed conda env remove -n env_ascore\n\nname: env_ascore\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.10\n  - pandas\n  - joblib\n  - statsmodels\n  - ipykernel\n  - zipp  \n  - pip\n  - pip: \n    - optbinning==0.17.3\n    - ortools==9.4.1874\n# set https_proxy=10.1.33.23:8080\n# set http_proxy=10.1.33.23:8080          \nCreate env use requirements.txt file\n\nconda list --export &gt; requirements.txt\nconda install --file requirements.txt\nConda common commands\n\n\nConda environment list\n\nconda info --env\n\nRemove conda environment\n\nconda deactivate\nconda env remove -n python38\n\nActivate conda environment\n\nconda activate python38\n\nClean unused library\n\nconda clean --all\npip cache remove *\nCreate environment for jupyter notebook\n\nUse conda to create new environment\nUse ipython\nconda activate python38\nipython kernel install --user --name=python38\nRemove jupyter notebook environment (require run as administrator)\njupyter kernelspec list\njupyter kernelspec uninstall python38 \n\nInstall packages in OFFLINE mode with pip\n\nInstall OFFLINE packages using requirements.txt\n\nStep 1: Input to requirements.txt file in current directory. Eg. content \"jupyter-contrib-nbextensions==0.5.1\"\n# export env if available\npip list --format=freeze &gt; requirements.txt\nStep 2: Create wheel folder (Eg. D:)\nStep 3: Run following command to download dependencies packages to folder wheel\n\npip download -r requirements.txt -d wheel\n\nStep 4: Run following command to install\n\npip install -r requirements.txt --find-links=D:\\wheel --no-index\nInstall OFFLINE Linux package\n\nCase 1: Activate same version python (i.e 3.7.0) and run following command\npip download --platform manylinux1_x86_64 --only-binary=:all: --no-binary=:none: pandas\nCase 2: Specify python version\npip download --platform manylinux1_x86_64 --only-binary=:all: --python-version=38 --no-binary=:none: pandas\n\n\nOther ultility commands\n\nCheck dependencies\n\npython -m pip check \npip freeze &gt; requirements.txt\n\nThemes jupyter notebook\n\njt -t onedork -fs 13 -altp -tfs 14 -nfs 14 -cellw 88% -T\n\nInstall Extension for jupyter notebook\n\npip install jupyter_contrib_nbextensions\npip install jupyter_nbextensions_configurator\njupyter contrib nbextension install --user\njupyter nbextensions_configurator enable --user",
    "crumbs": [
      "Python Snippet",
      "Environment"
    ]
  },
  {
    "objectID": "python-snippet/VisualizeFeatureImportances.html",
    "href": "python-snippet/VisualizeFeatureImportances.html",
    "title": "Visualize feature importances",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_feature_importance(importances, names):\n    plt.style.use('fivethirtyeight')\n    \n    feat_names = np.array(names)\n    indices = np.argsort(importances)[::-1]\n    \n    fig = plt.figure(figsize=(12, 8))\n    plt.title(\"Feature importances\")\n    plt.bar(range(len(indices)), importances[indices], color='lightblue', align=\"center\")\n    plt.step(range(len(indices)), np.cumsum(importances[indices]), where='mid', label='Cumulative')\n    plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical', fontsize=14)\n    plt.xlim([-1, len(indices)])\n    plt.show()",
    "crumbs": [
      "Python Snippet",
      "Visualize feature importances"
    ]
  },
  {
    "objectID": "python-snippet/VisualizeFeatureImportances.html#define-function-plot-feature-importance",
    "href": "python-snippet/VisualizeFeatureImportances.html#define-function-plot-feature-importance",
    "title": "Visualize feature importances",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_feature_importance(importances, names):\n    plt.style.use('fivethirtyeight')\n    \n    feat_names = np.array(names)\n    indices = np.argsort(importances)[::-1]\n    \n    fig = plt.figure(figsize=(12, 8))\n    plt.title(\"Feature importances\")\n    plt.bar(range(len(indices)), importances[indices], color='lightblue', align=\"center\")\n    plt.step(range(len(indices)), np.cumsum(importances[indices]), where='mid', label='Cumulative')\n    plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical', fontsize=14)\n    plt.xlim([-1, len(indices)])\n    plt.show()",
    "crumbs": [
      "Python Snippet",
      "Visualize feature importances"
    ]
  },
  {
    "objectID": "python-snippet/VisualizeFeatureImportances.html#get-example-data-german-credit",
    "href": "python-snippet/VisualizeFeatureImportances.html#get-example-data-german-credit",
    "title": "Visualize feature importances",
    "section": "Get example data (german credit)",
    "text": "Get example data (german credit)\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the German Credit Data\ndata_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\ncolumn_names = ['existing_checking', 'duration', 'credit_history', 'purpose', 'credit_amount', 'savings',\n                'employment', 'installment_rate', 'personal_status', 'other_debtors', 'residence_since',\n                'property', 'age', 'other_installment_plans', 'housing', 'existing_credits', 'job', 'people_liable',\n                'telephone', 'foreign_worker', 'class']\ndata = pd.read_csv(data_url, delimiter=' ', names=column_names)\n\n# Preprocess the data\nX = data.drop('class', axis=1)\ny = data['class']\nX = pd.get_dummies(X, drop_first=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ny_train[y_train == 2] = 0\ny_test[y_test == 2] = 0\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 21 columns):\n #   Column                   Non-Null Count  Dtype \n---  ------                   --------------  ----- \n 0   existing_checking        1000 non-null   object\n 1   duration                 1000 non-null   int64 \n 2   credit_history           1000 non-null   object\n 3   purpose                  1000 non-null   object\n 4   credit_amount            1000 non-null   int64 \n 5   savings                  1000 non-null   object\n 6   employment               1000 non-null   object\n 7   installment_rate         1000 non-null   int64 \n 8   personal_status          1000 non-null   object\n 9   other_debtors            1000 non-null   object\n 10  residence_since          1000 non-null   int64 \n 11  property                 1000 non-null   object\n 12  age                      1000 non-null   int64 \n 13  other_installment_plans  1000 non-null   object\n 14  housing                  1000 non-null   object\n 15  existing_credits         1000 non-null   int64 \n 16  job                      1000 non-null   object\n 17  people_liable            1000 non-null   int64 \n 18  telephone                1000 non-null   object\n 19  foreign_worker           1000 non-null   object\n 20  class                    1000 non-null   int64 \ndtypes: int64(8), object(13)\nmemory usage: 164.2+ KB\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n# Standardize features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)",
    "crumbs": [
      "Python Snippet",
      "Visualize feature importances"
    ]
  },
  {
    "objectID": "python-snippet/VisualizeFeatureImportances.html#visualize-feature-importances-using-a-lasso-regression",
    "href": "python-snippet/VisualizeFeatureImportances.html#visualize-feature-importances-using-a-lasso-regression",
    "title": "Visualize feature importances",
    "section": "Visualize feature importances using a Lasso regression",
    "text": "Visualize feature importances using a Lasso regression\n\nfrom sklearn.linear_model import LassoCV\n# Use LassoCV for feature selection\nmodel_lasso = LassoCV(alphas=[0.1, 1, 0.001, 0.0005]).fit(X_train_scaled, y_train)\n\n# Visualize feature importances using the function\nplot_feature_importance(np.abs(model_lasso.coef_), X_train.columns)",
    "crumbs": [
      "Python Snippet",
      "Visualize feature importances"
    ]
  },
  {
    "objectID": "python-snippet/VisualizeFeatureImportances.html#visualize-feature-importances-using-a-random-forest-classifier",
    "href": "python-snippet/VisualizeFeatureImportances.html#visualize-feature-importances-using-a-random-forest-classifier",
    "title": "Visualize feature importances",
    "section": "Visualize feature importances using a Random Forest classifier",
    "text": "Visualize feature importances using a Random Forest classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Use RandomForestClassifier for feature importance\nclf = RandomForestClassifier(n_estimators=10, random_state=123)\nclf.fit(X_train_scaled, y_train)\n\nnames = X_train.columns\nimportances = clf.feature_importances_\n\n# Call the plotting function\nplot_feature_importance(importances, names)",
    "crumbs": [
      "Python Snippet",
      "Visualize feature importances"
    ]
  },
  {
    "objectID": "python-snippet/VisualizeFeatureImportances.html#visualize-feature-importances-using-a-lightgbm-classifier",
    "href": "python-snippet/VisualizeFeatureImportances.html#visualize-feature-importances-using-a-lightgbm-classifier",
    "title": "Visualize feature importances",
    "section": "Visualize feature importances using a LightGBM classifier",
    "text": "Visualize feature importances using a LightGBM classifier\n\nimport lightgbm as lgb\n\n# Use LightGBM classifier\nclf = lgb.LGBMClassifier(n_estimators=100, random_state=123)\nclf.fit(X_train_scaled, y_train)\n\n# Get feature importances\nimportances = clf.feature_importances_\nnames = X_train.columns\n\n# Call the plotting function\nplot_feature_importance(importances, names)",
    "crumbs": [
      "Python Snippet",
      "Visualize feature importances"
    ]
  },
  {
    "objectID": "python-snippet/VisualizeFeatureImportances.html#visualize-feature-importances-using-a-xgboost-classifier",
    "href": "python-snippet/VisualizeFeatureImportances.html#visualize-feature-importances-using-a-xgboost-classifier",
    "title": "Visualize feature importances",
    "section": "Visualize feature importances using a XGBoost classifier",
    "text": "Visualize feature importances using a XGBoost classifier\n\nimport xgboost as xgb\n\n# Use XGBoost classifier\nclf = xgb.XGBClassifier(n_estimators=100, random_state=123)\nclf.fit(X_train_scaled, y_train)\n\n# Get feature importances\nimportances = clf.feature_importances_\nnames = X_train.columns\n\n# Call the plotting function\nplot_feature_importance(importances, names)",
    "crumbs": [
      "Python Snippet",
      "Visualize feature importances"
    ]
  },
  {
    "objectID": "python-snippet/beeswarm_strip_violin_plot.html",
    "href": "python-snippet/beeswarm_strip_violin_plot.html",
    "title": "Swarm-violin plot",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef generate_swarm_violin(data, x, y, hue):\n    \n    # Set figure size\n    plt.figure(figsize=(10,6))\n\n    # Create the beeswarm plot with different colors for each day and time\n    ax = sns.swarmplot(x=x, y=y, data=data, palette=\"Set1\", hue=hue, alpha=0.3, size=10)\n\n    # Add violin plot to the same axes\n    sns.violinplot(x=x, y=y, data=data, inner=None, color=\"white\", ax=ax)\n\n    # Hide the legend\n    ax.legend_.remove()\n\n    # Add title and labels\n    plt.title(f\"{y} by {x} ({hue})\")\n    plt.xlabel(x)\n    plt.ylabel(y)\n\n    # Show the plot\n    plt.show()\n\n\ngenerate_swarm_violin(data=tips, x=\"day\", y=\"total_bill\", hue=\"time\")",
    "crumbs": [
      "Python Snippet",
      "Swarm-violin plot"
    ]
  },
  {
    "objectID": "python-snippet/beeswarm_strip_violin_plot.html#swarm-violin-plot",
    "href": "python-snippet/beeswarm_strip_violin_plot.html#swarm-violin-plot",
    "title": "Swarm-violin plot",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef generate_swarm_violin(data, x, y, hue):\n    \n    # Set figure size\n    plt.figure(figsize=(10,6))\n\n    # Create the beeswarm plot with different colors for each day and time\n    ax = sns.swarmplot(x=x, y=y, data=data, palette=\"Set1\", hue=hue, alpha=0.3, size=10)\n\n    # Add violin plot to the same axes\n    sns.violinplot(x=x, y=y, data=data, inner=None, color=\"white\", ax=ax)\n\n    # Hide the legend\n    ax.legend_.remove()\n\n    # Add title and labels\n    plt.title(f\"{y} by {x} ({hue})\")\n    plt.xlabel(x)\n    plt.ylabel(y)\n\n    # Show the plot\n    plt.show()\n\n\ngenerate_swarm_violin(data=tips, x=\"day\", y=\"total_bill\", hue=\"time\")",
    "crumbs": [
      "Python Snippet",
      "Swarm-violin plot"
    ]
  },
  {
    "objectID": "python-snippet/beeswarm_strip_violin_plot.html#strip-violin-plot",
    "href": "python-snippet/beeswarm_strip_violin_plot.html#strip-violin-plot",
    "title": "Swarm-violin plot",
    "section": "Strip-violin plot",
    "text": "Strip-violin plot\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef generate_strip_violin(data, x, y, hue):\n    # Set figure size\n    plt.figure(figsize=(7,5))\n\n    # Create the strip plot with different colors for each category\n    ax = sns.stripplot(x=x, y=y, data=data, palette=\"Set1\", hue=hue, alpha=0.3, size=10)\n\n    # Add violin plot to the same axes\n    sns.violinplot(x=x, y=y, data=data, inner=None, color=\"white\", ax=ax)\n\n    # Hide the legend\n    ax.legend_.remove()\n\n    # Add title and labels\n    plt.title(f\"{y} by {x} ({hue})\")\n    plt.xlabel(x)\n    plt.ylabel(y)\n\n    # Show the plot\n    plt.show()\n\n\n# Load the data\ntips = sns.load_dataset(\"tips\")\n\ngenerate_strip_violin(data=tips, x=\"day\", y=\"total_bill\", hue=\"time\")",
    "crumbs": [
      "Python Snippet",
      "Swarm-violin plot"
    ]
  },
  {
    "objectID": "python-snippet/beeswarm_strip_violin_plot.html#strip-violin-plot-with-percentage-of-category",
    "href": "python-snippet/beeswarm_strip_violin_plot.html#strip-violin-plot-with-percentage-of-category",
    "title": "Swarm-violin plot",
    "section": "Strip-violin plot with percentage of category",
    "text": "Strip-violin plot with percentage of category\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef generate_strip_violin(data, x, y, hue):\n    # Set figure size\n    plt.figure(figsize=(10, 6))\n\n    # Create the strip plot with different colors for each category\n    ax = sns.stripplot(x=x, y=y, data=data, palette=\"Set1\", hue=hue, alpha=0.3, size=10)\n\n    # Add violin plot to the same axes\n    sns.violinplot(x=x, y=y, data=data, inner=None, color=\"white\", ax=ax)\n\n    # Get unique categories in the hue variable\n    unique_categories = data[hue].unique()\n\n    # Add percentage labels to the plot for each category\n    for i, category in enumerate(unique_categories):\n        # Get the collection for the current category\n        collection = ax.collections[i]\n        \n        # Get the x and y positions of the points\n        x_pos = collection.get_offsets()[:, 0]\n        y_pos = collection.get_offsets()[:, 1]\n\n        # Calculate the percentage of the category in the data\n        percentage = len(data.loc[(data[x] == category) & (data[hue] == category)]) / len(data) * 100\n        text = f\"{percentage:.1f}%\"\n        ax.annotate(text, xy=(x_pos.mean(), y_pos.mean()), fontsize=10, ha='center', va='center')\n        \n\n    # Hide the legend\n    ax.legend_.remove()\n\n    # Add title and labels\n    plt.title(f\"{y} by {x} ({hue})\")\n    plt.xlabel(x)\n    plt.ylabel(y)\n\n    # Show the plot\n    plt.show()\n    \n\n# Load the data\ntips = sns.load_dataset(\"tips\")\n\ngenerate_strip_violin(data=tips, x=\"day\", y=\"total_bill\", hue=\"day\")",
    "crumbs": [
      "Python Snippet",
      "Swarm-violin plot"
    ]
  },
  {
    "objectID": "python-snippet/beeswarm_strip_violin_plot.html#strip-violin-plot-1",
    "href": "python-snippet/beeswarm_strip_violin_plot.html#strip-violin-plot-1",
    "title": "Swarm-violin plot",
    "section": "Strip-violin plot",
    "text": "Strip-violin plot\n\nadd percentage of category\nadd mean value\nadd line graph\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef generate_strip_violin(data, x, y, hue):\n    # Set figure size\n    plt.figure(figsize=(10, 6))\n\n    # Create the strip plot with different colors for each category\n    ax = sns.stripplot(x=x, y=y, data=data, palette=\"Set1\", hue=hue, alpha=0.3, size=10)\n\n    # Add violin plot to the same axes\n    sns.violinplot(x=x, y=y, data=data, inner=None, color=\"white\", ax=ax)\n\n    # Get unique categories in the hue variable\n    unique_categories = data[hue].unique()\n\n    # Add percentage labels to the plot for each category\n    xline = []\n    yline = []\n    for i, category in enumerate(unique_categories):\n        # Get the collection for the current category\n        collection = ax.collections[i]\n        \n        # Get the x and y positions of the points\n        x_pos = collection.get_offsets()[:, 0]\n        y_pos = collection.get_offsets()[:, 1]\n        xline.append(x_pos.mean())\n        # Calculate the percentage of the category in the data\n        percentage = len(data.loc[(data[x] == category) & (data[hue] == category)]) / len(data) * 100\n        \n        # Calculate the mean of the 'numeric_column' within the current category\n        mean_value = data[data[hue] == category][y].mean()        \n        yline.append(y_pos.mean())\n        #text = f\"{percentage:.1f}%\"\n        text = f\"(Perc: {percentage:.1f}%) \\n(Mean: {y_pos.mean():.2f})\"\n        ax.annotate(text, xy=(x_pos.mean(), y_pos.mean()), fontsize=10, ha='center', va='center')\n        \n    sns.lineplot(x=xline, y=yline, markers=True, linewidth=3, dashes = True, marker='o')\n    \n    # Hide the legend\n    ax.legend_.remove()\n\n    # Add title and labels\n    plt.title(f\"{y} by {x} ({hue})\")\n    plt.xlabel(x)\n    plt.ylabel(y)\n\n    # Show the plot\n    plt.show()\n\n# Load the data\ntips = sns.load_dataset(\"tips\")\n\ngenerate_strip_violin(data=tips, x=\"day\", y=\"total_bill\", hue=\"day\")",
    "crumbs": [
      "Python Snippet",
      "Swarm-violin plot"
    ]
  },
  {
    "objectID": "python-snippet/beeswarm_strip_violin_plot.html#plot-correlation-between-2-categorical-variables",
    "href": "python-snippet/beeswarm_strip_violin_plot.html#plot-correlation-between-2-categorical-variables",
    "title": "Swarm-violin plot",
    "section": "Plot correlation between 2 categorical variables",
    "text": "Plot correlation between 2 categorical variables\n\nimport pandas as pd\nimport numpy as np\n\n# Generate random data\nnp.random.seed(0)\n\n# Number of data points\nn = 1000\n\n# Create a DataFrame\ndata = pd.DataFrame({\n    'ltv': np.random.uniform(0, 1, n),  # Random 'ltv' values between 0 and 1\n    'rating': np.random.choice(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'], n)\n})\n\n# Calculate the 'ltv_decile' using quantiles\ndecile_labels = [f'[{i/10:.1f}, {(i+1)/10:.1f})' for i in range(10)]\ndata['ltv_decile'] = pd.qcut(data['ltv'], 10, labels=decile_labels)\n\n# Display the first few rows of the DataFrame\nprint(data.head())\n\n        ltv rating  ltv_decile\n0  0.548814      B  [0.5, 0.6)\n1  0.715189      G  [0.7, 0.8)\n2  0.602763      F  [0.6, 0.7)\n3  0.544883      E  [0.5, 0.6)\n4  0.423655      E  [0.4, 0.5)\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_rating_ltv(data, category_var, numeric_var, num_quantiles, list_categories):\n    # Make a copy of the input data and filter it based on the list of categories\n    data = data.copy()\n    data = data[data[category_var].isin(list_categories)]\n    \n    # Generate the 'decile' column name based on numeric_var and num_quantiles\n    decile_col_name = f'{numeric_var}_decile_{num_quantiles}'\n    \n    # Calculate the 'decile' using quantiles and the generated column name\n    decile_labels = [f'[{i/num_quantiles:.1f}, {(i+1)/num_quantiles:.1f})' for i in range(num_quantiles)]\n    data[decile_col_name] = pd.qcut(data[numeric_var], num_quantiles, labels=decile_labels)\n\n    # Group the data by 'rating' and 'ltv_decile' and count the occurrences\n    rating_ltv_counts = data.groupby([category_var, decile_col_name]).size().unstack(fill_value=0)\n\n    # Normalize the counts to percentages within each 'rating' group\n    rating_ltv_percentages = rating_ltv_counts.div(rating_ltv_counts.sum(axis=1), axis=0) * 100\n\n    # Normalize the counts to cumulative percentages within each 'rating' group\n    rating_ltv_cumulative = rating_ltv_counts.cumsum(axis=1).div(rating_ltv_counts.sum(axis=1), axis=0) * 100\n\n    rating_ltv_position = rating_ltv_cumulative.rolling(window=2, axis=1, min_periods=1, center=True).mean().fillna(0)\n\n    # Plot a bar chart\n    ax = rating_ltv_percentages.plot(kind='bar', stacked=True, figsize=(10, 6))\n\n    # Add percentage labels to each segment\n    for column_name, column in rating_ltv_position.items():\n        for x, y in enumerate(column):\n            if y != 0:  # Exclude labels for segments with 0%\n                percentage = rating_ltv_percentages[column_name].values[x]\n                ax.text(x, y, f'{percentage:.1f}%', ha='center', va='top', fontsize=8)\n\n    ax.set_ylabel(\"Percentage\")\n    ax.set_xlabel(category_var)\n    ax.set_title(f\"Percentage of {decile_col_name} by {category_var}\")\n\n    # Move the legend to the right\n    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\n    # Add line plots for each category\n    for category in rating_ltv_position.columns:\n        x = rating_ltv_percentages[category]\n        y = rating_ltv_position[category]\n        ax.plot(x.index, y.values, label=f'{category_var} {category}', marker='o')\n\n    # Display the plot\n    plt.show()\n\n# Example usage:\nplot_rating_ltv(data, 'rating', 'ltv', 5, ['A', 'B', 'C'])",
    "crumbs": [
      "Python Snippet",
      "Swarm-violin plot"
    ]
  },
  {
    "objectID": "python-snippet/Ultilities.html",
    "href": "python-snippet/Ultilities.html",
    "title": "Ultilities",
    "section": "",
    "text": "Mouse click\nfrom pynput.mouse import Controller, Button\nimport time\nmouse = Controller()\nwhile True:\n  mouse.click(Button.left, 1)\n  print('clicked')  \n  time.sleep(5)",
    "crumbs": [
      "Python Snippet",
      "Ultilities"
    ]
  },
  {
    "objectID": "python-snippet/REST_API.html",
    "href": "python-snippet/REST_API.html",
    "title": "REST API",
    "section": "",
    "text": "Chỉnh sửa lại url, headers, data\n\nimport requests\n\n# set the API endpoint and headers\nurl = 'https://api.example.com/post'\nheaders = {'Content-type': 'application/json', 'Authorization': 'Bearer your_access_token'}\n\n# set the data to be sent in the request\ndata = {'name': 'John Doe', 'email': 'johndoe@example.com'}\n\n# send the POST request using the Requests library\nresponse = requests.post(url, headers=headers, json=data)\n\n# verify the response\nprint(response.status_code)\nprint(response.text)",
    "crumbs": [
      "Python Snippet",
      "REST API"
    ]
  },
  {
    "objectID": "python-snippet/REST_API.html#kiểm-tra-api-sử-dụng-post-method",
    "href": "python-snippet/REST_API.html#kiểm-tra-api-sử-dụng-post-method",
    "title": "REST API",
    "section": "",
    "text": "Chỉnh sửa lại url, headers, data\n\nimport requests\n\n# set the API endpoint and headers\nurl = 'https://api.example.com/post'\nheaders = {'Content-type': 'application/json', 'Authorization': 'Bearer your_access_token'}\n\n# set the data to be sent in the request\ndata = {'name': 'John Doe', 'email': 'johndoe@example.com'}\n\n# send the POST request using the Requests library\nresponse = requests.post(url, headers=headers, json=data)\n\n# verify the response\nprint(response.status_code)\nprint(response.text)",
    "crumbs": [
      "Python Snippet",
      "REST API"
    ]
  },
  {
    "objectID": "python-snippet/Graphs.html",
    "href": "python-snippet/Graphs.html",
    "title": "Graphs",
    "section": "",
    "text": "Candlestick\nimport yfinance as yf \ndata = yf.download(tickers = 'ETH-USD', period = 'max', interval = '1d')\ndfpl = data[0:100]\nimport plotly.graph_objects as go\nfig = go.Figure(data=go.Candlestick(\n    x = dfpl.index,\n    open = dfpl.Open,\n    high = dfpl.High,\n    low = dfpl.Low,\n    close = dfpl.Close    \n))\n\nfig.show()",
    "crumbs": [
      "Python Snippet",
      "Graphs"
    ]
  },
  {
    "objectID": "python-snippet/Statistics.html",
    "href": "python-snippet/Statistics.html",
    "title": "Statistic",
    "section": "",
    "text": "# https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation/notebook\n\nfrom scipy import stats\nfeatures_list = X_test.columns.values.tolist()\nfor feature in features_list:\n    statistic, pvalue = stats.kstest(X_train[feature], X_test[feature])\n    print(\"p-value %.2f\" %pvalue, \"for the feature\",feature)",
    "crumbs": [
      "Python Snippet",
      "Statistic"
    ]
  },
  {
    "objectID": "python-snippet/Statistics.html#ks-test",
    "href": "python-snippet/Statistics.html#ks-test",
    "title": "Statistic",
    "section": "",
    "text": "# https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation/notebook\n\nfrom scipy import stats\nfeatures_list = X_test.columns.values.tolist()\nfor feature in features_list:\n    statistic, pvalue = stats.kstest(X_train[feature], X_test[feature])\n    print(\"p-value %.2f\" %pvalue, \"for the feature\",feature)",
    "crumbs": [
      "Python Snippet",
      "Statistic"
    ]
  },
  {
    "objectID": "python-snippet/Statistics.html#lightgbm-focal-loss",
    "href": "python-snippet/Statistics.html#lightgbm-focal-loss",
    "title": "Statistic",
    "section": "2.1 Lightgbm focal loss",
    "text": "2.1 Lightgbm focal loss\n# https://maxhalford.github.io/blog/lightgbm-focal-loss/",
    "crumbs": [
      "Python Snippet",
      "Statistic"
    ]
  },
  {
    "objectID": "python-snippet/Statistics.html#metric-processing-time",
    "href": "python-snippet/Statistics.html#metric-processing-time",
    "title": "Statistic",
    "section": "3.1 Metric Processing Time",
    "text": "3.1 Metric Processing Time\nhttps://www.kaggle.com/code/rohanrao/amex-competition-metric-implementations",
    "crumbs": [
      "Python Snippet",
      "Statistic"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Snippets",
    "section": "",
    "text": "Welcome to the Snippets section! Here, you’ll find a collection of useful comparisons, code snippets, and resources to help you in your coding journey. Whether you’re a Python enthusiast, an R aficionado, or working with SQL Server, we’ve got you covered. Explore our curated snippets below:\n\n\n\nPL/SQL vs. DB2: Delve into the differences between PL/SQL and DB2 to make informed database decisions.\ndplyr vs. pandas: Compare the data manipulation capabilities of dplyr in R with pandas in Python.\nggplot2 vs. matplotlib: Explore the world of data visualization by contrasting ggplot2 and matplotlib.\n\n\n\n\n\nEnvironment Setup: Learn how to set up your Python environment for efficient coding.\nGraphs: Dive into the world of graph creation and visualization with Python.\nSlopegraph: Explore the power of slopegraphs in data storytelling.\nModeling: Get hands-on with Python’s modeling capabilities for data analysis.\nStatistics: Brush up on statistical concepts and techniques in Python.\nCode Optimization: Discover tips and tricks for optimizing your Python code.\nUtilities: Explore various utility functions and tools to streamline your Python development.\nREST API: Learn how to work with REST APIs in Python.\nJupyter Notebook Format: Dive into the world of Jupyter Notebooks and their formatting.\nVisualize Feature Importances: Gain insights into feature importance visualization in Python.\n\n\n\n\n\ndplyr: Master the art of data manipulation in R using the dplyr package.\nggplot2: Create stunning data visualizations with ggplot2 in R.\ngganimate: Animate your ggplot2 visualizations for dynamic data exploration.\nTreemap: Visualize hierarchical data using treemaps in R.\nDatabase Operations: Explore R’s capabilities for working with databases.\nUsing reticulate: Integrate Python into your R workflow with the reticulate package.\nWorking with Files and Folders: Learn how to efficiently handle files and folders in R.\nData Tables: Dive into data manipulation with data tables in R.\nCreating Packages: Discover how to create and manage packages in R.\n\n\n\n\n\nGeneral Info: Get an overview of SQL Server and its features.\nPermissions: Understand SQL Server permissions and access control.\nSynonyms: Learn about using synonyms in SQL Server for simplifying object references.\n\n\n\n\n\nTransformer Model: Explore the powerful Transformer model used in natural language processing and machine translation.\n\nHappy coding and exploring! Feel free to click on any of the links above to start your coding journey with these insightful snippets and comparisons.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#comparisons",
    "href": "index.html#comparisons",
    "title": "Snippets",
    "section": "",
    "text": "PL/SQL vs. DB2: Delve into the differences between PL/SQL and DB2 to make informed database decisions.\ndplyr vs. pandas: Compare the data manipulation capabilities of dplyr in R with pandas in Python.\nggplot2 vs. matplotlib: Explore the world of data visualization by contrasting ggplot2 and matplotlib.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#python-snippets",
    "href": "index.html#python-snippets",
    "title": "Snippets",
    "section": "",
    "text": "Environment Setup: Learn how to set up your Python environment for efficient coding.\nGraphs: Dive into the world of graph creation and visualization with Python.\nSlopegraph: Explore the power of slopegraphs in data storytelling.\nModeling: Get hands-on with Python’s modeling capabilities for data analysis.\nStatistics: Brush up on statistical concepts and techniques in Python.\nCode Optimization: Discover tips and tricks for optimizing your Python code.\nUtilities: Explore various utility functions and tools to streamline your Python development.\nREST API: Learn how to work with REST APIs in Python.\nJupyter Notebook Format: Dive into the world of Jupyter Notebooks and their formatting.\nVisualize Feature Importances: Gain insights into feature importance visualization in Python.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#r-snippets",
    "href": "index.html#r-snippets",
    "title": "Snippets",
    "section": "",
    "text": "dplyr: Master the art of data manipulation in R using the dplyr package.\nggplot2: Create stunning data visualizations with ggplot2 in R.\ngganimate: Animate your ggplot2 visualizations for dynamic data exploration.\nTreemap: Visualize hierarchical data using treemaps in R.\nDatabase Operations: Explore R’s capabilities for working with databases.\nUsing reticulate: Integrate Python into your R workflow with the reticulate package.\nWorking with Files and Folders: Learn how to efficiently handle files and folders in R.\nData Tables: Dive into data manipulation with data tables in R.\nCreating Packages: Discover how to create and manage packages in R.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#sql-server-snippets",
    "href": "index.html#sql-server-snippets",
    "title": "Snippets",
    "section": "",
    "text": "General Info: Get an overview of SQL Server and its features.\nPermissions: Understand SQL Server permissions and access control.\nSynonyms: Learn about using synonyms in SQL Server for simplifying object references.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#machine-learning",
    "href": "index.html#machine-learning",
    "title": "Snippets",
    "section": "",
    "text": "Transformer Model: Explore the powerful Transformer model used in natural language processing and machine translation.\n\nHappy coding and exploring! Feel free to click on any of the links above to start your coding journey with these insightful snippets and comparisons.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/09-Pandas-Exercises-2.html",
    "href": "Crash-Course-Pandas/09-Pandas-Exercises-2.html",
    "title": "1. Introduction",
    "section": "",
    "text": "For this project we were asked to select a dataset and using the data answer a question of our choosing. I selected the Titanic Data Set which looks at the characteristics of a sample of the passengers on the Titanic, including whether they survived or not, gender, age, siblings / spouses, parents and children, fare (cost of ticket), embarkation port.\nAfter looking at the contents of the dataset, I thought it would be interesting to look at the following questions:\n\nWhich gender had a better chance of survival?\nWhich social class had a better chance of survival?\nWhich age group had a better chance of survival?\n\n\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\nTitanic Data - Contains demographics and passenger information from 891 of the 2224 passengers and crew on board the Titanic.\n\n\n\n\n\n\n\n\nVariable\nDefinition\nKey\n\n\n\n\nSurvived\nSurvival\n0 = No, 1 = Yes\n\n\nPclass\nTicket class\n1 = 1st, 2 = 2nd, 3 = 3rd\n\n\nSex\nSex\n\n\n\nAge\nAge in years\n\n\n\nSibsp\nNumber of siblings / spouses aboard the Titanic\n\n\n\nParch\nNumber of parents / children aboard the Titanic\n\n\n\nTicket\nTicket number\n\n\n\nFare\nPassenger fare\n\n\n\nCabin\nCabin number\n\n\n\nEmbarked\nPort of Embarkation\nC = Cherbourg, Q = Queenstown,S = Southampton\n\n\n\nPclass: A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower\nAge: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\nSibsp: The dataset defines family relations in this way…  Sibling = brother, sister, stepbrother, stepsister  Spouse = husband, wife (mistresses and fiancés were ignored)\nParch: The dataset defines family relations in this way…  Parent = mother, father  Child = daughter, son, stepdaughter, stepson  Some children travelled only with a nanny, therefore parch=0 for them.",
    "crumbs": [
      "Introduction to Pandas",
      "1. Introduction"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#dataset-information-data-dictionaryvariable-notes",
    "href": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#dataset-information-data-dictionaryvariable-notes",
    "title": "1. Introduction",
    "section": "",
    "text": "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\nTitanic Data - Contains demographics and passenger information from 891 of the 2224 passengers and crew on board the Titanic.\n\n\n\n\n\n\n\n\nVariable\nDefinition\nKey\n\n\n\n\nSurvived\nSurvival\n0 = No, 1 = Yes\n\n\nPclass\nTicket class\n1 = 1st, 2 = 2nd, 3 = 3rd\n\n\nSex\nSex\n\n\n\nAge\nAge in years\n\n\n\nSibsp\nNumber of siblings / spouses aboard the Titanic\n\n\n\nParch\nNumber of parents / children aboard the Titanic\n\n\n\nTicket\nTicket number\n\n\n\nFare\nPassenger fare\n\n\n\nCabin\nCabin number\n\n\n\nEmbarked\nPort of Embarkation\nC = Cherbourg, Q = Queenstown,S = Southampton\n\n\n\nPclass: A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower\nAge: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\nSibsp: The dataset defines family relations in this way…  Sibling = brother, sister, stepbrother, stepsister  Spouse = husband, wife (mistresses and fiancés were ignored)\nParch: The dataset defines family relations in this way…  Parent = mother, father  Child = daughter, son, stepdaughter, stepson  Some children travelled only with a nanny, therefore parch=0 for them.",
    "crumbs": [
      "Introduction to Pandas",
      "1. Introduction"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#investigating-dataset-and-finding-missing-incomplete-data",
    "href": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#investigating-dataset-and-finding-missing-incomplete-data",
    "title": "1. Introduction",
    "section": "4.1 Investigating Dataset and finding missing / incomplete data",
    "text": "4.1 Investigating Dataset and finding missing / incomplete data\n\n# print out information about the data\ntitanic_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\nAfter printing out the dataset information above, we can see that the Age, Cabin and Embarked columns are missing entries. As the Cabin column is not relevant to the analysis of the data I will be removing that column however I will need to find a way update populate the missing ages and embarked port.",
    "crumbs": [
      "Introduction to Pandas",
      "1. Introduction"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#missing-ages",
    "href": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#missing-ages",
    "title": "1. Introduction",
    "section": "4.2 Missing Ages",
    "text": "4.2 Missing Ages\nIn order to populate the missing ages I will use the mean age based on the Sex and Pclass\n\n\nmissing_ages = titanic_df[titanic_df['Age'].isnull()]\n# determine mean age based on Sex and Pclass\nmean_ages = titanic_df.groupby(['Sex','Pclass'])['Age'].mean()\n\ndef remove_na_ages(row):\n    '''\n    function to check if the age is null and replace wth the mean from \n    the mean ages dataframe \n    '''\n    if pd.isnull(row['Age']):\n        return mean_ages[row['Sex'],row['Pclass']]\n    else:\n        return row['Age']\n\ntitanic_df['Age'] = titanic_df.apply(remove_na_ages, axis=1)",
    "crumbs": [
      "Introduction to Pandas",
      "1. Introduction"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#missing-embarkation-ports",
    "href": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#missing-embarkation-ports",
    "title": "1. Introduction",
    "section": "4.3 Missing embarkation ports",
    "text": "4.3 Missing embarkation ports\nIn order to populate the missing embarked ports I need to first determine if the people with the missing information may have been travelling with others.\n\nmissing_ports = titanic_df[titanic_df['Embarked'].isnull()]\nmissing_ports\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n61\n62\n1\n1\nIcard, Miss. Amelie\nfemale\n38.0\n0\n0\n113572\n80.0\nB28\nNaN\n\n\n829\n830\n1\n1\nStone, Mrs. George Nelson (Martha Evelyn)\nfemale\n62.0\n0\n0\n113572\n80.0\nB28\nNaN\n\n\n\n\n\n\n\n\n\n# search by ticket number and cabin\ntitanic_df[(titanic_df['Embarked'].notnull()) & ((titanic_df['Ticket'] == '113572') | (titanic_df['Cabin'] == 'B28'))]\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n\n\n\n\n\n\nSince searching for similar records did not return any results and it appears that both were travelling in the same cabin and with the same ticket number and the bulk of passengers were travelling from Southhampton, I have choosen to use Southhampton as the missing value.\n\ntitanic_df['Embarked'].fillna('S',inplace=True)",
    "crumbs": [
      "Introduction to Pandas",
      "1. Introduction"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#remove-un-wanted-columns",
    "href": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#remove-un-wanted-columns",
    "title": "1. Introduction",
    "section": "4.4 Remove un-wanted columns",
    "text": "4.4 Remove un-wanted columns\nSince the Cabin, Name and Ticket numbers are not required in this analysis I will remove them to improve the speed of processing the dataframe.\n\n# dropping columns Cabin, Name and Ticket\ntitanic_df = titanic_df.drop(['Cabin','Name','Ticket'], axis=1)\ntitanic_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 9 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Sex          891 non-null    object \n 4   Age          891 non-null    float64\n 5   SibSp        891 non-null    int64  \n 6   Parch        891 non-null    int64  \n 7   Fare         891 non-null    float64\n 8   Embarked     891 non-null    object \ndtypes: float64(2), int64(5), object(2)\nmemory usage: 62.8+ KB",
    "crumbs": [
      "Introduction to Pandas",
      "1. Introduction"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#mapping-data-values-to-descriptions",
    "href": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#mapping-data-values-to-descriptions",
    "title": "1. Introduction",
    "section": "4.5 Mapping data (values to descriptions)",
    "text": "4.5 Mapping data (values to descriptions)\nIn order to intrepret the data easier the following fields need to be modified:\nSurvived - changed to boolean (1 = True, 0 = False) Pclass - changed to Socio-Economic status (1st - Upper Class, 2nd - Middle Class, 3rd - Lower Class) Embarked - changed to name of embarkation port (C = Cherbourg; Q = Queenstown; S = Southampton) I will also add a Family Size column so that I can compare the size of families with the number of survivors.\n\ndef map_data(df):\n    '''\n    Function which takes the original dataframe and returns a \n    clean / updated dataframe\n    '''\n    # survived map\n    survived_map = {0: False, 1: True}\n    df['Survived'] = df['Survived'].map(survived_map)\n\n    # PClass map\n    pclass_map = {1: 'Upper Class', 2: 'Middle Class', 3: 'Lower Class'}\n    df['Pclass'] = df['Pclass'].map(pclass_map)\n\n    # Embarkation port map\n    port_map = {'S': 'Southampton', 'C': 'Cherbourg','Q':'Queenstown'}\n    df['Embarked'] = df['Embarked'].map(port_map)\n    \n    # add new column (FamilySize) to dataframe - sum of SibSp and Parch\n    df['FamilySize'] = df['SibSp'] + df['Parch']\n    \n    return df\n\ntitanic_df = map_data(titanic_df)\ntitanic_df.head(3)\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nSex\nAge\nSibSp\nParch\nFare\nEmbarked\nFamilySize\n\n\n\n\n0\n1\nFalse\nLower Class\nmale\n22.0\n1\n0\n7.2500\nSouthampton\n1\n\n\n1\n2\nTrue\nUpper Class\nfemale\n38.0\n1\n0\n71.2833\nCherbourg\n1\n\n\n2\n3\nTrue\nLower Class\nfemale\n26.0\n0\n0\n7.9250\nSouthampton\n0",
    "crumbs": [
      "Introduction to Pandas",
      "1. Introduction"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#grouping-binning-ages",
    "href": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#grouping-binning-ages",
    "title": "1. Introduction",
    "section": "4.6 Grouping / Binning Ages",
    "text": "4.6 Grouping / Binning Ages\nTo make the ages easier to analyse I thought it would be a good idea to group / bin the ages. This way we can compare groups of ages instead of individual ages.\n\nage_labels = ['0-9', '10-19', '20-29', '30-39', '40-49', '50-59', '60-69', '70-79']\ntitanic_df['age_group'] = pd.cut(titanic_df.Age, range(0, 81, 10), right=False, labels=age_labels)\ntitanic_df.head(3)\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nSex\nAge\nSibSp\nParch\nFare\nEmbarked\nFamilySize\nage_group\n\n\n\n\n0\n1\nFalse\nLower Class\nmale\n22.0\n1\n0\n7.2500\nSouthampton\n1\n20-29\n\n\n1\n2\nTrue\nUpper Class\nfemale\n38.0\n1\n0\n71.2833\nCherbourg\n1\n30-39\n\n\n2\n3\nTrue\nLower Class\nfemale\n26.0\n0\n0\n7.9250\nSouthampton\n0\n20-29",
    "crumbs": [
      "Introduction to Pandas",
      "1. Introduction"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#number-of-survivors",
    "href": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#number-of-survivors",
    "title": "1. Introduction",
    "section": "5.1 Number of Survivors",
    "text": "5.1 Number of Survivors\n\nsurvivors_data = titanic_df[titanic_df.Survived==True]\nnon_survivors_data = titanic_df[titanic_df.Survived==False]",
    "crumbs": [
      "Introduction to Pandas",
      "1. Introduction"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#which-gender-had-a-better-chance-of-survival",
    "href": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#which-gender-had-a-better-chance-of-survival",
    "title": "1. Introduction",
    "section": "5.2 Which gender had a better chance of survival?",
    "text": "5.2 Which gender had a better chance of survival?\nIn order to answer this question we need to look at how many males and females were on board and which gender had the highest survival rate.\n\nCount of Survivors by Gender\n\ntable = pd.crosstab(titanic_df['Survived'],titanic_df['Sex'])\nprint(table)\n\nSex       female  male\nSurvived              \nFalse         81   468\nTrue         233   109\n\n\n\n\nProportion of survivors by Gender\n\nprint(titanic_df.groupby('Sex').Survived.mean())\n\nSex\nfemale    0.742038\nmale      0.188908\nName: Survived, dtype: float64\n\n\n\n# calculate values for each survival status\nsurvivors_gender = survivors_data.groupby(['Sex']).size().values\nnon_survivors_gender = non_survivors_data.groupby(['Sex']).size().values\n\n# calculate totals for percentates\ntotals = survivors_gender + non_survivors_gender\n\n# use calculate_percentage_function to calculate percentage of the total\ndata1_percentages = calculate_percentage(survivors_gender, totals)*100 \ndata2_percentages = calculate_percentage(non_survivors_gender, totals)*100 \n\ngender_categories = ['Female', 'Male']\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n# plot chart for count of survivors by class\nax1.bar(range(len(survivors_gender)), survivors_gender, label='Survivors', alpha=0.5, color='g')\nax1.bar(range(len(non_survivors_gender)), non_survivors_gender, bottom=survivors_gender, label='Non-Survivors', alpha=0.5, color='r')\nplt.sca(ax1)\nplt.xticks([0.4, 1.4], gender_categories )\nax1.set_ylabel(\"Count\")\nax1.set_xlabel(\"\")\nax1.set_title(\"Count of survivors by gender\",fontsize=14)\nplt.legend(loc='upper left')\n\n# plot chart for percentage of survivors by class\nax2.bar(range(len(data1_percentages)), data1_percentages, alpha=0.5, color='g')\nax2.bar(range(len(data2_percentages)), data2_percentages, bottom=data1_percentages, alpha=0.5, color='r')\nplt.sca(ax2)\nplt.xticks([0.4, 1.4],  gender_categories)\nax2.set_ylabel(\"Percentage\")\nax2.set_xlabel(\"\")\nax2.set_title(\"% of survivors by gender\",fontsize=14)\n\nText(0.5, 1.0, '% of survivors by gender')\n\n\n\n\n\n\n\n\n\nThe plots and proportions above show that there were a significant more males on board the Titanic compared to the number of females. Whilst the second plot (% of survivors by gender) shows that Females had a higher proportion (74.2%) of survivors compared to the proportion of males (18.9%). This shows that females had a greater rate of survival.",
    "crumbs": [
      "Introduction to Pandas",
      "1. Introduction"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#which-social-class-had-a-better-chance-of-survival",
    "href": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#which-social-class-had-a-better-chance-of-survival",
    "title": "1. Introduction",
    "section": "5.3 Which social class had a better chance of survival?",
    "text": "5.3 Which social class had a better chance of survival?\n\nCount of survivors by class\n\n# CODE HERE\n\n\n\n\nPclass    Lower Class  Middle Class  Upper Class\nSurvived                                        \nFalse             372            97           80\nTrue              119            87          136\n\n\n\n\nProportion of survivors by class\n\n# CODE HERE\n\n\n\n\nPclass\nLower Class     0.242363\nMiddle Class    0.472826\nUpper Class     0.629630\nName: Survived, dtype: float64\n\n\n\n# CODE HERE\n\n\n\n\nText(0.5, 1.0, '% of survivors by class')\n\n\n\n\n\n\n\n\n\nThe graphs above so that whilst the lower class had more passengers, than all classes, and more survivors than the middle class, the lower class had the lowest survival rate. The Upper Class passengers had the highest survival rate",
    "crumbs": [
      "Introduction to Pandas",
      "1. Introduction"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#which-age-group-had-a-better-chance-of-survival",
    "href": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#which-age-group-had-a-better-chance-of-survival",
    "title": "1. Introduction",
    "section": "5.4 Which age group had a better chance of survival?",
    "text": "5.4 Which age group had a better chance of survival?\n\nDistribution of Age Groups\n\ntitanic_df.groupby(['age_group']).size().plot(kind='bar',stacked=True)\nplt.title(\"Distribution of Age Groups\",fontsize=14)\nplt.ylabel('Count')\nplt.xlabel('Age Group');\n\n\n\n\n\n\n\n\nFrom the plot above we can see that the majority of passengers were aged between 20-29\n\n\nProportion of survivors by age group\n\n# CODE HERE\n\n\n\n\nage_group\n0-9      0.612903\n10-19    0.401961\n20-29    0.315642\n30-39    0.454054\n40-49    0.354545\n50-59    0.416667\n60-69    0.315789\n70-79    0.000000\nName: Survived, dtype: float64\n\n\n\n# CODE HERE\n\n\n\n\nText(0.5, 1.0, '% of survivors by age group')\n\n\n\n\n\n\n\n\n\nWhen looking at proportions and percentages of survivors per age group, initially I was suprised by the results, until I thought that this analysis shoudl take into consideration the gender / sex of the passengers as well.\n\nprint(titanic_df.groupby(['Sex','age_group']).Survived.mean())\n\nSex     age_group\nfemale  0-9          0.633333\n        10-19        0.755556\n        20-29        0.681034\n        30-39        0.855072\n        40-49        0.687500\n        50-59        0.888889\n        60-69        1.000000\n        70-79             NaN\nmale    0-9          0.593750\n        10-19        0.122807\n        20-29        0.140496\n        30-39        0.215517\n        40-49        0.217949\n        50-59        0.133333\n        60-69        0.133333\n        70-79        0.000000\nName: Survived, dtype: float64\n\n\n\nmale_data = titanic_df[titanic_df.Sex == \"male\"].groupby('age_group').Survived.mean().values\nfemale_data = titanic_df[titanic_df.Sex == \"female\"].groupby('age_group').Survived.mean().values\nax = plt.subplot()\nmale_plt_position = np.array(range(len(age_labels)))\nfemale_plt_position = np.array(range(len(age_labels)))+0.4\nax.bar(male_plt_position, male_data,width=0.4,label='Male',color='b')\nax.bar(female_plt_position, female_data,width=0.4,label='Female',color='r')\nplt.xticks(tick_spacing,  age_labels)\nax.set_ylabel(\"Proportion\")\nax.set_xlabel(\"Age Group\")\nax.set_title(\"Proportion of survivors by age group / Gender\",fontsize=14)\nplt.legend(loc='best')\nplt.show()\n\n\n\n\n\n\n\n\nAfter relooking at the proportion of survivors by age group and gender, the data supports notion of women and children to be given preferential treatment over men. The plot “Proportion of survivors by age group / gender”, shows that children (0-9 years old, male and female) and women (all ages) had a much higher proportion of survivors. This supports the notion of the seats in the lifeboats been given to Women and Children first.",
    "crumbs": [
      "Introduction to Pandas",
      "1. Introduction"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#limitations-of-dataset",
    "href": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#limitations-of-dataset",
    "title": "1. Introduction",
    "section": "6.1 Limitations of dataset",
    "text": "6.1 Limitations of dataset\nThe are a number of limitations with the Titanic Dataset, which are caused by:\n\nmissing data\nonly been a sample of the data\n\nThe missing data and size of the sample could skew the results for example the missing ages.\n\nprint(missing_ages.groupby('Sex').size())\n\nSex\nfemale     53\nmale      124\ndtype: int64\n\n\nThe above shows that there were 53 ages missing for females and 124 ages missing for males. I had a choice with how to handle the missing ages each with their pros and cons.\ndelete the rows with the missing ages - this would limit the accuracy of the analysis of the gender and class, however the accuracy of the analysis of the Age factor would be more accurate generate ages based on the mean of ages - this could skew the results of the age analysis. In section 4.2, I choose this option, however I based the ages on the average for the Gender and Class. The size of the sample data could also impact the results as we don’t know if this is a random sample or if the selection of the data is biased or unbiased.",
    "crumbs": [
      "Introduction to Pandas",
      "1. Introduction"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#other-variables",
    "href": "Crash-Course-Pandas/09-Pandas-Exercises-2.html#other-variables",
    "title": "1. Introduction",
    "section": "6.2 Other variables",
    "text": "6.2 Other variables\nAs with most datasets the more information we have the better it can be analysed. I believe that we could add the following variables:\n\npassenger or crew - the current dataset doesn’t distingush between passenger or crew, however from history we know that a mixture of both survived.\nlife boat number - the reason why there were so many fatalities on board the Titanic was due to there not been enough lifeboats. I believe that if we knew the lifeboat number and the capacity of a lifeboat we could determine if it was possible for more people to survive",
    "crumbs": [
      "Introduction to Pandas",
      "1. Introduction"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/01-Series.html",
    "href": "Crash-Course-Pandas/01-Series.html",
    "title": "Series",
    "section": "",
    "text": "The first main data type we will learn about for pandas is the Series data type. Let’s import Pandas and explore the Series object.\nA Series is very similar to a NumPy array (in fact it is built on top of the NumPy array object). What differentiates the NumPy array from a Series, is that a Series can have axis labels, meaning it can be indexed by a label, instead of just a number location. It also doesn’t need to hold numeric data, it can hold any arbitrary Python Object.\nLet’s explore this concept through some examples:\n\nimport numpy as np\nimport pandas as pd\n\n\n\nYou can convert a list,numpy array, or dictionary to a Series:\n\nlabels = ['a','b','c']\nmy_list = [10,20,30]\narr = np.array([10,20,30])\nd = {'a':10,'b':20,'c':30}\n\n\n\n\npd.Series(data=my_list)\n\n0    10\n1    20\n2    30\ndtype: int64\n\n\n\npd.Series(data=my_list,index=labels)\n\na    10\nb    20\nc    30\ndtype: int64\n\n\n\npd.Series(my_list,labels)\n\na    10\nb    20\nc    30\ndtype: int64\n\n\n\n\n\n\npd.Series(arr)\n\n0    10\n1    20\n2    30\ndtype: int64\n\n\n\npd.Series(arr,labels)\n\na    10\nb    20\nc    30\ndtype: int64\n\n\n\n\n\n\npd.Series(d)\n\na    10\nb    20\nc    30\ndtype: int64\n\n\n\n\n\nA pandas Series can hold a variety of object types:\n\npd.Series(data=labels)\n\n0    a\n1    b\n2    c\ndtype: object\n\n\n\n# Even functions (although unlikely that you will use this)\npd.Series([sum,print,len])\n\n0      &lt;built-in function sum&gt;\n1    &lt;built-in function print&gt;\n2      &lt;built-in function len&gt;\ndtype: object\n\n\n\n\n\n\nThe key to using a Series is understanding its index. Pandas makes use of these index names or numbers by allowing for fast look ups of information (works like a hash table or dictionary).\nLet’s see some examples of how to grab information from a Series. Let us create two sereis, ser1 and ser2:\n\nser1 = pd.Series([1,2,3,4],index = ['USA', 'Germany','USSR', 'Japan'])                                   \n\n\nser1\n\nUSA        1\nGermany    2\nUSSR       3\nJapan      4\ndtype: int64\n\n\n\nser2 = pd.Series([1,2,5,4],index = ['USA', 'Germany','Italy', 'Japan'])                                   \n\n\nser2\n\nUSA        1\nGermany    2\nItaly      5\nJapan      4\ndtype: int64\n\n\n\nser1['USA']\n\n1\n\n\nOperations are then also done based off of index:\n\nser1 + ser2\n\nGermany    4.0\nItaly      NaN\nJapan      8.0\nUSA        2.0\nUSSR       NaN\ndtype: float64\n\n\nLet’s stop here for now and move on to DataFrames, which will expand on the concept of Series! # Great Job!",
    "crumbs": [
      "Introduction to Pandas",
      "Series"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/01-Series.html#creating-a-series",
    "href": "Crash-Course-Pandas/01-Series.html#creating-a-series",
    "title": "Series",
    "section": "",
    "text": "You can convert a list,numpy array, or dictionary to a Series:\n\nlabels = ['a','b','c']\nmy_list = [10,20,30]\narr = np.array([10,20,30])\nd = {'a':10,'b':20,'c':30}\n\n\n\n\npd.Series(data=my_list)\n\n0    10\n1    20\n2    30\ndtype: int64\n\n\n\npd.Series(data=my_list,index=labels)\n\na    10\nb    20\nc    30\ndtype: int64\n\n\n\npd.Series(my_list,labels)\n\na    10\nb    20\nc    30\ndtype: int64\n\n\n\n\n\n\npd.Series(arr)\n\n0    10\n1    20\n2    30\ndtype: int64\n\n\n\npd.Series(arr,labels)\n\na    10\nb    20\nc    30\ndtype: int64\n\n\n\n\n\n\npd.Series(d)\n\na    10\nb    20\nc    30\ndtype: int64\n\n\n\n\n\nA pandas Series can hold a variety of object types:\n\npd.Series(data=labels)\n\n0    a\n1    b\n2    c\ndtype: object\n\n\n\n# Even functions (although unlikely that you will use this)\npd.Series([sum,print,len])\n\n0      &lt;built-in function sum&gt;\n1    &lt;built-in function print&gt;\n2      &lt;built-in function len&gt;\ndtype: object",
    "crumbs": [
      "Introduction to Pandas",
      "Series"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/01-Series.html#using-an-index",
    "href": "Crash-Course-Pandas/01-Series.html#using-an-index",
    "title": "Series",
    "section": "",
    "text": "The key to using a Series is understanding its index. Pandas makes use of these index names or numbers by allowing for fast look ups of information (works like a hash table or dictionary).\nLet’s see some examples of how to grab information from a Series. Let us create two sereis, ser1 and ser2:\n\nser1 = pd.Series([1,2,3,4],index = ['USA', 'Germany','USSR', 'Japan'])                                   \n\n\nser1\n\nUSA        1\nGermany    2\nUSSR       3\nJapan      4\ndtype: int64\n\n\n\nser2 = pd.Series([1,2,5,4],index = ['USA', 'Germany','Italy', 'Japan'])                                   \n\n\nser2\n\nUSA        1\nGermany    2\nItaly      5\nJapan      4\ndtype: int64\n\n\n\nser1['USA']\n\n1\n\n\nOperations are then also done based off of index:\n\nser1 + ser2\n\nGermany    4.0\nItaly      NaN\nJapan      8.0\nUSA        2.0\nUSSR       NaN\ndtype: float64\n\n\nLet’s stop here for now and move on to DataFrames, which will expand on the concept of Series! # Great Job!",
    "crumbs": [
      "Introduction to Pandas",
      "Series"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/03-Missing-Data.html",
    "href": "Crash-Course-Pandas/03-Missing-Data.html",
    "title": "Missing Data",
    "section": "",
    "text": "Missing Data\nLet’s show a few convenient methods to deal with Missing Data in pandas:\n\nimport numpy as np\nimport pandas as pd\n\n\ndf = pd.DataFrame({'A':[1,2,np.nan],\n                  'B':[5,np.nan,np.nan],\n                  'C':[1,2,3]})\n\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1.0\n5.0\n1\n\n\n1\n2.0\nNaN\n2\n\n\n2\nNaN\nNaN\n3\n\n\n\n\n\n\n\n\n\ndf.dropna()\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1.0\n5.0\n1\n\n\n\n\n\n\n\n\n\ndf.dropna(axis=1)\n\n\n\n\n\n\n\n\nC\n\n\n\n\n0\n1\n\n\n1\n2\n\n\n2\n3\n\n\n\n\n\n\n\n\n\ndf.dropna(thresh=2)\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1.0\n5.0\n1\n\n\n1\n2.0\nNaN\n2\n\n\n\n\n\n\n\n\n\ndf.fillna(value='FILL VALUE')\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n5\n1\n\n\n1\n2\nFILL VALUE\n2\n\n\n2\nFILL VALUE\nFILL VALUE\n3\n\n\n\n\n\n\n\n\n\ndf['A'].fillna(value=df['A'].mean())\n\n0    1.0\n1    2.0\n2    1.5\nName: A, dtype: float64\n\n\n\n\nGreat Job!",
    "crumbs": [
      "Introduction to Pandas",
      "Missing Data"
    ]
  },
  {
    "objectID": "Crash-Course-Pandas/07-Pandas-Exercises-1.html",
    "href": "Crash-Course-Pandas/07-Pandas-Exercises-1.html",
    "title": "Pandas Exercises",
    "section": "",
    "text": "Pandas Exercises\nTASK: Import pandas\n\n# CODE HERE\n\nTASK: Read in the bank.csv file that is located under the 01-Crash-Course-Pandas folder. Pay close attention to where the .csv file is located!\n\n# CODE HERE\n\nTASK: Display the first 5 rows of the data set\n\n# CODE HERE\n\n\n\n\n\n\n\n\n\n\n\n\nage\njob\nmarital\neducation\ndefault\nbalance\nhousing\nloan\ncontact\nday\nmonth\nduration\ncampaign\npdays\nprevious\npoutcome\ny\n\n\n\n\n0\n30\nunemployed\nmarried\nprimary\nno\n1787\nno\nno\ncellular\n19\noct\n79\n1\n-1\n0\nunknown\nno\n\n\n1\n33\nservices\nmarried\nsecondary\nno\n4789\nyes\nyes\ncellular\n11\nmay\n220\n1\n339\n4\nfailure\nno\n\n\n2\n35\nmanagement\nsingle\ntertiary\nno\n1350\nyes\nno\ncellular\n16\napr\n185\n1\n330\n1\nfailure\nno\n\n\n3\n30\nmanagement\nmarried\ntertiary\nno\n1476\nyes\nyes\nunknown\n3\njun\n199\n4\n-1\n0\nunknown\nno\n\n\n4\n59\nblue-collar\nmarried\nsecondary\nno\n0\nyes\nno\nunknown\n5\nmay\n226\n1\n-1\n0\nunknown\nno\n\n\n\n\n\n\n\n\nTASK: What is the average (mean) age of the people in the dataset?\n\n# CODE HERE\n\n\n\n\n41.17009511170095\n\n\nTASK: What is the marital status of the youngest person in the dataset?\nHINT\n\n# CODE HERE\n\n\n\n\n'single'\n\n\nTASK: How many unique job categories are there?\n\n# CODE HERE\n\n\n\n\n12\n\n\nTASK: How many people are there per job category? (Take a peek at the expected output)\n\n# CODE HERE\n\n\n\n\nmanagement       969\nblue-collar      946\ntechnician       768\nadmin.           478\nservices         417\nretired          230\nself-employed    183\nentrepreneur     168\nunemployed       128\nhousemaid        112\nstudent           84\nunknown           38\nName: job, dtype: int64\n\n\nTASK: What percent of people in the dataset were married? \n\n#CODE HERE\n\n\n\n\n61.86684361866843\n\n\nTASK: There is a column labeled “default”. Use pandas’ .map() method to create a new column called “default code” which contains a 0 if there was no default, or a 1 if there was a default. Then show the head of the dataframe with this new column.\nHelpful Hint Link One\nHelpful Hint Link Two\n\n# CODE HERE\n\n\n\n\n\n\n\n\n\n\n\n\nage\njob\nmarital\neducation\ndefault\nbalance\nhousing\nloan\ncontact\nday\nmonth\nduration\ncampaign\npdays\nprevious\npoutcome\ny\ndefault code\n\n\n\n\n0\n30\nunemployed\nmarried\nprimary\nno\n1787\nno\nno\ncellular\n19\noct\n79\n1\n-1\n0\nunknown\nno\n0\n\n\n1\n33\nservices\nmarried\nsecondary\nno\n4789\nyes\nyes\ncellular\n11\nmay\n220\n1\n339\n4\nfailure\nno\n0\n\n\n2\n35\nmanagement\nsingle\ntertiary\nno\n1350\nyes\nno\ncellular\n16\napr\n185\n1\n330\n1\nfailure\nno\n0\n\n\n3\n30\nmanagement\nmarried\ntertiary\nno\n1476\nyes\nyes\nunknown\n3\njun\n199\n4\n-1\n0\nunknown\nno\n0\n\n\n4\n59\nblue-collar\nmarried\nsecondary\nno\n0\nyes\nno\nunknown\n5\nmay\n226\n1\n-1\n0\nunknown\nno\n0\n\n\n\n\n\n\n\n\nTASK: Using pandas .apply() method, create a new column called “marital code”. This column will only contained a shortened code of the possible marital status first letter. (For example “m” for “married” , “s” for “single” etc… See if you can do this with a lambda expression. Lots of ways to do this one!\nHint Link\n\n# CODE HERE\n\n\n\n\n\n\n\n\n\n\n\n\nage\njob\nmarital\neducation\ndefault\nbalance\nhousing\nloan\ncontact\nday\nmonth\nduration\ncampaign\npdays\nprevious\npoutcome\ny\ndefault code\nmarital code\n\n\n\n\n0\n30\nunemployed\nmarried\nprimary\nno\n1787\nno\nno\ncellular\n19\noct\n79\n1\n-1\n0\nunknown\nno\n0\nm\n\n\n1\n33\nservices\nmarried\nsecondary\nno\n4789\nyes\nyes\ncellular\n11\nmay\n220\n1\n339\n4\nfailure\nno\n0\nm\n\n\n2\n35\nmanagement\nsingle\ntertiary\nno\n1350\nyes\nno\ncellular\n16\napr\n185\n1\n330\n1\nfailure\nno\n0\ns\n\n\n3\n30\nmanagement\nmarried\ntertiary\nno\n1476\nyes\nyes\nunknown\n3\njun\n199\n4\n-1\n0\nunknown\nno\n0\nm\n\n\n4\n59\nblue-collar\nmarried\nsecondary\nno\n0\nyes\nno\nunknown\n5\nmay\n226\n1\n-1\n0\nunknown\nno\n0\nm\n\n\n\n\n\n\n\n\nTASK: What was the longest lasting duration?\n\n# CODE HERE\n\n\n\n\n3025\n\n\nTASK: What is the most common education level for people who are unemployed?\n\n# CODE HERE\n\n\n\n\nsecondary    68\ntertiary     32\nprimary      26\nunknown       2\nName: education, dtype: int64\n\n\nTASK: What is the average (mean) age for being unemployed?\n\n# CODE HERE\n\n\n\n\n40.90625",
    "crumbs": [
      "Introduction to Pandas",
      "Pandas Exercises"
    ]
  },
  {
    "objectID": "sqlserver-snippet/synonym.html",
    "href": "sqlserver-snippet/synonym.html",
    "title": "Synonym",
    "section": "",
    "text": "Creating a synonym for a table in another database\n\nFirst, create a new database named test and set the current database to test:\nCREATE DATABASE test;\nGO\n\nUSE test;\nGO\nNext, create a new schema named purchasing inside the test database:\nCREATE SCHEMA purchasing;\nGO\nThen, create a new table in the purchasing schema of the test database:\nCREATE TABLE test.purchasing.suppliers\n(\n    supplier_id   INT\n    PRIMARY KEY IDENTITY, \n    supplier_name NVARCHAR(100) NOT NULL\n);\nAfter that, from the BikeStores database, create a synonym for the purchasing.suppliers table in the test database:\nUSE BikeStores;\nGO\n\nCREATE SYNONYM suppliers \nFOR test.purchasing.suppliers;\nFinally, from the BikeStores database, refer to the test.purchasing.suppliers table using the suppliers synonym:\nSELECT * FROM suppliers;\nRemoving a synonym\nDROP SYNONYM IF EXISTS orders;\n\n\n\nReference\n\nsqlservertutorial",
    "crumbs": [
      "SQL-Server Snippet",
      "Synonym"
    ]
  },
  {
    "objectID": "sqlserver-snippet/permissions.html",
    "href": "sqlserver-snippet/permissions.html",
    "title": "Permissions",
    "section": "",
    "text": "Grant create/ excecute\nuse BSCORE_CREDITCARD\ngo\ngrant create procedure to [NOR\\HUYENDT14]\ngrant alter on schema::[dbo] to [NOR\\HUYENDT14]\ngrant execute to [NOR\\HUYENDT14]\n\n\nFind all permissions/access for all users in a database\n/*\n\nSecurity Audit Report\n1) List all access provisioned to a sql user or windows user/group directly \n2) List all access provisioned to a sql user or windows user/group through a database or application role\n3) List all access provisioned to the public role\n\nColumns Returned:\nUserName        : SQL or Windows/Active Directory user account.  This could also be an Active Directory group.\nUserType        : Value will be either 'SQL User' or 'Windows User'.  This reflects the type of user defined for the \n                  SQL Server user account.\nDatabaseUserName: Name of the associated user as defined in the database user account.  The database user may not be the\n                  same as the server user.\nRole            : The role name.  This will be null if the associated permissions to the object are defined at directly\n                  on the user account, otherwise this will be the name of the role that the user is a member of.\nPermissionType  : Type of permissions the user/role has on an object. Examples could include CONNECT, EXECUTE, SELECT\n                  DELETE, INSERT, ALTER, CONTROL, TAKE OWNERSHIP, VIEW DEFINITION, etc.\n                  This value may not be populated for all roles.  Some built in roles have implicit permission\n                  definitions.\nPermissionState : Reflects the state of the permission type, examples could include GRANT, DENY, etc.\n                  This value may not be populated for all roles.  Some built in roles have implicit permission\n                  definitions.\nObjectType      : Type of object the user/role is assigned permissions on.  Examples could include USER_TABLE, \n                  SQL_SCALAR_FUNCTION, SQL_INLINE_TABLE_VALUED_FUNCTION, SQL_STORED_PROCEDURE, VIEW, etc.   \n                  This value may not be populated for all roles.  Some built in roles have implicit permission\n                  definitions.          \nObjectName      : Name of the object that the user/role is assigned permissions on.  \n                  This value may not be populated for all roles.  Some built in roles have implicit permission\n                  definitions.\nColumnName      : Name of the column of the object that the user/role is assigned permissions on. This value\n                  is only populated if the object is a table, view or a table value function.                 \n*/\n\n--List all access provisioned to a sql user or windows user/group directly \nSELECT  \n    [UserName] = CASE princ.[type] \n                    WHEN 'S' THEN princ.[name]\n                    WHEN 'U' THEN ulogin.[name] COLLATE Latin1_General_CI_AI\n                 END,\n    [UserType] = CASE princ.[type]\n                    WHEN 'S' THEN 'SQL User'\n                    WHEN 'U' THEN 'Windows User'\n                 END,  \n    [DatabaseUserName] = princ.[name],       \n    [Role] = null,      \n    [PermissionType] = perm.[permission_name],       \n    [PermissionState] = perm.[state_desc],       \n    [ObjectType] = obj.type_desc,--perm.[class_desc],       \n    [ObjectName] = OBJECT_NAME(perm.major_id),\n    [ColumnName] = col.[name]\nFROM    \n    --database user\n    sys.database_principals princ  \nLEFT JOIN\n    --Login accounts\n    sys.login_token ulogin on princ.[sid] = ulogin.[sid]\nLEFT JOIN        \n    --Permissions\n    sys.database_permissions perm ON perm.[grantee_principal_id] = princ.[principal_id]\nLEFT JOIN\n    --Table columns\n    sys.columns col ON col.[object_id] = perm.major_id \n                    AND col.[column_id] = perm.[minor_id]\nLEFT JOIN\n    sys.objects obj ON perm.[major_id] = obj.[object_id]\nWHERE \n    princ.[type] in ('S','U')\nUNION\n--List all access provisioned to a sql user or windows user/group through a database or application role\nSELECT  \n    [UserName] = CASE memberprinc.[type] \n                    WHEN 'S' THEN memberprinc.[name]\n                    WHEN 'U' THEN ulogin.[name] COLLATE Latin1_General_CI_AI\n                 END,\n    [UserType] = CASE memberprinc.[type]\n                    WHEN 'S' THEN 'SQL User'\n                    WHEN 'U' THEN 'Windows User'\n                 END, \n    [DatabaseUserName] = memberprinc.[name],   \n    [Role] = roleprinc.[name],      \n    [PermissionType] = perm.[permission_name],       \n    [PermissionState] = perm.[state_desc],       \n    [ObjectType] = obj.type_desc,--perm.[class_desc],   \n    [ObjectName] = OBJECT_NAME(perm.major_id),\n    [ColumnName] = col.[name]\nFROM    \n    --Role/member associations\n    sys.database_role_members members\nJOIN\n    --Roles\n    sys.database_principals roleprinc ON roleprinc.[principal_id] = members.[role_principal_id]\nJOIN\n    --Role members (database users)\n    sys.database_principals memberprinc ON memberprinc.[principal_id] = members.[member_principal_id]\nLEFT JOIN\n    --Login accounts\n    sys.login_token ulogin on memberprinc.[sid] = ulogin.[sid]\nLEFT JOIN        \n    --Permissions\n    sys.database_permissions perm ON perm.[grantee_principal_id] = roleprinc.[principal_id]\nLEFT JOIN\n    --Table columns\n    sys.columns col on col.[object_id] = perm.major_id \n                    AND col.[column_id] = perm.[minor_id]\nLEFT JOIN\n    sys.objects obj ON perm.[major_id] = obj.[object_id]\nUNION\n--List all access provisioned to the public role, which everyone gets by default\nSELECT  \n    [UserName] = '{All Users}',\n    [UserType] = '{All Users}', \n    [DatabaseUserName] = '{All Users}',       \n    [Role] = roleprinc.[name],      \n    [PermissionType] = perm.[permission_name],       \n    [PermissionState] = perm.[state_desc],       \n    [ObjectType] = obj.type_desc,--perm.[class_desc],  \n    [ObjectName] = OBJECT_NAME(perm.major_id),\n    [ColumnName] = col.[name]\nFROM    \n    --Roles\n    sys.database_principals roleprinc\nLEFT JOIN        \n    --Role permissions\n    sys.database_permissions perm ON perm.[grantee_principal_id] = roleprinc.[principal_id]\nLEFT JOIN\n    --Table columns\n    sys.columns col on col.[object_id] = perm.major_id \n                    AND col.[column_id] = perm.[minor_id]                   \nJOIN \n    --All objects   \n    sys.objects obj ON obj.[object_id] = perm.[major_id]\nWHERE\n    --Only roles\n    roleprinc.[type] = 'R' AND\n    --Only public role\n    roleprinc.[name] = 'public' AND\n    --Only objects of ours, not the MS objects\n    obj.is_ms_shipped = 0\nORDER BY\n    princ.[Name],\n    OBJECT_NAME(perm.major_id),\n    col.[name],\n    perm.[permission_name],\n    perm.[state_desc],\n    obj.type_desc--perm.[class_desc] \n\n\nReferences\n\nfind-all-permissions",
    "crumbs": [
      "SQL-Server Snippet",
      "Permissions"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/03-NumPy-Exercises.html",
    "href": "Crash-Course-Numpy/03-NumPy-Exercises.html",
    "title": "NumPy Exercises",
    "section": "",
    "text": "Now that we’ve learned about NumPy let’s test your knowledge. We’ll start off with a few simple tasks and then you’ll be asked some more complicated questions.\n\nIMPORTANT NOTE! Make sure you don’t run the cells directly above the example output shown, otherwise you will end up writing over the example output!\n\n\n\n\n\n\n\n# CODE HERE\n\n\n# DON'T WRITE HERE\n\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\n\n\n\n# DON'T WRITE HERE\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\n\n\n\n# DON'T WRITE HERE\n\narray([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.])\n\n\n\n\n\n\n# DON'T WRITE HERE\n\narray([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n       27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n       44, 45, 46, 47, 48, 49, 50])\n\n\n\n\n\n\n# DON'T WRITE HERE\n\narray([10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42,\n       44, 46, 48, 50])\n\n\n\n\n\n\n# DON'T WRITE HERE\n\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n\n\n\n\n\n\n# DON'T WRITE HERE\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\n\n\n\n# DON'T WRITE HERE\n\narray([0.65248055])\n\n\n\n\n\n\n# DON'T WRITE HERE\n\narray([ 1.80076712, -1.12375847, -0.98524305,  0.11673573,  1.96346762,\n        1.81378592, -0.33790771,  0.85012656,  0.0100703 , -0.91005957,\n        0.29064366,  0.69906357,  0.1774377 , -0.61958694, -0.45498611,\n       -2.0804685 , -0.06778549,  1.06403819,  0.4311884 , -1.09853837,\n        1.11980469, -0.48751963,  1.32517611, -0.61775122, -0.00622865])\n\n\n\n\n\n\n# DON'T WRITE HERE\n\narray([[0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ],\n       [0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 ],\n       [0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 ],\n       [0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 ],\n       [0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 ],\n       [0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 ],\n       [0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 ],\n       [0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 ],\n       [0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 ],\n       [0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.  ]])\n\n\n\n\n\n\n# DON'T WRITE HERE\n\narray([0.        , 0.05263158, 0.10526316, 0.15789474, 0.21052632,\n       0.26315789, 0.31578947, 0.36842105, 0.42105263, 0.47368421,\n       0.52631579, 0.57894737, 0.63157895, 0.68421053, 0.73684211,\n       0.78947368, 0.84210526, 0.89473684, 0.94736842, 1.        ])\n\n\n\n\n\nNow you will be given a starting matrix (be sure to run the cell below!), and be asked to replicate the resulting matrix outputs:\n\n# RUN THIS CELL - THIS IS OUR STARTING MATRIX\nmat = np.arange(1,26).reshape(5,5)\nmat\n\narray([[ 1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10],\n       [11, 12, 13, 14, 15],\n       [16, 17, 18, 19, 20],\n       [21, 22, 23, 24, 25]])\n\n\n\n\n\n# CODE HERE\n\n\n# DON'T WRITE HERE\n\narray([[12, 13, 14, 15],\n       [17, 18, 19, 20],\n       [22, 23, 24, 25]])\n\n\n\n\n\n\n# DON'T WRITE HERE\n\n20\n\n\n\n\n\n\n# DON'T WRITE HERE\n\narray([[ 2],\n       [ 7],\n       [12]])\n\n\n\n\n\n\n# DON'T WRITE HERE\n\narray([21, 22, 23, 24, 25])\n\n\n\n\n\n\n# DON'T WRITE HERE\n\narray([[16, 17, 18, 19, 20],\n       [21, 22, 23, 24, 25]])\n\n\n\n\n\n\n\n\n\n# DON'T WRITE HERE\n\n325\n\n\n\n\n\n\n# DON'T WRITE HERE\n\n7.211102550927978\n\n\n\n\n\n\n# DON'T WRITE HERE\n\narray([55, 60, 65, 70, 75])\n\n\n\n\n\n\nWe worked a lot with random data with numpy, but is there a way we can insure that we always get the same random numbers? Click Here for a Hint",
    "crumbs": [
      "NumPy",
      "NumPy Exercises"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/03-NumPy-Exercises.html#numpy-indexing-and-selection",
    "href": "Crash-Course-Numpy/03-NumPy-Exercises.html#numpy-indexing-and-selection",
    "title": "NumPy Exercises",
    "section": "",
    "text": "Now you will be given a starting matrix (be sure to run the cell below!), and be asked to replicate the resulting matrix outputs:\n\n# RUN THIS CELL - THIS IS OUR STARTING MATRIX\nmat = np.arange(1,26).reshape(5,5)\nmat\n\narray([[ 1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10],\n       [11, 12, 13, 14, 15],\n       [16, 17, 18, 19, 20],\n       [21, 22, 23, 24, 25]])\n\n\n\n\n\n# CODE HERE\n\n\n# DON'T WRITE HERE\n\narray([[12, 13, 14, 15],\n       [17, 18, 19, 20],\n       [22, 23, 24, 25]])\n\n\n\n\n\n\n# DON'T WRITE HERE\n\n20\n\n\n\n\n\n\n# DON'T WRITE HERE\n\narray([[ 2],\n       [ 7],\n       [12]])\n\n\n\n\n\n\n# DON'T WRITE HERE\n\narray([21, 22, 23, 24, 25])\n\n\n\n\n\n\n# DON'T WRITE HERE\n\narray([[16, 17, 18, 19, 20],\n       [21, 22, 23, 24, 25]])",
    "crumbs": [
      "NumPy",
      "NumPy Exercises"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/03-NumPy-Exercises.html#numpy-operations",
    "href": "Crash-Course-Numpy/03-NumPy-Exercises.html#numpy-operations",
    "title": "NumPy Exercises",
    "section": "",
    "text": "# DON'T WRITE HERE\n\n325\n\n\n\n\n\n\n# DON'T WRITE HERE\n\n7.211102550927978\n\n\n\n\n\n\n# DON'T WRITE HERE\n\narray([55, 60, 65, 70, 75])",
    "crumbs": [
      "NumPy",
      "NumPy Exercises"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/03-NumPy-Exercises.html#bonus-question",
    "href": "Crash-Course-Numpy/03-NumPy-Exercises.html#bonus-question",
    "title": "NumPy Exercises",
    "section": "",
    "text": "We worked a lot with random data with numpy, but is there a way we can insure that we always get the same random numbers? Click Here for a Hint",
    "crumbs": [
      "NumPy",
      "NumPy Exercises"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/01-NumPy-Indexing-and-Selection.html",
    "href": "Crash-Course-Numpy/01-NumPy-Indexing-and-Selection.html",
    "title": "NumPy Indexing and Selection",
    "section": "",
    "text": "In this lecture we will discuss how to select elements or groups of elements from an array.\n\nimport numpy as np\n\n\n#Creating sample array\narr = np.arange(0,11)\n\n\n#Show\narr\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n\n\n\n\nThe simplest way to pick one or some elements of an array looks very similar to python lists:\n\n#Get a value at an index\narr[8]\n\n8\n\n\n\n#Get values in a range\narr[1:5]\n\narray([1, 2, 3, 4])\n\n\n\n#Get values in a range\narr[0:5]\n\narray([0, 1, 2, 3, 4])\n\n\n\n\n\nNumPy arrays differ from normal Python lists because of their ability to broadcast. With lists, you can only reassign parts of a list with new parts of the same size and shape. That is, if you wanted to replace the first 5 elements in a list with a new value, you would have to pass in a new 5 element list. With NumPy arrays, you can broadcast a single value across a larger set of values:\n\n#Setting a value with index range (Broadcasting)\narr[0:5]=100\n\n#Show\narr\n\narray([100, 100, 100, 100, 100,   5,   6,   7,   8,   9,  10])\n\n\n\n# Reset array, we'll see why I had to reset in  a moment\narr = np.arange(0,11)\n\n#Show\narr\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n\n\n\n#Important notes on Slices\nslice_of_arr = arr[0:6]\n\n#Show slice\nslice_of_arr\n\narray([0, 1, 2, 3, 4, 5])\n\n\n\n#Change Slice\nslice_of_arr[:]=99\n\n#Show Slice again\nslice_of_arr\n\narray([99, 99, 99, 99, 99, 99])\n\n\nNow note the changes also occur in our original array!\n\narr\n\narray([99, 99, 99, 99, 99, 99,  6,  7,  8,  9, 10])\n\n\nData is not copied, it’s a view of the original array! This avoids memory problems!\n\n#To get a copy, need to be explicit\narr_copy = arr.copy()\n\narr_copy\n\narray([99, 99, 99, 99, 99, 99,  6,  7,  8,  9, 10])\n\n\n\n\n\nThe general format is arr_2d[row][col] or arr_2d[row,col]. I recommend using the comma notation for clarity.\n\narr_2d = np.array(([5,10,15],[20,25,30],[35,40,45]))\n\n#Show\narr_2d\n\narray([[ 5, 10, 15],\n       [20, 25, 30],\n       [35, 40, 45]])\n\n\n\n#Indexing row\narr_2d[1]\n\narray([20, 25, 30])\n\n\n\n# Format is arr_2d[row][col] or arr_2d[row,col]\n\n# Getting individual element value\narr_2d[1][0]\n\n20\n\n\n\n# Getting individual element value\narr_2d[1,0]\n\n20\n\n\n\n# 2D array slicing\n\n#Shape (2,2) from top right corner\narr_2d[:2,1:]\n\narray([[10, 15],\n       [25, 30]])\n\n\n\n#Shape bottom row\narr_2d[2]\n\narray([35, 40, 45])\n\n\n\n#Shape bottom row\narr_2d[2,:]\n\narray([35, 40, 45])\n\n\n\n\n\nIndexing a 2D matrix can be a bit confusing at first, especially when you start to add in step size. Try google image searching NumPy indexing to find useful images, like this one:\n Image source: http://www.scipy-lectures.org/intro/numpy/numpy.html\n\n\n\nThis is a very fundamental concept that will directly translate to pandas later on, make sure you understand this part!\nLet’s briefly go over how to use brackets for selection based off of comparison operators.\n\narr = np.arange(1,11)\narr\n\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n\n\n\narr &gt; 4\n\narray([False, False, False, False,  True,  True,  True,  True,  True,\n        True])\n\n\n\nbool_arr = arr&gt;4\n\n\nbool_arr\n\narray([False, False, False, False,  True,  True,  True,  True,  True,\n        True])\n\n\n\narr[bool_arr]\n\narray([ 5,  6,  7,  8,  9, 10])\n\n\n\narr[arr&gt;2]\n\narray([ 3,  4,  5,  6,  7,  8,  9, 10])\n\n\n\nx = 2\narr[arr&gt;x]\n\narray([ 3,  4,  5,  6,  7,  8,  9, 10])",
    "crumbs": [
      "NumPy",
      "NumPy Indexing and Selection"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/01-NumPy-Indexing-and-Selection.html#bracket-indexing-and-selection",
    "href": "Crash-Course-Numpy/01-NumPy-Indexing-and-Selection.html#bracket-indexing-and-selection",
    "title": "NumPy Indexing and Selection",
    "section": "",
    "text": "The simplest way to pick one or some elements of an array looks very similar to python lists:\n\n#Get a value at an index\narr[8]\n\n8\n\n\n\n#Get values in a range\narr[1:5]\n\narray([1, 2, 3, 4])\n\n\n\n#Get values in a range\narr[0:5]\n\narray([0, 1, 2, 3, 4])",
    "crumbs": [
      "NumPy",
      "NumPy Indexing and Selection"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/01-NumPy-Indexing-and-Selection.html#broadcasting",
    "href": "Crash-Course-Numpy/01-NumPy-Indexing-and-Selection.html#broadcasting",
    "title": "NumPy Indexing and Selection",
    "section": "",
    "text": "NumPy arrays differ from normal Python lists because of their ability to broadcast. With lists, you can only reassign parts of a list with new parts of the same size and shape. That is, if you wanted to replace the first 5 elements in a list with a new value, you would have to pass in a new 5 element list. With NumPy arrays, you can broadcast a single value across a larger set of values:\n\n#Setting a value with index range (Broadcasting)\narr[0:5]=100\n\n#Show\narr\n\narray([100, 100, 100, 100, 100,   5,   6,   7,   8,   9,  10])\n\n\n\n# Reset array, we'll see why I had to reset in  a moment\narr = np.arange(0,11)\n\n#Show\narr\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n\n\n\n#Important notes on Slices\nslice_of_arr = arr[0:6]\n\n#Show slice\nslice_of_arr\n\narray([0, 1, 2, 3, 4, 5])\n\n\n\n#Change Slice\nslice_of_arr[:]=99\n\n#Show Slice again\nslice_of_arr\n\narray([99, 99, 99, 99, 99, 99])\n\n\nNow note the changes also occur in our original array!\n\narr\n\narray([99, 99, 99, 99, 99, 99,  6,  7,  8,  9, 10])\n\n\nData is not copied, it’s a view of the original array! This avoids memory problems!\n\n#To get a copy, need to be explicit\narr_copy = arr.copy()\n\narr_copy\n\narray([99, 99, 99, 99, 99, 99,  6,  7,  8,  9, 10])",
    "crumbs": [
      "NumPy",
      "NumPy Indexing and Selection"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/01-NumPy-Indexing-and-Selection.html#indexing-a-2d-array-matrices",
    "href": "Crash-Course-Numpy/01-NumPy-Indexing-and-Selection.html#indexing-a-2d-array-matrices",
    "title": "NumPy Indexing and Selection",
    "section": "",
    "text": "The general format is arr_2d[row][col] or arr_2d[row,col]. I recommend using the comma notation for clarity.\n\narr_2d = np.array(([5,10,15],[20,25,30],[35,40,45]))\n\n#Show\narr_2d\n\narray([[ 5, 10, 15],\n       [20, 25, 30],\n       [35, 40, 45]])\n\n\n\n#Indexing row\narr_2d[1]\n\narray([20, 25, 30])\n\n\n\n# Format is arr_2d[row][col] or arr_2d[row,col]\n\n# Getting individual element value\narr_2d[1][0]\n\n20\n\n\n\n# Getting individual element value\narr_2d[1,0]\n\n20\n\n\n\n# 2D array slicing\n\n#Shape (2,2) from top right corner\narr_2d[:2,1:]\n\narray([[10, 15],\n       [25, 30]])\n\n\n\n#Shape bottom row\narr_2d[2]\n\narray([35, 40, 45])\n\n\n\n#Shape bottom row\narr_2d[2,:]\n\narray([35, 40, 45])",
    "crumbs": [
      "NumPy",
      "NumPy Indexing and Selection"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/01-NumPy-Indexing-and-Selection.html#more-indexing-help",
    "href": "Crash-Course-Numpy/01-NumPy-Indexing-and-Selection.html#more-indexing-help",
    "title": "NumPy Indexing and Selection",
    "section": "",
    "text": "Indexing a 2D matrix can be a bit confusing at first, especially when you start to add in step size. Try google image searching NumPy indexing to find useful images, like this one:\n Image source: http://www.scipy-lectures.org/intro/numpy/numpy.html",
    "crumbs": [
      "NumPy",
      "NumPy Indexing and Selection"
    ]
  },
  {
    "objectID": "Crash-Course-Numpy/01-NumPy-Indexing-and-Selection.html#conditional-selection",
    "href": "Crash-Course-Numpy/01-NumPy-Indexing-and-Selection.html#conditional-selection",
    "title": "NumPy Indexing and Selection",
    "section": "",
    "text": "This is a very fundamental concept that will directly translate to pandas later on, make sure you understand this part!\nLet’s briefly go over how to use brackets for selection based off of comparison operators.\n\narr = np.arange(1,11)\narr\n\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n\n\n\narr &gt; 4\n\narray([False, False, False, False,  True,  True,  True,  True,  True,\n        True])\n\n\n\nbool_arr = arr&gt;4\n\n\nbool_arr\n\narray([False, False, False, False,  True,  True,  True,  True,  True,\n        True])\n\n\n\narr[bool_arr]\n\narray([ 5,  6,  7,  8,  9, 10])\n\n\n\narr[arr&gt;2]\n\narray([ 3,  4,  5,  6,  7,  8,  9, 10])\n\n\n\nx = 2\narr[arr&gt;x]\n\narray([ 3,  4,  5,  6,  7,  8,  9, 10])",
    "crumbs": [
      "NumPy",
      "NumPy Indexing and Selection"
    ]
  },
  {
    "objectID": "comparision/plsql-db2.html",
    "href": "comparision/plsql-db2.html",
    "title": "plsql-db2",
    "section": "",
    "text": "Here’s a table comparing DB2 and PL/SQL based on the provided features and their syntax:\nThese SQL commands are mainly categorized into five categories:",
    "crumbs": [
      "Comparision",
      "plsql-db2"
    ]
  },
  {
    "objectID": "comparision/plsql-db2.html#basic-sql-commands",
    "href": "comparision/plsql-db2.html#basic-sql-commands",
    "title": "plsql-db2",
    "section": "Basic SQL commands:",
    "text": "Basic SQL commands:\n\nDDL – Data Definition Language\n\n\n\n\n\n\n\n\nFeature\nPL/SQL Syntax\nDB2 Syntax\n\n\n\n\nCreate Table\nCREATE TABLE new_table AS SELECT * FROM existing_table;\nsimilar\n\n\nDrop Table\nDROP TABLE table_name [PURGE];\nDROP TABLE IF EXISTS table_name;\n\n\nRename Columns\nALTER TABLE table_name RENAME COLUMN old_column TO new_column;\nsimilar\n\n\nModify Columns\nALTER TABLE table_name MODIFY COLUMN column_name datatype;\nsimilar\n\n\nAdd Columns\nALTER TABLE table_name ADD COLUMN new_column datatype;\nsimilar\n\n\n\nDQL – Data Query Language\n\n\n\n\n\n\n\n\nFeature\nPL/SQL Syntax\nDB2 Syntax\n\n\n\n\nLeft Join\nSELECT * FROM table1 LEFT JOIN table2 ON table1.column = table2.column;\nsimilar\n\n\nInner Join\nSELECT * FROM table1 INNER JOIN table2 ON table1.column = table2.column;\nsimilar\n\n\nFull Join\nSELECT * FROM table1 FULL JOIN table2 ON table1.column = table2.column;\nsimilar\n\n\nAnti Join\nSELECT column1, column2 FROM table1 WHERE NOT EXISTS (SELECT * FROM table2 WHERE table1.column = table2.column)\nsimilar\n\n\nMerge\nMERGE INTO table_name USING source_table ON (table_name.column = source_table.column) WHEN MATCHED THEN UPDATE SET column1 = value1 WHEN NOT MATCHED THEN INSERT (column1) VALUES (value1);\nsimilar\n\n\nInsert Into\nINSERT INTO table_name (column1, column2) VALUES (value1, value2);\nsimilar\n\n\nUpdate\nUPDATE staff a SET salary = (SELECT AVG(salary) + 2000 FROM staff b WHERE a.dept = b.dept) WHERE id &lt; 60;\nsimilar\n\n\nTruncate Table\nTRUNCATE TABLE table_name;\nTRUNCATE TABLE table_name IMMEDIATE;\n\n\nTemporary Table\nDECLARE GLOBAL TEMPORARY TABLE table_name (column1 datatype, column2 datatype);\nCREATE GLOBAL TEMPORARY TABLE table_name (column1 datatype, column2 datatype);\n\n\nGroup By\nSELECT column1, COUNT(*) FROM table_name GROUP BY column1;\nsimilar\n\n\nPartition By\nSELECT column1, RANK() OVER (PARTITION BY column2 ORDER BY column3) FROM table_name;\nsimilar\n\n\nRun Procedure\nEXECUTE procedure_name;\nCALL procedure_name();\n\n\nLoop\nFOR row IN (SELECT * FROM table_name) LOOP &lt;statements&gt; END LOOP;\nFOR row AS cursor_name CURSOR FOR SELECT * FROM table_name DO &lt;statements&gt; END FOR;\n\n\nParallel\nINSERT /*+APPEND PARALLEL(8)*/ INTO\nSELECT * FROM table_name PARALLEL 4;\n\n\n\nMERGE /*+ parallel(8) */ INTO\n\n\n\n\nSELECT /*+ PARALLEL(8) */ * FROM\n\n\n\n\nDCL – Data Control Language\n\n\n\n\n\n\n\n\nFeature\nPL/SQL Syntax\nDB2 Syntax\n\n\n\n\nSelect\nSELECT column1, column2 FROM table_name;\nsimilar",
    "crumbs": [
      "Comparision",
      "plsql-db2"
    ]
  },
  {
    "objectID": "comparision/plsql-db2.html#utilities",
    "href": "comparision/plsql-db2.html#utilities",
    "title": "plsql-db2",
    "section": "Utilities",
    "text": "Utilities\n\n\n\n\n\n\n\n\nFeature\nPL/SQL Syntax\nDB2 Syntax\n\n\n\n\nSYS_INFO\n\nSELECT * FROM SYSIBMADM.ENV_SYS_INFO\n\n\nNVL\nNVL(column, 0)\nCOALESCE(column, 0)\n\n\n\nCOALESCE(column, 0)\nCOALESCE(column, 0)\n\n\nDiff Month\nSELECT (EXTRACT(YEAR FROM end_date) - EXTRACT(YEAR FROM start_date)) * 12 + EXTRACT(MONTH FROM end_date) - EXTRACT(MONTH FROM start_date) AS month_diff FROM your_table;\nSELECT (YEAR(end_date) - YEAR(start_date)) * 12 + MONTH(end_date) - MONTH(start_date) AS month_diff FROM your_table;\n\n\nAdd Months\nSELECT ADD_MONTHS(date1, number_of_months) FROM dual;\nSELECT ADD_YEARS(current_date, 3), ADD_MONTHS(current_date, 3 ), ADD_DAYS(current_date, 3), ADD_HOURS(current timestamp, 3), ADD_MINUTES(current timestamp, 3), ADD_SECONDS(current timestamp, 3) FROM sysibm.sysdummy1;\n\n\nDiff Date\nSELECT date2 - date1 FROM dual;\n\n\n\nCompare 2 Tables\n(SELECT * FROM B_TMP MINUS SELECT * FROM B_TMP1) UNION ALL(SELECT * FROM B_TMP1 MINUS SELECT * FROM B_TMP)\nSELECT * FROM table1 EXCEPT SELECT * FROM table2;",
    "crumbs": [
      "Comparision",
      "plsql-db2"
    ]
  },
  {
    "objectID": "comparision/plsql-db2.html#references",
    "href": "comparision/plsql-db2.html#references",
    "title": "plsql-db2",
    "section": "References",
    "text": "References\n\ndb2-sql-cookbook",
    "crumbs": [
      "Comparision",
      "plsql-db2"
    ]
  },
  {
    "objectID": "comparision/dplyr-pandas.html",
    "href": "comparision/dplyr-pandas.html",
    "title": "dplyr vs. pandas",
    "section": "",
    "text": "Here’s a markdown table comparing dplyr (R) and pandas (Python) for various data manipulation tasks:\n\nData frame verbs\n\nRows\n\n\n\n\n\n\n\n\nFeature\ndplyr (R)\npandas (Python)\n\n\n\n\nArrange\narrange(df, col)\ndf.sort_values(by='col', ascending=True)\n\n\nDistinct\ndistinct(df, col)\ndf.drop_duplicates(subset='col')\n\n\nFilter\nfilter(df, condition)\ndf[df['condition']]\n\n\nSlice\nslice(df, rows)\ndf.iloc[rows]\n\n\n\nColumns\n\n\n\n\n\n\n\n\nFeature\ndplyr (R)\npandas (Python)\n\n\n\n\nGlimpse\nglimpse(df)\ndf.info()\n\n\nMutate\nmutate(df, new_col = func(old_col))\ndf['new_col'] = df['old_col'].apply(func)\n\n\nPull\npull(df, col)\ndf['col']\n\n\nRename\nrename(df, new_col_name = old_col_name)\ndf.rename(columns={'old_col_name': 'new_col_name'}, inplace=True)\n\n\nSelect\nselect(df, col1, col2, ...)\ndf[['col1', 'col2', ...]]\n\n\n\nGroups\n\n\n\n\n\n\n\n\nFeature\ndplyr (R)\npandas (Python)\n\n\n\n\nGroup By\ngroup_by(df, col)\ndf.groupby('col')\n\n\nSummarise\nsummarise(df, new_col = func(col))\ndf.agg({'col': func})\n\n\n\nData frames\n\n\n\n\n\n\n\n\nFeature\ndplyr (R)\npandas (Python)\n\n\n\n\nbind_cols\nbind_cols(df1, df2)\npd.concat([df1, df2], axis=1)\n\n\nbind_rows\nbind_rows(df1, df2)\npd.concat([df1, df2], axis=0)\n\n\nInner Join\ninner_join(df1, df2, by = \"key_column\")\npd.merge(df1, df2, on='key_column', how='inner')\n\n\nLeft Join\nleft_join(df1, df2, by = \"key_column\")\npd.merge(df1, df2, on='key_column', how='left')\n\n\nRight Join\nright_join(df1, df2, by = \"key_column\")\npd.merge(df1, df2, on='key_column', how='right')\n\n\nFull Join\nfull_join(df1, df2, by = \"key_column\")\npd.merge(df1, df2, on='key_column', how='outer')\n\n\nSemi Join\nsemi_join(df1, df2, by = \"key_column\")\nNot directly available; can use merge and isin together for similar effect.\n\n\nAnti Join\nanti_join(df1, df2, by = \"key_column\")\nNot directly available; can use merge and isin together for similar effect.\n\n\n\nVector functions\n\n\n\n\n\n\n\n\nFeature\ndplyr (R)\npandas (Python)\n\n\n\n\nif_else\nmutate(df, new_col = if_else(condition, true_val, false_val))\ndf['new_col'] = np.where(condition, true_val, false_val)\n\n\nna_if\nmutate(df, col = na_if(col, value))\ndf['col'].replace(value, np.nan)\n\n\nn_distinct\nn_distinct(df, col)\ndf['col'].nunique()\n\n\nsample_n\nsample_n(df, n)\ndf.sample(n=n)\n\n\nsample_frac\nsample_frac(df, fraction)\ndf.sample(frac=fraction)\n\n\ncase_when\nmutate(df, new_col = case_when(condition1 ~ value1, condition2 ~ value2, ...))\ndf['new_col'] = np.select([condition1, condition2, ...], [value1, value2, ...], default=default_value)\n\n\ncummean\nmutate(df, new_col = cummean(col))\ndf['new_col'] = df['col'].expanding().mean()\n\n\nrow_number\nmutate(df, row_num = row_number())\ndf['row_num'] = range(1, len(df)+1)\n\n\nmin_rank\nmutate(df, rank = min_rank(col))\ndf['rank'] = df['col'].rank(method='min')\n\n\ndense_rank\nmutate(df, rank = dense_rank(col))\ndf['rank'] = df['col'].rank(method='dense')\n\n\n\nNote that while many functionalities are directly available in both dplyr and pandas, some might require slight variations or custom functions to achieve the same result.",
    "crumbs": [
      "Comparision",
      "dplyr vs. pandas"
    ]
  },
  {
    "objectID": "r-snippet/reticulate.html",
    "href": "r-snippet/reticulate.html",
    "title": "reticulate",
    "section": "",
    "text": "Install python in r step by step\n\nCreate environment\nlibrary(reticulate)\nconda_create(envname = 'D:/python/env/r-reticulate', python_version = '3.6.13')\n\n\nEdit .Rprofile\n# usethis::edit_r_profile()\n# Sys.setenv(RETICULATE_PYTHON = 'D:\\\\python\\\\env\\\\r-reticulate\\\\') # version 3.6\n\n\nInstall python package\n\nDefault environment\nmy_env &lt;- \"D:\\\\python\\\\envs\\\\r-reticulate\"\n\n\nInstall packages using conda\nreticulate::conda_install(envname = my_env, packages = \"lightgbm\")\n\n\nInstall packages using pip\nreticulate::py_install(packages = \"xgboost\", envname = my_env)\n\n\n\nExtra\n\ninstall miniconda if needed\nreticulate::install_miniconda(path = \"D:/python/\")\nreticulate::miniconda_update(path = \"D:/python/\") # update if needed\n\n\ncheck python version\nreticulate::py_version()\nreticulate::py_versions_windows()\n\n\ncheck config\nreticulate::py_config() \nreticulate::py_discover_config()\n\n\ncheck conda envs\nreticulate::conda_list()\n# conda_remove(\"pillow\")\nreticulate::virtualenv_list()\n\n\nconvert to rmd\nrmarkdown:::convert_ipynb(\"python/volatility-prediction.ipynb\")",
    "crumbs": [
      "R snippet",
      "reticulate"
    ]
  },
  {
    "objectID": "r-snippet/file_folders.html",
    "href": "r-snippet/file_folders.html",
    "title": "file and folder",
    "section": "",
    "text": "r18_2017 &lt;- Sys.glob(paste0(my_folder, \"2017/*.xlsx\"))\n\nr18_2017 &lt;- list.files(paste0(my_folder, \"2017/\"), full.names = T)\n\ndt &lt;- rio::import_list(r18_files, rbind = TRUE)",
    "crumbs": [
      "R snippet",
      "file and folder"
    ]
  },
  {
    "objectID": "r-snippet/DT.html",
    "href": "r-snippet/DT.html",
    "title": "DT",
    "section": "",
    "text": "create_dt &lt;- function(x){\n  DT::datatable(x,\n                extensions = 'Buttons',\n                options = list(dom = 'Blfrtip',\n                               buttons = c('copy', 'csv', 'excel', 'pdf', 'print'),\n                               lengthMenu = list(c(10,25,50,-1),\n                                                 c(10,25,50,\"All\"))))\n}\n\n## create_dt(iris)",
    "crumbs": [
      "R snippet",
      "DT"
    ]
  },
  {
    "objectID": "r-snippet/dplyr.html",
    "href": "r-snippet/dplyr.html",
    "title": "dplyr",
    "section": "",
    "text": "We need to do the following:\n\nUse quo_name() to convert the input expression to string\nUse := helper provided by rlang\n\n# https://www.onceupondata.com/2017/08/12/my-first-steps-into-the-world-of-tidyeval/\nadd_tag_count &lt;- function(x, cname, count_col){\n  \n  cname &lt;- enquo(cname)\n  \n  count_col &lt;- enquo(count_col)\n  count_col_name &lt;- quo_name(count_col)\n\n  x %&gt;% \n    mutate(!!count_col_name := map_int(!!cname, length))\n}\n\n\n\nfreq_tbl &lt;- function(df, ..., percent = TRUE){\n  group &lt;- quos(...)\n  \n  out &lt;- df %&gt;% \n    group_by(!!!group) %&gt;% \n    summarise(freq = n()) %&gt;% \n    arrange(desc(freq)) %&gt;% \n    ungroup()\n  \n  if(percent == TRUE){\n    out &lt;- out %&gt;% \n      mutate(percentage = 100*freq/sum(freq))\n  }\n  return(out)\n}",
    "crumbs": [
      "R snippet",
      "dplyr"
    ]
  },
  {
    "objectID": "r-snippet/dplyr.html#dplyr-with-bang-bang-example",
    "href": "r-snippet/dplyr.html#dplyr-with-bang-bang-example",
    "title": "dplyr",
    "section": "",
    "text": "We need to do the following:\n\nUse quo_name() to convert the input expression to string\nUse := helper provided by rlang\n\n# https://www.onceupondata.com/2017/08/12/my-first-steps-into-the-world-of-tidyeval/\nadd_tag_count &lt;- function(x, cname, count_col){\n  \n  cname &lt;- enquo(cname)\n  \n  count_col &lt;- enquo(count_col)\n  count_col_name &lt;- quo_name(count_col)\n\n  x %&gt;% \n    mutate(!!count_col_name := map_int(!!cname, length))\n}\n\n\n\nfreq_tbl &lt;- function(df, ..., percent = TRUE){\n  group &lt;- quos(...)\n  \n  out &lt;- df %&gt;% \n    group_by(!!!group) %&gt;% \n    summarise(freq = n()) %&gt;% \n    arrange(desc(freq)) %&gt;% \n    ungroup()\n  \n  if(percent == TRUE){\n    out &lt;- out %&gt;% \n      mutate(percentage = 100*freq/sum(freq))\n  }\n  return(out)\n}",
    "crumbs": [
      "R snippet",
      "dplyr"
    ]
  },
  {
    "objectID": "r-snippet/dplyr.html#references",
    "href": "r-snippet/dplyr.html#references",
    "title": "dplyr",
    "section": "References",
    "text": "References\n\ntidyeval",
    "crumbs": [
      "R snippet",
      "dplyr"
    ]
  },
  {
    "objectID": "r-snippet/gganimate.html",
    "href": "r-snippet/gganimate.html",
    "title": "gganimate",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gganimate)\nlibrary(gapminder)\ntheme_set(theme_classic())\n\ngap &lt;- gapminder %&gt;%\n  filter(continent == \"Asia\") %&gt;%\n  group_by(year) %&gt;%\n  # The * 1 makes it possible to have non-integer ranks while sliding\n  mutate(rank = min_rank(-gdpPercap) * 1) %&gt;%\n  ungroup()\n\np &lt;- ggplot(gap, aes(rank, group = country, \n                     fill = as.factor(country), color = as.factor(country))) +\n  geom_tile(aes(y = gdpPercap/2,\n                height = gdpPercap,\n                width = 0.9), alpha = 0.8, color = NA) +\n  \n  # text in x-axis (requires clip = \"off\" in coord_*)\n  # paste(country, \" \")  is a hack to make pretty spacing, since hjust &gt; 1 \n  #   leads to weird artifacts in text spacing.\n  geom_text(aes(y = 0, label = paste(country, \" \")), vjust = 0.2, hjust = 1) +\n  \n  coord_flip(clip = \"off\", expand = FALSE) +\n  scale_y_continuous(labels = scales::comma) +\n  scale_x_reverse() +\n  guides(color = FALSE, fill = FALSE) +\n  \n  labs(title='{closest_state}', x = \"\", y = \"GFP per capita\") +\n  theme(plot.title = element_text(hjust = 0, size = 22),\n        axis.ticks.y = element_blank(),  # These relate to the axes post-flip\n        axis.text.y  = element_blank(),  # These relate to the axes post-flip\n        plot.margin = margin(1,1,1,4, \"cm\")) +\n  \n  transition_states(year, transition_length = 4, state_length = 1) +\n  ease_aes('cubic-in-out')\n\nanimate(p, fps = 25, duration = 20, width = 800, height = 600)",
    "crumbs": [
      "R snippet",
      "gganimate"
    ]
  },
  {
    "objectID": "Crash-Course-Matplotlib/Exercise 3 - Matplotlib.html",
    "href": "Crash-Course-Matplotlib/Exercise 3 - Matplotlib.html",
    "title": "Matplotlib - 2D and 3D plotting in Python",
    "section": "",
    "text": "# This line configures matplotlib to show figures embedded in the notebook, \n# instead of opening a new window for each figure, similar to Exercise-0-Jupyter \n%matplotlib inline\n\n\n\nMatplotlib is an excellent 2D and 3D graphics library for generating scientific figures. Some of the many advantages of this library include:\n\nEasy to get started\nGreat control of every element in a figure, including figure size and DPI.\nHigh-quality output in many formats, including PNG, PDF, SVG, EPS, and PGF.\nGUI for interactively exploring figures and support for headless generation of figure files (useful for batch jobs).\n\nOne of the key features of matplotlib that I would like to emphasize, and that I think makes matplotlib highly suitable for generating figures for scientific publications is that all aspects of the figure can be controlled programmatically. This is important for reproducibility and convenient when one needs to regenerate the figure with updated data or change its appearance.\nMore information at the Matplotlib web page: http://matplotlib.org/\nTo get started using Matplotlib in a Python program, either include the symbols from the pylab module (the easy way):\n\nfrom pylab import *\n\nor import the matplotlib.pyplot module under the name plt (the tidy way):\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n\nimport numpy as np\n\n\n\n\nThe easiest way to get started with plotting using matplotlib is often to use the MATLAB-like API provided by matplotlib.\nIt is designed to be compatible with MATLAB’s plotting functions, so it is easy to get started with if you are familiar with MATLAB.\nTo use this API from matplotlib, we need to include the symbols in the pylab module:\n\nfrom pylab import *\n\n\n\nA simple figure with MATLAB-like plotting API:\n\nx = np.linspace(0, 5, 10)\ny = x ** 2\n\n\nfigure()\nplot(x, y, 'r')\nxlabel('x')\nylabel('y')\ntitle('title')\nshow()\n\n\n\n\n\n\n\n\nMost of the plotting related functions in MATLAB are covered by the pylab module. For example, subplot and color/symbol selection:\n\nsubplot(1,2,1)\nplot(x, y, 'r--')\nsubplot(1,2,2)\nplot(y, x, 'g*-');\n\n\n\n\n\n\n\n\nThe good thing about the pylab MATLAB-style API is that it is easy to get started with if you are familiar with MATLAB, and it has a minumum of coding overhead for simple plots.\nHowever, I’d encourrage not using the MATLAB compatible API for anything but the simplest figures.\nInstead, I recommend learning and using matplotlib’s object-oriented plotting API. It is remarkably powerful. For advanced figures with subplots, insets and other components it is very nice to work with.\n\n\n\n\nThe main idea with object-oriented programming is to have objects that one can apply functions and actions on, and no object or program states should be global (such as the MATLAB-like API). The real advantage of this approach becomes apparent when more than one figure is created, or when a figure contains more than one subplot.\nTo use the object-oriented API we start out very much like in the previous example, but instead of creating a new global figure instance we store a reference to the newly created figure instance in the fig variable, and from it we create a new axis instance axes using the add_axes method in the Figure class instance fig:\n\nfig = plt.figure()\n\naxes = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # left, bottom, width, height (range 0 to 1)\n\naxes.plot(x, y, 'r')\n\naxes.set_xlabel('x')\naxes.set_ylabel('y')\naxes.set_title('title');\n\n\n\n\n\n\n\n\nAlthough a little bit more code is involved, the advantage is that we now have full control of where the plot axes are placed, and we can easily add more than one axis to the figure:\n\nfig = plt.figure()\n\naxes1 = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # main axes\naxes2 = fig.add_axes([0.2, 0.5, 0.4, 0.3]) # inset axes\n\n# main figure\naxes1.plot(x, y, 'r')\naxes1.set_xlabel('x')\naxes1.set_ylabel('y')\naxes1.set_title('title')\n\n# insert\naxes2.plot(y, x, 'g')\naxes2.set_xlabel('y')\naxes2.set_ylabel('x')\naxes2.set_title('insert title');\n\n\n\n\n\n\n\n\nIf we don’t care about being explicit about where our plot axes are placed in the figure canvas, then we can use one of the many axis layout managers in matplotlib. My favorite is subplots, which can be used like this:\n\nfig, axes = plt.subplots()\n\naxes.plot(x, y, 'r')\naxes.set_xlabel('x')\naxes.set_ylabel('y')\naxes.set_title('title');\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(nrows=1, ncols=2)\n\nfor ax in axes:\n    ax.plot(x, y, 'r')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('title')\n\n\n\n\n\n\n\n\nThat was easy, but it isn’t so pretty with overlapping figure axes and labels, right?\nWe can deal with that by using the fig.tight_layout method, which automatically adjusts the positions of the axes on the figure canvas so that there is no overlapping content:\n\nfig, axes = plt.subplots(nrows=1, ncols=2)\n\nfor ax in axes:\n    ax.plot(x, y, 'r')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('title')\n    \nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\nTo save a figure to a file we can use the savefig method in the Figure class:\n\nfig.savefig(\"filename.png\")\n\n\n\nMatplotlib can generate high-quality output in a number formats, including PNG, JPG, EPS, SVG, PGF and PDF. For scientific papers, I recommend using PDF whenever possible. (LaTeX documents compiled with pdflatex can include PDFs using the includegraphics command). In some cases, PGF can also be good alternative.\n\n\n\n\nNow that we have covered the basics of how to create a figure canvas and add axes instances to the canvas, let’s look at how decorate a figure with titles, axis labels, and legends.\nFigure titles\nA title can be added to each axis instance in a figure. To set the title, use the set_title method in the axes instance:\n\nax.set_title(\"title\");\n\nAxis labels\nSimilarly, with the methods set_xlabel and set_ylabel, we can set the labels of the X and Y axes:\n\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\");\n\nLegends\nLegends for curves in a figure can be added in two ways. One method is to use the legend method of the axis object and pass a list/tuple of legend texts for the previously defined curves:\n\nax.legend([\"curve1\", \"curve2\", \"curve3\"]);\n\nThe method described above follows the MATLAB API. It is somewhat prone to errors and unflexible if curves are added to or removed from the figure (resulting in a wrongly labelled curve).\nA better method is to use the label=\"label text\" keyword argument when plots or other objects are added to the figure, and then using the legend method without arguments to add the legend to the figure:\n\nax.plot(x, x**2, label=\"curve1\")\nax.plot(x, x**3, label=\"curve2\")\nax.legend();\n\nThe advantage with this method is that if curves are added or removed from the figure, the legend is automatically updated accordingly.\nThe legend function takes an optional keyword argument loc that can be used to specify where in the figure the legend is to be drawn. The allowed values of loc are numerical codes for the various places the legend can be drawn. See http://matplotlib.org/users/legend_guide.html#legend-location for details. Some of the most common loc values are:\n\nax.legend(loc=0) # let matplotlib decide the optimal location\nax.legend(loc=1) # upper right corner\nax.legend(loc=2) # upper left corner\nax.legend(loc=3) # lower left corner\nax.legend(loc=4) # lower right corner\n# .. many more options are available\n\nThe following figure shows how to use the figure title, axis labels and legends described above:\n\nfig, ax = plt.subplots()\n\nax.plot(x, x**2, label=\"y = x**2\")\nax.plot(x, x**3, label=\"y = x**3\")\nax.legend(loc=2); # upper left corner\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('title');\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith matplotlib, we can define the colors of lines and other graphical elements in a number of ways. First of all, we can use the MATLAB-like syntax where 'b' means blue, 'g' means green, etc. The MATLAB API for selecting line styles are also supported: where, for example, ‘b.-’ means a blue line with dots:\n\n# MATLAB style line color and style \nax.plot(x, x**2, 'b.-') # blue line with dots\nax.plot(x, x**3, 'g--') # green dashed line\n\nWe can also define colors by their names or RGB hex codes and optionally provide an alpha value using the color and alpha keyword arguments:\n\nfig, ax = plt.subplots()\n\nax.plot(x, x+1, color=\"red\", alpha=0.5) # half-transparant red\nax.plot(x, x+2, color=\"#1155dd\")        # RGB hex code for a bluish color\nax.plot(x, x+3, color=\"#15cc55\")        # RGB hex code for a greenish color\n\n\n\n\n\n\n\n\n\n\n\nTo change the line width, we can use the linewidth or lw keyword argument. The line style can be selected using the linestyle or ls keyword arguments:\n\nfig, ax = plt.subplots(figsize=(12,6))\n\nax.plot(x, x+1, color=\"blue\", linewidth=0.25)\nax.plot(x, x+2, color=\"blue\", linewidth=0.50)\nax.plot(x, x+3, color=\"blue\", linewidth=1.00)\nax.plot(x, x+4, color=\"blue\", linewidth=2.00)\n\n# possible linestype options ‘-‘, ‘--’, ‘-.’, ‘:’, ‘steps’\nax.plot(x, x+5, color=\"red\", lw=2, linestyle='-')\nax.plot(x, x+6, color=\"red\", lw=2, ls='-.')\nax.plot(x, x+7, color=\"red\", lw=2, ls=':')\n\n# custom dash\nline, = ax.plot(x, x+8, color=\"black\", lw=1.50)\nline.set_dashes([5, 10, 15, 10]) # format: line length, space length, ...\n\n# possible marker symbols: marker = '+', 'o', '*', 's', ',', '.', '1', '2', '3', '4', ...\nax.plot(x, x+ 9, color=\"green\", lw=2, ls='--', marker='+')\nax.plot(x, x+10, color=\"green\", lw=2, ls='--', marker='o')\nax.plot(x, x+11, color=\"green\", lw=2, ls='--', marker='s')\nax.plot(x, x+12, color=\"green\", lw=2, ls='--', marker='1')\n\n# marker size and color\nax.plot(x, x+13, color=\"purple\", lw=1, ls='-', marker='o', markersize=2)\nax.plot(x, x+14, color=\"purple\", lw=1, ls='-', marker='o', markersize=4)\nax.plot(x, x+15, color=\"purple\", lw=1, ls='-', marker='o', markersize=8, markerfacecolor=\"red\")\nax.plot(x, x+16, color=\"purple\", lw=1, ls='-', marker='s', markersize=8, \n        markerfacecolor=\"yellow\", markeredgewidth=2, markeredgecolor=\"blue\");\n\n\n\n\n\n\n\n\n\n\n\n\nThe appearance of the axes is an important aspect of a figure that we often need to modify to make a publication quality graphics. We need to be able to control where the ticks and labels are placed, modify the font size and possibly the labels used on the axes. In this section we will look at controling those properties in a matplotlib figure.\n\n\nThe first thing we might want to configure is the ranges of the axes. We can do this using the set_ylim and set_xlim methods in the axis object, or axis('tight') for automatrically getting “tightly fitted” axes ranges:\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\n\naxes[0].plot(x, x**2, x, x**3)\naxes[0].set_title(\"default axes ranges\")\n\naxes[1].plot(x, x**2, x, x**3)\naxes[1].axis('tight')\naxes[1].set_title(\"tight axes\")\n\naxes[2].plot(x, x**2, x, x**3)\naxes[2].set_ylim([0, 60])\naxes[2].set_xlim([2, 5])\naxes[2].set_title(\"custom axes range\");\n\n\n\n\n\n\n\n\n\n\n\nIt is also possible to set a logarithmic scale for one or both axes. This functionality is in fact only one application of a more general transformation system in Matplotlib. Each of the axes’ scales are set seperately using set_xscale and set_yscale methods which accept one parameter (with the value “log” in this case):\n\nfig, axes = plt.subplots(1, 2, figsize=(10,4))\n      \naxes[0].plot(x, x**2, x, np.exp(x))\naxes[0].set_title(\"Normal scale\")\n\naxes[1].plot(x, x**2, x, np.exp(x))\naxes[1].set_yscale(\"log\")\naxes[1].set_title(\"Logarithmic scale (y)\");\n\n\n\n\n\n\n\n\n\n\n\nWith large numbers on axes, it is often better use scientific notation:\n\nfig, ax = plt.subplots(1, 1)\n      \nax.plot(x, x**2, x, np.exp(x))\nax.set_title(\"scientific notation\")\n\nax.set_yticks([0, 50, 100, 150])\n\nfrom matplotlib import ticker\nformatter = ticker.ScalarFormatter(useMathText=True)\nformatter.set_scientific(True) \nformatter.set_powerlimits((-1,1)) \nax.yaxis.set_major_formatter(formatter) \n\n\n\n\n\n\n\n\n\n\n\n\nWith the grid method in the axis object, we can turn on and off grid lines. We can also customize the appearance of the grid lines using the same keyword arguments as the plot function:\n\nfig, axes = plt.subplots(1, 2, figsize=(10,3))\n\n# default grid appearance\naxes[0].plot(x, x**2, x, x**3, lw=2)\naxes[0].grid(True)\n\n# custom grid appearance\naxes[1].plot(x, x**2, x, x**3, lw=2)\naxes[1].grid(color='b', alpha=0.5, linestyle='dashed', linewidth=0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\n\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\n\nax.xaxis.set_ticks_position('bottom')\nax.spines['bottom'].set_position(('data',0)) # set position of x spine to x=0\n\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data',0))   # set position of y spine to y=0\n\nxx = np.linspace(-0.75, 1., 100)\nax.plot(xx, xx**3);\n\n\n\n\n\n\n\n\n\n\n\nIn addition to the regular plot method, there are a number of other functions for generating different kind of plots. See the matplotlib plot gallery for a complete list of available plot types: http://matplotlib.org/gallery.html. Some of the more useful ones are show below:\n\nn = np.array([0,1,2,3,4,5])\n\n\nfig, axes = plt.subplots(1, 4, figsize=(12,3))\n\naxes[0].scatter(xx, xx + 0.25*np.random.randn(len(xx)))\naxes[0].set_title(\"scatter\")\n\naxes[1].step(n, n**2, lw=2)\naxes[1].set_title(\"step\")\n\naxes[2].bar(n, n**2, align=\"center\", width=0.5, alpha=0.5)\naxes[2].set_title(\"bar\")\n\naxes[3].fill_between(x, x**2, x**3, color=\"green\", alpha=0.5);\naxes[3].set_title(\"fill_between\");\n\n\n\n\n\n\n\n\n\n# A histogram\nn = np.random.randn(100000)\nfig, axes = plt.subplots(1, 2, figsize=(12,4))\n\naxes[0].hist(n)\naxes[0].set_title(\"Default histogram\")\naxes[0].set_xlim((min(n), max(n)))\n\naxes[1].hist(n, cumulative=True, bins=50)\naxes[1].set_title(\"Cumulative detailed histogram\")\naxes[1].set_xlim((min(n), max(n)));\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttp://www.matplotlib.org - The project web page for matplotlib.\nhttps://github.com/matplotlib/matplotlib - The source code for matplotlib.\nhttp://matplotlib.org/gallery.html - A large gallery showcaseing various types of plots matplotlib can create. Highly recommended!\nhttp://www.loria.fr/~rougier/teaching/matplotlib - A good matplotlib tutorial.\nhttp://scipy-lectures.github.io/matplotlib/matplotlib.html - Another good matplotlib reference.\n\n\n\n\n\n%reload_ext version_information\n%version_information numpy, scipy, matplotlib\n\n\n\n\n\nSoftware\nVersion\n\n\nPython\n2.7.10 64bit [GCC 4.2.1 (Apple Inc. build 5577)]\n\n\nIPython\n3.2.1\n\n\nOS\nDarwin 14.1.0 x86_64 i386 64bit\n\n\nnumpy\n1.9.2\n\n\nscipy\n0.16.0\n\n\nmatplotlib\n1.4.3\n\n\nSat Aug 15 11:30:23 2015 JST",
    "crumbs": [
      "Matplotlib - 2D and 3D plotting in Python"
    ]
  },
  {
    "objectID": "Crash-Course-Matplotlib/Exercise 3 - Matplotlib.html#introduction",
    "href": "Crash-Course-Matplotlib/Exercise 3 - Matplotlib.html#introduction",
    "title": "Matplotlib - 2D and 3D plotting in Python",
    "section": "",
    "text": "Matplotlib is an excellent 2D and 3D graphics library for generating scientific figures. Some of the many advantages of this library include:\n\nEasy to get started\nGreat control of every element in a figure, including figure size and DPI.\nHigh-quality output in many formats, including PNG, PDF, SVG, EPS, and PGF.\nGUI for interactively exploring figures and support for headless generation of figure files (useful for batch jobs).\n\nOne of the key features of matplotlib that I would like to emphasize, and that I think makes matplotlib highly suitable for generating figures for scientific publications is that all aspects of the figure can be controlled programmatically. This is important for reproducibility and convenient when one needs to regenerate the figure with updated data or change its appearance.\nMore information at the Matplotlib web page: http://matplotlib.org/\nTo get started using Matplotlib in a Python program, either include the symbols from the pylab module (the easy way):\n\nfrom pylab import *\n\nor import the matplotlib.pyplot module under the name plt (the tidy way):\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n\nimport numpy as np",
    "crumbs": [
      "Matplotlib - 2D and 3D plotting in Python"
    ]
  },
  {
    "objectID": "Crash-Course-Matplotlib/Exercise 3 - Matplotlib.html#matlab-like-api",
    "href": "Crash-Course-Matplotlib/Exercise 3 - Matplotlib.html#matlab-like-api",
    "title": "Matplotlib - 2D and 3D plotting in Python",
    "section": "",
    "text": "The easiest way to get started with plotting using matplotlib is often to use the MATLAB-like API provided by matplotlib.\nIt is designed to be compatible with MATLAB’s plotting functions, so it is easy to get started with if you are familiar with MATLAB.\nTo use this API from matplotlib, we need to include the symbols in the pylab module:\n\nfrom pylab import *\n\n\n\nA simple figure with MATLAB-like plotting API:\n\nx = np.linspace(0, 5, 10)\ny = x ** 2\n\n\nfigure()\nplot(x, y, 'r')\nxlabel('x')\nylabel('y')\ntitle('title')\nshow()\n\n\n\n\n\n\n\n\nMost of the plotting related functions in MATLAB are covered by the pylab module. For example, subplot and color/symbol selection:\n\nsubplot(1,2,1)\nplot(x, y, 'r--')\nsubplot(1,2,2)\nplot(y, x, 'g*-');\n\n\n\n\n\n\n\n\nThe good thing about the pylab MATLAB-style API is that it is easy to get started with if you are familiar with MATLAB, and it has a minumum of coding overhead for simple plots.\nHowever, I’d encourrage not using the MATLAB compatible API for anything but the simplest figures.\nInstead, I recommend learning and using matplotlib’s object-oriented plotting API. It is remarkably powerful. For advanced figures with subplots, insets and other components it is very nice to work with.",
    "crumbs": [
      "Matplotlib - 2D and 3D plotting in Python"
    ]
  },
  {
    "objectID": "Crash-Course-Matplotlib/Exercise 3 - Matplotlib.html#the-matplotlib-object-oriented-api",
    "href": "Crash-Course-Matplotlib/Exercise 3 - Matplotlib.html#the-matplotlib-object-oriented-api",
    "title": "Matplotlib - 2D and 3D plotting in Python",
    "section": "",
    "text": "The main idea with object-oriented programming is to have objects that one can apply functions and actions on, and no object or program states should be global (such as the MATLAB-like API). The real advantage of this approach becomes apparent when more than one figure is created, or when a figure contains more than one subplot.\nTo use the object-oriented API we start out very much like in the previous example, but instead of creating a new global figure instance we store a reference to the newly created figure instance in the fig variable, and from it we create a new axis instance axes using the add_axes method in the Figure class instance fig:\n\nfig = plt.figure()\n\naxes = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # left, bottom, width, height (range 0 to 1)\n\naxes.plot(x, y, 'r')\n\naxes.set_xlabel('x')\naxes.set_ylabel('y')\naxes.set_title('title');\n\n\n\n\n\n\n\n\nAlthough a little bit more code is involved, the advantage is that we now have full control of where the plot axes are placed, and we can easily add more than one axis to the figure:\n\nfig = plt.figure()\n\naxes1 = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # main axes\naxes2 = fig.add_axes([0.2, 0.5, 0.4, 0.3]) # inset axes\n\n# main figure\naxes1.plot(x, y, 'r')\naxes1.set_xlabel('x')\naxes1.set_ylabel('y')\naxes1.set_title('title')\n\n# insert\naxes2.plot(y, x, 'g')\naxes2.set_xlabel('y')\naxes2.set_ylabel('x')\naxes2.set_title('insert title');\n\n\n\n\n\n\n\n\nIf we don’t care about being explicit about where our plot axes are placed in the figure canvas, then we can use one of the many axis layout managers in matplotlib. My favorite is subplots, which can be used like this:\n\nfig, axes = plt.subplots()\n\naxes.plot(x, y, 'r')\naxes.set_xlabel('x')\naxes.set_ylabel('y')\naxes.set_title('title');\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(nrows=1, ncols=2)\n\nfor ax in axes:\n    ax.plot(x, y, 'r')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('title')\n\n\n\n\n\n\n\n\nThat was easy, but it isn’t so pretty with overlapping figure axes and labels, right?\nWe can deal with that by using the fig.tight_layout method, which automatically adjusts the positions of the axes on the figure canvas so that there is no overlapping content:\n\nfig, axes = plt.subplots(nrows=1, ncols=2)\n\nfor ax in axes:\n    ax.plot(x, y, 'r')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('title')\n    \nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\nTo save a figure to a file we can use the savefig method in the Figure class:\n\nfig.savefig(\"filename.png\")\n\n\n\nMatplotlib can generate high-quality output in a number formats, including PNG, JPG, EPS, SVG, PGF and PDF. For scientific papers, I recommend using PDF whenever possible. (LaTeX documents compiled with pdflatex can include PDFs using the includegraphics command). In some cases, PGF can also be good alternative.\n\n\n\n\nNow that we have covered the basics of how to create a figure canvas and add axes instances to the canvas, let’s look at how decorate a figure with titles, axis labels, and legends.\nFigure titles\nA title can be added to each axis instance in a figure. To set the title, use the set_title method in the axes instance:\n\nax.set_title(\"title\");\n\nAxis labels\nSimilarly, with the methods set_xlabel and set_ylabel, we can set the labels of the X and Y axes:\n\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\");\n\nLegends\nLegends for curves in a figure can be added in two ways. One method is to use the legend method of the axis object and pass a list/tuple of legend texts for the previously defined curves:\n\nax.legend([\"curve1\", \"curve2\", \"curve3\"]);\n\nThe method described above follows the MATLAB API. It is somewhat prone to errors and unflexible if curves are added to or removed from the figure (resulting in a wrongly labelled curve).\nA better method is to use the label=\"label text\" keyword argument when plots or other objects are added to the figure, and then using the legend method without arguments to add the legend to the figure:\n\nax.plot(x, x**2, label=\"curve1\")\nax.plot(x, x**3, label=\"curve2\")\nax.legend();\n\nThe advantage with this method is that if curves are added or removed from the figure, the legend is automatically updated accordingly.\nThe legend function takes an optional keyword argument loc that can be used to specify where in the figure the legend is to be drawn. The allowed values of loc are numerical codes for the various places the legend can be drawn. See http://matplotlib.org/users/legend_guide.html#legend-location for details. Some of the most common loc values are:\n\nax.legend(loc=0) # let matplotlib decide the optimal location\nax.legend(loc=1) # upper right corner\nax.legend(loc=2) # upper left corner\nax.legend(loc=3) # lower left corner\nax.legend(loc=4) # lower right corner\n# .. many more options are available\n\nThe following figure shows how to use the figure title, axis labels and legends described above:\n\nfig, ax = plt.subplots()\n\nax.plot(x, x**2, label=\"y = x**2\")\nax.plot(x, x**3, label=\"y = x**3\")\nax.legend(loc=2); # upper left corner\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('title');\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith matplotlib, we can define the colors of lines and other graphical elements in a number of ways. First of all, we can use the MATLAB-like syntax where 'b' means blue, 'g' means green, etc. The MATLAB API for selecting line styles are also supported: where, for example, ‘b.-’ means a blue line with dots:\n\n# MATLAB style line color and style \nax.plot(x, x**2, 'b.-') # blue line with dots\nax.plot(x, x**3, 'g--') # green dashed line\n\nWe can also define colors by their names or RGB hex codes and optionally provide an alpha value using the color and alpha keyword arguments:\n\nfig, ax = plt.subplots()\n\nax.plot(x, x+1, color=\"red\", alpha=0.5) # half-transparant red\nax.plot(x, x+2, color=\"#1155dd\")        # RGB hex code for a bluish color\nax.plot(x, x+3, color=\"#15cc55\")        # RGB hex code for a greenish color\n\n\n\n\n\n\n\n\n\n\n\nTo change the line width, we can use the linewidth or lw keyword argument. The line style can be selected using the linestyle or ls keyword arguments:\n\nfig, ax = plt.subplots(figsize=(12,6))\n\nax.plot(x, x+1, color=\"blue\", linewidth=0.25)\nax.plot(x, x+2, color=\"blue\", linewidth=0.50)\nax.plot(x, x+3, color=\"blue\", linewidth=1.00)\nax.plot(x, x+4, color=\"blue\", linewidth=2.00)\n\n# possible linestype options ‘-‘, ‘--’, ‘-.’, ‘:’, ‘steps’\nax.plot(x, x+5, color=\"red\", lw=2, linestyle='-')\nax.plot(x, x+6, color=\"red\", lw=2, ls='-.')\nax.plot(x, x+7, color=\"red\", lw=2, ls=':')\n\n# custom dash\nline, = ax.plot(x, x+8, color=\"black\", lw=1.50)\nline.set_dashes([5, 10, 15, 10]) # format: line length, space length, ...\n\n# possible marker symbols: marker = '+', 'o', '*', 's', ',', '.', '1', '2', '3', '4', ...\nax.plot(x, x+ 9, color=\"green\", lw=2, ls='--', marker='+')\nax.plot(x, x+10, color=\"green\", lw=2, ls='--', marker='o')\nax.plot(x, x+11, color=\"green\", lw=2, ls='--', marker='s')\nax.plot(x, x+12, color=\"green\", lw=2, ls='--', marker='1')\n\n# marker size and color\nax.plot(x, x+13, color=\"purple\", lw=1, ls='-', marker='o', markersize=2)\nax.plot(x, x+14, color=\"purple\", lw=1, ls='-', marker='o', markersize=4)\nax.plot(x, x+15, color=\"purple\", lw=1, ls='-', marker='o', markersize=8, markerfacecolor=\"red\")\nax.plot(x, x+16, color=\"purple\", lw=1, ls='-', marker='s', markersize=8, \n        markerfacecolor=\"yellow\", markeredgewidth=2, markeredgecolor=\"blue\");\n\n\n\n\n\n\n\n\n\n\n\n\nThe appearance of the axes is an important aspect of a figure that we often need to modify to make a publication quality graphics. We need to be able to control where the ticks and labels are placed, modify the font size and possibly the labels used on the axes. In this section we will look at controling those properties in a matplotlib figure.\n\n\nThe first thing we might want to configure is the ranges of the axes. We can do this using the set_ylim and set_xlim methods in the axis object, or axis('tight') for automatrically getting “tightly fitted” axes ranges:\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\n\naxes[0].plot(x, x**2, x, x**3)\naxes[0].set_title(\"default axes ranges\")\n\naxes[1].plot(x, x**2, x, x**3)\naxes[1].axis('tight')\naxes[1].set_title(\"tight axes\")\n\naxes[2].plot(x, x**2, x, x**3)\naxes[2].set_ylim([0, 60])\naxes[2].set_xlim([2, 5])\naxes[2].set_title(\"custom axes range\");\n\n\n\n\n\n\n\n\n\n\n\nIt is also possible to set a logarithmic scale for one or both axes. This functionality is in fact only one application of a more general transformation system in Matplotlib. Each of the axes’ scales are set seperately using set_xscale and set_yscale methods which accept one parameter (with the value “log” in this case):\n\nfig, axes = plt.subplots(1, 2, figsize=(10,4))\n      \naxes[0].plot(x, x**2, x, np.exp(x))\naxes[0].set_title(\"Normal scale\")\n\naxes[1].plot(x, x**2, x, np.exp(x))\naxes[1].set_yscale(\"log\")\naxes[1].set_title(\"Logarithmic scale (y)\");\n\n\n\n\n\n\n\n\n\n\n\nWith large numbers on axes, it is often better use scientific notation:\n\nfig, ax = plt.subplots(1, 1)\n      \nax.plot(x, x**2, x, np.exp(x))\nax.set_title(\"scientific notation\")\n\nax.set_yticks([0, 50, 100, 150])\n\nfrom matplotlib import ticker\nformatter = ticker.ScalarFormatter(useMathText=True)\nformatter.set_scientific(True) \nformatter.set_powerlimits((-1,1)) \nax.yaxis.set_major_formatter(formatter) \n\n\n\n\n\n\n\n\n\n\n\n\nWith the grid method in the axis object, we can turn on and off grid lines. We can also customize the appearance of the grid lines using the same keyword arguments as the plot function:\n\nfig, axes = plt.subplots(1, 2, figsize=(10,3))\n\n# default grid appearance\naxes[0].plot(x, x**2, x, x**3, lw=2)\naxes[0].grid(True)\n\n# custom grid appearance\naxes[1].plot(x, x**2, x, x**3, lw=2)\naxes[1].grid(color='b', alpha=0.5, linestyle='dashed', linewidth=0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\n\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\n\nax.xaxis.set_ticks_position('bottom')\nax.spines['bottom'].set_position(('data',0)) # set position of x spine to x=0\n\nax.yaxis.set_ticks_position('left')\nax.spines['left'].set_position(('data',0))   # set position of y spine to y=0\n\nxx = np.linspace(-0.75, 1., 100)\nax.plot(xx, xx**3);\n\n\n\n\n\n\n\n\n\n\n\nIn addition to the regular plot method, there are a number of other functions for generating different kind of plots. See the matplotlib plot gallery for a complete list of available plot types: http://matplotlib.org/gallery.html. Some of the more useful ones are show below:\n\nn = np.array([0,1,2,3,4,5])\n\n\nfig, axes = plt.subplots(1, 4, figsize=(12,3))\n\naxes[0].scatter(xx, xx + 0.25*np.random.randn(len(xx)))\naxes[0].set_title(\"scatter\")\n\naxes[1].step(n, n**2, lw=2)\naxes[1].set_title(\"step\")\n\naxes[2].bar(n, n**2, align=\"center\", width=0.5, alpha=0.5)\naxes[2].set_title(\"bar\")\n\naxes[3].fill_between(x, x**2, x**3, color=\"green\", alpha=0.5);\naxes[3].set_title(\"fill_between\");\n\n\n\n\n\n\n\n\n\n# A histogram\nn = np.random.randn(100000)\nfig, axes = plt.subplots(1, 2, figsize=(12,4))\n\naxes[0].hist(n)\naxes[0].set_title(\"Default histogram\")\naxes[0].set_xlim((min(n), max(n)))\n\naxes[1].hist(n, cumulative=True, bins=50)\naxes[1].set_title(\"Cumulative detailed histogram\")\naxes[1].set_xlim((min(n), max(n)));",
    "crumbs": [
      "Matplotlib - 2D and 3D plotting in Python"
    ]
  },
  {
    "objectID": "Crash-Course-Matplotlib/Exercise 3 - Matplotlib.html#further-reading",
    "href": "Crash-Course-Matplotlib/Exercise 3 - Matplotlib.html#further-reading",
    "title": "Matplotlib - 2D and 3D plotting in Python",
    "section": "",
    "text": "http://www.matplotlib.org - The project web page for matplotlib.\nhttps://github.com/matplotlib/matplotlib - The source code for matplotlib.\nhttp://matplotlib.org/gallery.html - A large gallery showcaseing various types of plots matplotlib can create. Highly recommended!\nhttp://www.loria.fr/~rougier/teaching/matplotlib - A good matplotlib tutorial.\nhttp://scipy-lectures.github.io/matplotlib/matplotlib.html - Another good matplotlib reference.",
    "crumbs": [
      "Matplotlib - 2D and 3D plotting in Python"
    ]
  },
  {
    "objectID": "Crash-Course-Matplotlib/Exercise 3 - Matplotlib.html#versions",
    "href": "Crash-Course-Matplotlib/Exercise 3 - Matplotlib.html#versions",
    "title": "Matplotlib - 2D and 3D plotting in Python",
    "section": "",
    "text": "%reload_ext version_information\n%version_information numpy, scipy, matplotlib\n\n\n\n\n\nSoftware\nVersion\n\n\nPython\n2.7.10 64bit [GCC 4.2.1 (Apple Inc. build 5577)]\n\n\nIPython\n3.2.1\n\n\nOS\nDarwin 14.1.0 x86_64 i386 64bit\n\n\nnumpy\n1.9.2\n\n\nscipy\n0.16.0\n\n\nmatplotlib\n1.4.3\n\n\nSat Aug 15 11:30:23 2015 JST",
    "crumbs": [
      "Matplotlib - 2D and 3D plotting in Python"
    ]
  }
]