[
  {
    "objectID": "comparision/plsql-db2.html",
    "href": "comparision/plsql-db2.html",
    "title": "plsql-db2",
    "section": "",
    "text": "Here’s a table comparing DB2 and PL/SQL based on the provided features and their syntax:\nThese SQL commands are mainly categorized into five categories:"
  },
  {
    "objectID": "comparision/plsql-db2.html#basic-sql-commands",
    "href": "comparision/plsql-db2.html#basic-sql-commands",
    "title": "plsql-db2",
    "section": "Basic SQL commands:",
    "text": "Basic SQL commands:\n\nDDL – Data Definition Language\n\n\n\n\n\n\n\n\nFeature\nPL/SQL Syntax\nDB2 Syntax\n\n\n\n\nCreate Table\nCREATE TABLE new_table AS SELECT * FROM existing_table;\nsimilar\n\n\nDrop Table\nDROP TABLE table_name [PURGE];\nDROP TABLE IF EXISTS table_name;\n\n\nRename Columns\nALTER TABLE table_name RENAME COLUMN old_column TO new_column;\nsimilar\n\n\nModify Columns\nALTER TABLE table_name MODIFY COLUMN column_name datatype;\nsimilar\n\n\nAdd Columns\nALTER TABLE table_name ADD COLUMN new_column datatype;\nsimilar\n\n\n\nDQL – Data Query Language\n\n\n\n\n\n\n\n\nFeature\nPL/SQL Syntax\nDB2 Syntax\n\n\n\n\nLeft Join\nSELECT * FROM table1 LEFT JOIN table2 ON table1.column = table2.column;\nsimilar\n\n\nInner Join\nSELECT * FROM table1 INNER JOIN table2 ON table1.column = table2.column;\nsimilar\n\n\nFull Join\nSELECT * FROM table1 FULL JOIN table2 ON table1.column = table2.column;\nsimilar\n\n\nAnti Join\nSELECT column1, column2 FROM table1 WHERE NOT EXISTS (SELECT * FROM table2 WHERE table1.column = table2.column)\nsimilar\n\n\nMerge\nMERGE INTO table_name USING source_table ON (table_name.column = source_table.column) WHEN MATCHED THEN UPDATE SET column1 = value1 WHEN NOT MATCHED THEN INSERT (column1) VALUES (value1);\nsimilar\n\n\nInsert Into\nINSERT INTO table_name (column1, column2) VALUES (value1, value2);\nsimilar\n\n\nUpdate\nUPDATE staff a SET salary = (SELECT AVG(salary) + 2000 FROM staff b WHERE a.dept = b.dept) WHERE id &lt; 60;\nsimilar\n\n\nTruncate Table\nTRUNCATE TABLE table_name;\nTRUNCATE TABLE table_name IMMEDIATE;\n\n\nTemporary Table\nDECLARE GLOBAL TEMPORARY TABLE table_name (column1 datatype, column2 datatype);\nCREATE GLOBAL TEMPORARY TABLE table_name (column1 datatype, column2 datatype);\n\n\nGroup By\nSELECT column1, COUNT(*) FROM table_name GROUP BY column1;\nsimilar\n\n\nPartition By\nSELECT column1, RANK() OVER (PARTITION BY column2 ORDER BY column3) FROM table_name;\nsimilar\n\n\nRun Procedure\nEXECUTE procedure_name;\nCALL procedure_name();\n\n\nLoop\nFOR row IN (SELECT * FROM table_name) LOOP &lt;statements&gt; END LOOP;\nFOR row AS cursor_name CURSOR FOR SELECT * FROM table_name DO &lt;statements&gt; END FOR;\n\n\nParallel\nINSERT /*+APPEND PARALLEL(8)*/ INTO\nSELECT * FROM table_name PARALLEL 4;\n\n\n\nMERGE /*+ parallel(8) */ INTO\n\n\n\n\nSELECT /*+ PARALLEL(8) */ * FROM\n\n\n\n\nDCL – Data Control Language\n\n\n\n\n\n\n\n\nFeature\nPL/SQL Syntax\nDB2 Syntax\n\n\n\n\nSelect\nSELECT column1, column2 FROM table_name;\nsimilar"
  },
  {
    "objectID": "comparision/plsql-db2.html#utilities",
    "href": "comparision/plsql-db2.html#utilities",
    "title": "plsql-db2",
    "section": "Utilities",
    "text": "Utilities\n\n\n\n\n\n\n\n\nFeature\nPL/SQL Syntax\nDB2 Syntax\n\n\n\n\nSYS_INFO\n\nSELECT * FROM SYSIBMADM.ENV_SYS_INFO\n\n\nNVL\nNVL(column, 0)\nCOALESCE(column, 0)\n\n\n\nCOALESCE(column, 0)\nCOALESCE(column, 0)\n\n\nDiff Month\nSELECT (EXTRACT(YEAR FROM end_date) - EXTRACT(YEAR FROM start_date)) * 12 + EXTRACT(MONTH FROM end_date) - EXTRACT(MONTH FROM start_date) AS month_diff FROM your_table;\nSELECT (YEAR(end_date) - YEAR(start_date)) * 12 + MONTH(end_date) - MONTH(start_date) AS month_diff FROM your_table;\n\n\nAdd Months\nSELECT ADD_MONTHS(date1, number_of_months) FROM dual;\nSELECT ADD_YEARS(current_date, 3), ADD_MONTHS(current_date, 3 ), ADD_DAYS(current_date, 3), ADD_HOURS(current timestamp, 3), ADD_MINUTES(current timestamp, 3), ADD_SECONDS(current timestamp, 3) FROM sysibm.sysdummy1;\n\n\nDiff Date\nSELECT date2 - date1 FROM dual;\n\n\n\nCompare 2 Tables\n(SELECT * FROM B_TMP MINUS SELECT * FROM B_TMP1) UNION ALL(SELECT * FROM B_TMP1 MINUS SELECT * FROM B_TMP)\nSELECT * FROM table1 EXCEPT SELECT * FROM table2;"
  },
  {
    "objectID": "comparision/plsql-db2.html#references",
    "href": "comparision/plsql-db2.html#references",
    "title": "plsql-db2",
    "section": "References",
    "text": "References\n\ndb2-sql-cookbook"
  },
  {
    "objectID": "comparision/Index.html",
    "href": "comparision/Index.html",
    "title": "Comparision",
    "section": "",
    "text": "R vs Python\nPl/sql vs db2\nDb2 vs mssql"
  },
  {
    "objectID": "python-snippet/Statistics.html",
    "href": "python-snippet/Statistics.html",
    "title": "Statistic",
    "section": "",
    "text": "# https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation/notebook\n\nfrom scipy import stats\nfeatures_list = X_test.columns.values.tolist()\nfor feature in features_list:\n    statistic, pvalue = stats.kstest(X_train[feature], X_test[feature])\n    print(\"p-value %.2f\" %pvalue, \"for the feature\",feature)"
  },
  {
    "objectID": "python-snippet/Statistics.html#ks-test",
    "href": "python-snippet/Statistics.html#ks-test",
    "title": "Statistic",
    "section": "",
    "text": "# https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation/notebook\n\nfrom scipy import stats\nfeatures_list = X_test.columns.values.tolist()\nfor feature in features_list:\n    statistic, pvalue = stats.kstest(X_train[feature], X_test[feature])\n    print(\"p-value %.2f\" %pvalue, \"for the feature\",feature)"
  },
  {
    "objectID": "python-snippet/Statistics.html#lightgbm-focal-loss",
    "href": "python-snippet/Statistics.html#lightgbm-focal-loss",
    "title": "Statistic",
    "section": "2.1 Lightgbm focal loss",
    "text": "2.1 Lightgbm focal loss\n# https://maxhalford.github.io/blog/lightgbm-focal-loss/"
  },
  {
    "objectID": "python-snippet/Statistics.html#metric-processing-time",
    "href": "python-snippet/Statistics.html#metric-processing-time",
    "title": "Statistic",
    "section": "3.1 Metric Processing Time",
    "text": "3.1 Metric Processing Time\nhttps://www.kaggle.com/code/rohanrao/amex-competition-metric-implementations"
  },
  {
    "objectID": "python-snippet/Graphs.html",
    "href": "python-snippet/Graphs.html",
    "title": "Graphs",
    "section": "",
    "text": "Candlestick\nimport yfinance as yf \ndata = yf.download(tickers = 'ETH-USD', period = 'max', interval = '1d')\ndfpl = data[0:100]\nimport plotly.graph_objects as go\nfig = go.Figure(data=go.Candlestick(\n    x = dfpl.index,\n    open = dfpl.Open,\n    high = dfpl.High,\n    low = dfpl.Low,\n    close = dfpl.Close    \n))\n\nfig.show()"
  },
  {
    "objectID": "python-snippet/Environment.html",
    "href": "python-snippet/Environment.html",
    "title": "Environment",
    "section": "",
    "text": "Environment setup\n\nUsing conda\n\nCreate conda environment\n\nQuick create env using conda create\nconda create -n env_rdm python=3.10 pip ipykernel notebook\nconda activate env_rdm\nCreate env use environment.yaml file\n\nFirst, create environment.yaml file with example content following\nSecond, run command conda env create -f environment.yaml\nOptional, remove environment if needed conda env remove -n env_ascore\n\nname: env_ascore\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.10\n  - pandas\n  - joblib\n  - statsmodels\n  - ipykernel\n  - zipp  \n  - pip\n  - pip: \n    - optbinning==0.17.3\n    - ortools==9.4.1874\n# set https_proxy=10.1.33.23:8080\n# set http_proxy=10.1.33.23:8080          \nCreate env use requirements.txt file\n\nconda list --export &gt; requirements.txt\nconda install --file requirements.txt\nConda common commands\n\n\nConda environment list\n\nconda info --env\n\nRemove conda environment\n\nconda deactivate\nconda env remove -n python38\n\nActivate conda environment\n\nconda activate python38\n\nClean unused library\n\nconda clean --all\npip cache remove *\nCreate environment for jupyter notebook\n\nUse conda to create new environment\nUse ipython\nconda activate python38\nipython kernel install --user --name=python38\nRemove jupyter notebook environment (require run as administrator)\njupyter kernelspec list\njupyter kernelspec uninstall python38 \n\nInstall packages in OFFLINE mode with pip\n\nInstall OFFLINE packages using requirements.txt\n\nStep 1: Input to requirements.txt file in current directory. Eg. content \"jupyter-contrib-nbextensions==0.5.1\"\n# export env if available\npip list --format=freeze &gt; requirements.txt\nStep 2: Create wheel folder (Eg. D:)\nStep 3: Run following command to download dependencies packages to folder wheel\n\npip download -r requirements.txt -d wheel\n\nStep 4: Run following command to install\n\npip install -r requirements.txt --find-links=D:\\wheel --no-index\nInstall OFFLINE Linux package\n\nCase 1: Activate same version python (i.e 3.7.0) and run following command\npip download --platform manylinux1_x86_64 --only-binary=:all: --no-binary=:none: pandas\nCase 2: Specify python version\npip download --platform manylinux1_x86_64 --only-binary=:all: --python-version=38 --no-binary=:none: pandas\n\n\nOther ultility commands\n\nCheck dependencies\n\npython -m pip check \npip freeze &gt; requirements.txt\n\nThemes jupyter notebook\n\njt -t onedork -fs 13 -altp -tfs 14 -nfs 14 -cellw 88% -T\n\nInstall Extension for jupyter notebook\n\npip install jupyter_contrib_nbextensions\npip install jupyter_nbextensions_configurator\njupyter contrib nbextension install --user\njupyter nbextensions_configurator enable --user"
  },
  {
    "objectID": "sqlserver-snippet/permissions.html",
    "href": "sqlserver-snippet/permissions.html",
    "title": "Permissions",
    "section": "",
    "text": "Grant create/ excecute\nuse BSCORE_CREDITCARD\ngo\ngrant create procedure to [NOR\\HUYENDT14]\ngrant alter on schema::[dbo] to [NOR\\HUYENDT14]\ngrant execute to [NOR\\HUYENDT14]\n\n\nFind all permissions/access for all users in a database\n/*\n\nSecurity Audit Report\n1) List all access provisioned to a sql user or windows user/group directly \n2) List all access provisioned to a sql user or windows user/group through a database or application role\n3) List all access provisioned to the public role\n\nColumns Returned:\nUserName        : SQL or Windows/Active Directory user account.  This could also be an Active Directory group.\nUserType        : Value will be either 'SQL User' or 'Windows User'.  This reflects the type of user defined for the \n                  SQL Server user account.\nDatabaseUserName: Name of the associated user as defined in the database user account.  The database user may not be the\n                  same as the server user.\nRole            : The role name.  This will be null if the associated permissions to the object are defined at directly\n                  on the user account, otherwise this will be the name of the role that the user is a member of.\nPermissionType  : Type of permissions the user/role has on an object. Examples could include CONNECT, EXECUTE, SELECT\n                  DELETE, INSERT, ALTER, CONTROL, TAKE OWNERSHIP, VIEW DEFINITION, etc.\n                  This value may not be populated for all roles.  Some built in roles have implicit permission\n                  definitions.\nPermissionState : Reflects the state of the permission type, examples could include GRANT, DENY, etc.\n                  This value may not be populated for all roles.  Some built in roles have implicit permission\n                  definitions.\nObjectType      : Type of object the user/role is assigned permissions on.  Examples could include USER_TABLE, \n                  SQL_SCALAR_FUNCTION, SQL_INLINE_TABLE_VALUED_FUNCTION, SQL_STORED_PROCEDURE, VIEW, etc.   \n                  This value may not be populated for all roles.  Some built in roles have implicit permission\n                  definitions.          \nObjectName      : Name of the object that the user/role is assigned permissions on.  \n                  This value may not be populated for all roles.  Some built in roles have implicit permission\n                  definitions.\nColumnName      : Name of the column of the object that the user/role is assigned permissions on. This value\n                  is only populated if the object is a table, view or a table value function.                 \n*/\n\n--List all access provisioned to a sql user or windows user/group directly \nSELECT  \n    [UserName] = CASE princ.[type] \n                    WHEN 'S' THEN princ.[name]\n                    WHEN 'U' THEN ulogin.[name] COLLATE Latin1_General_CI_AI\n                 END,\n    [UserType] = CASE princ.[type]\n                    WHEN 'S' THEN 'SQL User'\n                    WHEN 'U' THEN 'Windows User'\n                 END,  \n    [DatabaseUserName] = princ.[name],       \n    [Role] = null,      \n    [PermissionType] = perm.[permission_name],       \n    [PermissionState] = perm.[state_desc],       \n    [ObjectType] = obj.type_desc,--perm.[class_desc],       \n    [ObjectName] = OBJECT_NAME(perm.major_id),\n    [ColumnName] = col.[name]\nFROM    \n    --database user\n    sys.database_principals princ  \nLEFT JOIN\n    --Login accounts\n    sys.login_token ulogin on princ.[sid] = ulogin.[sid]\nLEFT JOIN        \n    --Permissions\n    sys.database_permissions perm ON perm.[grantee_principal_id] = princ.[principal_id]\nLEFT JOIN\n    --Table columns\n    sys.columns col ON col.[object_id] = perm.major_id \n                    AND col.[column_id] = perm.[minor_id]\nLEFT JOIN\n    sys.objects obj ON perm.[major_id] = obj.[object_id]\nWHERE \n    princ.[type] in ('S','U')\nUNION\n--List all access provisioned to a sql user or windows user/group through a database or application role\nSELECT  \n    [UserName] = CASE memberprinc.[type] \n                    WHEN 'S' THEN memberprinc.[name]\n                    WHEN 'U' THEN ulogin.[name] COLLATE Latin1_General_CI_AI\n                 END,\n    [UserType] = CASE memberprinc.[type]\n                    WHEN 'S' THEN 'SQL User'\n                    WHEN 'U' THEN 'Windows User'\n                 END, \n    [DatabaseUserName] = memberprinc.[name],   \n    [Role] = roleprinc.[name],      \n    [PermissionType] = perm.[permission_name],       \n    [PermissionState] = perm.[state_desc],       \n    [ObjectType] = obj.type_desc,--perm.[class_desc],   \n    [ObjectName] = OBJECT_NAME(perm.major_id),\n    [ColumnName] = col.[name]\nFROM    \n    --Role/member associations\n    sys.database_role_members members\nJOIN\n    --Roles\n    sys.database_principals roleprinc ON roleprinc.[principal_id] = members.[role_principal_id]\nJOIN\n    --Role members (database users)\n    sys.database_principals memberprinc ON memberprinc.[principal_id] = members.[member_principal_id]\nLEFT JOIN\n    --Login accounts\n    sys.login_token ulogin on memberprinc.[sid] = ulogin.[sid]\nLEFT JOIN        \n    --Permissions\n    sys.database_permissions perm ON perm.[grantee_principal_id] = roleprinc.[principal_id]\nLEFT JOIN\n    --Table columns\n    sys.columns col on col.[object_id] = perm.major_id \n                    AND col.[column_id] = perm.[minor_id]\nLEFT JOIN\n    sys.objects obj ON perm.[major_id] = obj.[object_id]\nUNION\n--List all access provisioned to the public role, which everyone gets by default\nSELECT  \n    [UserName] = '{All Users}',\n    [UserType] = '{All Users}', \n    [DatabaseUserName] = '{All Users}',       \n    [Role] = roleprinc.[name],      \n    [PermissionType] = perm.[permission_name],       \n    [PermissionState] = perm.[state_desc],       \n    [ObjectType] = obj.type_desc,--perm.[class_desc],  \n    [ObjectName] = OBJECT_NAME(perm.major_id),\n    [ColumnName] = col.[name]\nFROM    \n    --Roles\n    sys.database_principals roleprinc\nLEFT JOIN        \n    --Role permissions\n    sys.database_permissions perm ON perm.[grantee_principal_id] = roleprinc.[principal_id]\nLEFT JOIN\n    --Table columns\n    sys.columns col on col.[object_id] = perm.major_id \n                    AND col.[column_id] = perm.[minor_id]                   \nJOIN \n    --All objects   \n    sys.objects obj ON obj.[object_id] = perm.[major_id]\nWHERE\n    --Only roles\n    roleprinc.[type] = 'R' AND\n    --Only public role\n    roleprinc.[name] = 'public' AND\n    --Only objects of ours, not the MS objects\n    obj.is_ms_shipped = 0\nORDER BY\n    princ.[Name],\n    OBJECT_NAME(perm.major_id),\n    col.[name],\n    perm.[permission_name],\n    perm.[state_desc],\n    obj.type_desc--perm.[class_desc] \n\n\nReferences\n\nfind-all-permissions"
  },
  {
    "objectID": "sqlserver-snippet/Index.html#schemas",
    "href": "sqlserver-snippet/Index.html#schemas",
    "title": "SQL-Server Snippet",
    "section": "Schemas",
    "text": "Schemas"
  },
  {
    "objectID": "sqlserver-snippet/Index.html#optimize",
    "href": "sqlserver-snippet/Index.html#optimize",
    "title": "SQL-Server Snippet",
    "section": "Optimize",
    "text": "Optimize"
  },
  {
    "objectID": "sqlserver-snippet/Index.html#references",
    "href": "sqlserver-snippet/Index.html#references",
    "title": "SQL-Server Snippet",
    "section": "References",
    "text": "References\n\n[SHORTCUT] (https://manhng.com/blog/sql-query/)\n[SYNONYM] (https://www.sqlservertutorial.net/sql-server-basics/sql-server-synonym/)\n[STORE PROCEDURE] (https://comdy.vn/sql-server/stored-procedure-trong-sql-server/)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Making shareable documents with Quarto",
    "section": "",
    "text": "It’s possible to create beautiful documentation to share online with Quarto that auto-updates with GitHub. This is very new and incredibly cool. This tutorial is an example of a quarto website — it is a really powerful way to create and share your work. You can communicate about science using the same reproducible workflow you and/or your colleagues use for analyses, whether or not you write code.\nCreating websites with Quarto can be done without knowing R, Python or HTML, CSS, etc, and that’s where we’ll start. However, Quarto integrates with these tools so you can make your websites as complex and beautiful as you like as you see examples and reuse and remix from others in the open community. This tutorial borrows heavily from a lot of great tutorials and resources you should check out too – there are links throughout."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Making shareable documents with Quarto",
    "section": "",
    "text": "It’s possible to create beautiful documentation to share online with Quarto that auto-updates with GitHub. This is very new and incredibly cool. This tutorial is an example of a quarto website — it is a really powerful way to create and share your work. You can communicate about science using the same reproducible workflow you and/or your colleagues use for analyses, whether or not you write code.\nCreating websites with Quarto can be done without knowing R, Python or HTML, CSS, etc, and that’s where we’ll start. However, Quarto integrates with these tools so you can make your websites as complex and beautiful as you like as you see examples and reuse and remix from others in the open community. This tutorial borrows heavily from a lot of great tutorials and resources you should check out too – there are links throughout."
  },
  {
    "objectID": "index.html#what-is-quarto",
    "href": "index.html#what-is-quarto",
    "title": "Making shareable documents with Quarto",
    "section": "What is Quarto?",
    "text": "What is Quarto?\nQuarto helps you have your ideas and your code in one place, and present it in a beautiful way.\nQuarto unifies and extends the RMarkdown ecosystem - it unifies by combining the functionality of R Markdown, bookdown, distill, xaringian, etc into a single consistent system. And it extends in several ways: all features are possible beyond R too, including Python and Javascript. It also has more “guardrails”: accessibility and inclusion are centered in the design. Quarto is for people who love RMarkdown, and it’s for people who have never used RMarkdown.\nThe ability for Quarto to streamline collaboration has been so cool and important for our NASA Openscapes project. Quarto has been a common place for us to collaborate - across R and Python languages and coding expertise."
  },
  {
    "objectID": "index.html#what-is-this-tutorial",
    "href": "index.html#what-is-this-tutorial",
    "title": "Making shareable documents with Quarto",
    "section": "What is this tutorial?",
    "text": "What is this tutorial?\nThis is a 1-hour tutorial that can be used to teach or as self-paced learning.\nWe introduce Quarto by exploring this tutorial website, and practicing the basic Quarto workflow using different tools (GitHub browser, RStudio, and Jupyter) for editing your website.\nWe’ll start off from the browser so you don’t need to install any additional software, however this approach is very limited and you will soon outgrow its capabilities. If you don’t already have a workflow to edit files and sync to GitHub from your computer, I recommend RStudio. You don’t need to know R to use RStudio, and it has powerful editor features that make for happy workflows.\nQuarto.org is the go-to place for full documentation and more tutorials!"
  },
  {
    "objectID": "index.html#example-quarto-sites",
    "href": "index.html#example-quarto-sites",
    "title": "Making shareable documents with Quarto",
    "section": "Example Quarto sites",
    "text": "Example Quarto sites\nA few Quarto websites from Openscapes - so far we have been using Quarto for documentation using Quarto and Markdown files and Jupyter Notebooks.\n\nChampions Lessons Series\nOpenscapes Approach Guide\n\n2021 NASA Cloud Hackathon\nFaylab Lab Manual\nA Quarto tip a day, by Mine Çetinkaya-Rundel"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Making shareable documents with Quarto",
    "section": "About",
    "text": "About\nOpenscapes is about better science for future us. We help researchers reimagine data analysis, develop modern skills that are of immediate value to them, and cultivate collaborative and inclusive research teams as part of the broader global open movement.\nWe’re developing this tutorial to help folks with different levels of technical skills use Quarto for documentation and tutorial building. This tutorial was originally created for several different audiences: NASA-Openscapes researcher support engineers using Python, communications directors at organizations promoting open science who do not identify as coders, and fisheries scientists curious about transitioning from RMarkdown. We’re hoping it’s useful to folks with backgrounds as wide as these; if you find it useful or have suggestions for improvement, please let us know by clicking “Edit this page” or “Report an issue” at the upper right side of any page."
  },
  {
    "objectID": "sqlserver-snippet/synonym.html",
    "href": "sqlserver-snippet/synonym.html",
    "title": "Synonym",
    "section": "",
    "text": "Creating a synonym for a table in another database\n\nFirst, create a new database named test and set the current database to test:\nCREATE DATABASE test;\nGO\n\nUSE test;\nGO\nNext, create a new schema named purchasing inside the test database:\nCREATE SCHEMA purchasing;\nGO\nThen, create a new table in the purchasing schema of the test database:\nCREATE TABLE test.purchasing.suppliers\n(\n    supplier_id   INT\n    PRIMARY KEY IDENTITY, \n    supplier_name NVARCHAR(100) NOT NULL\n);\nAfter that, from the BikeStores database, create a synonym for the purchasing.suppliers table in the test database:\nUSE BikeStores;\nGO\n\nCREATE SYNONYM suppliers \nFOR test.purchasing.suppliers;\nFinally, from the BikeStores database, refer to the test.purchasing.suppliers table using the suppliers synonym:\nSELECT * FROM suppliers;\nRemoving a synonym\nDROP SYNONYM IF EXISTS orders;\n\n\n\nReference\n\nsqlservertutorial"
  },
  {
    "objectID": "python-snippet/REST_API.html",
    "href": "python-snippet/REST_API.html",
    "title": "REST API",
    "section": "",
    "text": "Chỉnh sửa lại url, headers, data\n\nimport requests\n\n# set the API endpoint and headers\nurl = 'https://api.example.com/post'\nheaders = {'Content-type': 'application/json', 'Authorization': 'Bearer your_access_token'}\n\n# set the data to be sent in the request\ndata = {'name': 'John Doe', 'email': 'johndoe@example.com'}\n\n# send the POST request using the Requests library\nresponse = requests.post(url, headers=headers, json=data)\n\n# verify the response\nprint(response.status_code)\nprint(response.text)"
  },
  {
    "objectID": "python-snippet/REST_API.html#kiểm-tra-api-sử-dụng-post-method",
    "href": "python-snippet/REST_API.html#kiểm-tra-api-sử-dụng-post-method",
    "title": "REST API",
    "section": "",
    "text": "Chỉnh sửa lại url, headers, data\n\nimport requests\n\n# set the API endpoint and headers\nurl = 'https://api.example.com/post'\nheaders = {'Content-type': 'application/json', 'Authorization': 'Bearer your_access_token'}\n\n# set the data to be sent in the request\ndata = {'name': 'John Doe', 'email': 'johndoe@example.com'}\n\n# send the POST request using the Requests library\nresponse = requests.post(url, headers=headers, json=data)\n\n# verify the response\nprint(response.status_code)\nprint(response.text)"
  },
  {
    "objectID": "python-snippet/Modelling.html",
    "href": "python-snippet/Modelling.html",
    "title": "Modelling",
    "section": "",
    "text": "Model explain\n\ndalex\nmodelStudio - R & Python examples"
  },
  {
    "objectID": "python-snippet/Ultilities.html",
    "href": "python-snippet/Ultilities.html",
    "title": "Ultilities",
    "section": "",
    "text": "Mouse click\nfrom pynput.mouse import Controller, Button\nimport time\nmouse = Controller()\nwhile True:\n  mouse.click(Button.left, 1)\n  print('clicked')  \n  time.sleep(5)"
  },
  {
    "objectID": "python-snippet/Code - optimize.html",
    "href": "python-snippet/Code - optimize.html",
    "title": "Code optimize",
    "section": "",
    "text": "import cProfile\nimport optuna\nimport time\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\n# Define the LSTMModel class and Trainer class here\n\n# Define the objective function for Optuna optimization\ndef objective(trial):\n    # ... (rest of the code)\n\n# Profile the objective function\nprofiler = cProfile.Profile()\nprofiler.enable()\n\n# Perform hyperparameter optimization with Optuna\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\nprofiler.disable()\nprofiler.print_stats(sort='cumulative')"
  },
  {
    "objectID": "python-snippet/Code - optimize.html#profiling",
    "href": "python-snippet/Code - optimize.html#profiling",
    "title": "Code optimize",
    "section": "",
    "text": "import cProfile\nimport optuna\nimport time\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\n# Define the LSTMModel class and Trainer class here\n\n# Define the objective function for Optuna optimization\ndef objective(trial):\n    # ... (rest of the code)\n\n# Profile the objective function\nprofiler = cProfile.Profile()\nprofiler.enable()\n\n# Perform hyperparameter optimization with Optuna\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\nprofiler.disable()\nprofiler.print_stats(sort='cumulative')"
  },
  {
    "objectID": "machine-learning/transformer.html",
    "href": "machine-learning/transformer.html",
    "title": "Transformer",
    "section": "",
    "text": "Kiến trúc Transformer là một mô hình học sâu phổ biến được giới thiệu trong bài báo “Attention is All You Need” của Vaswani và đồng nghiệp (2017). Nó đã cách mạng hóa nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) và trở thành cơ sở cho các mô hình tiên tiến như BERT và GPT.\nKiến trúc Transformer dựa trên khái niệm self-attention, cho phép mô hình tập trung vào các phần khác nhau của chuỗi đầu vào khi xử lý từng phần tử. Dưới đây là một cái nhìn tổng quan về kiến trúc Transformer:\n\n\n\nKiến trúc Transformer\n\n\n\nNhúng Đầu Vào (Input Embeddings):\n\nChuỗi đầu vào được nhúng thành các biểu diễn liên tục được gọi là nhúng đầu vào.\nMỗi từ hoặc thành phần trong chuỗi đầu vào được biểu diễn bằng một vector mật độ.\n\nMã Hóa Vị Trí (Positional Encoding):\n\nMã hóa vị trí được thêm vào nhúng đầu vào để cung cấp thông tin về thứ tự hoặc vị trí của các từ trong chuỗi.\nĐiều này cho phép mô hình bắt chước thông tin tuần tự, vì kiến trúc Transformer không có kết nối lặp lại.\n\nBộ Mã Hóa (Encoder):\n\nTransformer bao gồm một cấu trúc mã hóa-giải mã (encoder-decoder), nhưng để đơn giản, chúng ta tập trung vào bộ mã hóa.\nBộ mã hóa bao gồm nhiều lớp giống nhau, mỗi lớp bao gồm hai lớp con:\n\nMulti-head Self-Attention:\n\nSelf-attention cho phép mô hình đánh trọng số sự quan trọng của các từ khác nhau trong chuỗi đầu vào khi xử lý một từ cụ thể.\nNó tính toán tổng trọng số của các nhúng từ tất cả các từ, trong đó trọng số được xác định bởi sự tương đồng giữa các từ.\n\nFeed-Forward Neural Network:\n\nA simple feed-forward neural network được áp dụng cho từng vị trí trong chuỗi độc lập.\nNó giúp bắt chước mối quan hệ phi tuyến giữa các từ trong chuỗi.\n\n\n\nBộ Giải Mã (Decoder):\n\nBộ giải mã tương tự như bộ mã hóa nhưng có cơ chế attention bổ sung.\nNó tạo ra chuỗi đầu ra bằng cách chú ý đến các biểu diễn đầu ra của bộ mã hóa và các từ đã được tạo ra trước đó.\n\nCơ Chế Che (Masking):\n\nTrong quá trình huấn luyện, cơ chế che được sử dụng để ngăn mô hình chú ý đến các vị trí trong tương lai.\nĐiều này đảm bảo rằng mỗi vị trí chỉ có thể chú ý đến các vị trí trước đó trong chuỗi đầu vào.\n\nTạo Ra Đầu Ra:\n\nLớp cuối cùng của bộ giải mã tạo ra xác suất đầu ra cho mỗi vị trí trong chuỗi đầu ra.\nCác xác suất này thường được tính bằng cách sử dụng hàm kích hoạt softmax.\n\n\nKiến trúc Transformer có một số lợi thế, như khả năng song song hóa, khả năng bắt chước các phụ thuộc xa, và khả năng tổng quát tốt hơn nhờ self-attention. Nó đã đạt được kết quả tiên tiến trên nhiều nhiệm vụ NLP, bao gồm dịch máy, tóm tắt văn bản và hiểu ngôn ngữ.\n\n\n\n\n\nSelf-attention, còn được gọi là intra-attention hay scaled dot-product attention (tích vô hướng giữa 2 vector), là một cơ chế tính toán trọng số chú ý cho các phần tử trong cùng một chuỗi đầu vào. Nó cho phép mô hình đánh giá mức độ quan trọng của mỗi phần tử trong chuỗi dựa trên mức liên quan của nó đối với các phần tử khác.\n\n\n\nTrong self-attention, mỗi từ trong chuỗi đầu vào được biểu diễn bằng một vector, thông thường được gọi là nhúng hay vector nhúng (embedding vector). Giả sử nhúng từ cho chuỗi đầu vào như sau:\nVí dụ câu như sau: “Con mèo ngồi trên chiếc thảm.”\nĐể áp dụng self-attention, chúng ta trước tiên biểu diễn mỗi từ trong câu thành một vector nhúng. Hãy giả sử chúng ta có các vector nhúng từ sau đây:\nNhúng từ:\n- \"Con\": [0.2, 0.5, -0.3]\n- \"mèo\": [-0.1, 0.7, 0.2]\n- \"ngồi\": [0.4, -0.2, 0.6]\n- \"trên\": [-0.5, 0.3, -0.1]\n- \"chiếc\": [0.2, 0.5, -0.3]\n- \"thảm\": [0.3, 0.1, 0.8]\n- \".\": [0.0, 0.0, 0.0]\nBây giờ, chúng ta tính toán trọng số self-attention cho mỗi từ trong câu. Trong self-attention, mỗi từ được so sánh với tất cả các từ khác để xác định độ quan trọng hoặc liên quan của nó. Quá trình so sánh được thực hiện bằng cách tính tích vô hướng giữa nhúng từ, sau đó áp dụng phép softmax để thu được trọng số chú ý.\nVí dụ, đối với từ “Con,” tích vô hướng được tính toán với nhúng từ của tất cả các từ khác:\nTrọng số Chú ý cho “Con”:\n- \"Con\" so sánh với \"Con\": 0.3\n- \"Con\" so sánh với \"mèo\": 0.1\n- \"Con\" so sánh với \"ngồi\": 0.2\n- \"Con\" so sánh với \"trên\": 0.05\n- \"Con\" so sánh với \"chiếc\": 0.35\n- \"Con\" so sánh với \"thảm\": 0.0\n- \"Con\" so sánh với \".\": 0.0\nCác trọng số chú ý phản ánh sự quan trọng của mỗi từ đối với “Con” dựa trên sự tương đồng ngữ nghĩa hoặc liên quan ngữ cảnh của chúng. Phép softmax đảm bảo rằng các trọng số tổng cộng bằng 1.\nChúng ta lặp lại quá trình này cho mỗi từ trong câu để thu được trọng số self-attention cho toàn bộ câu. Những trọng số chú ý này sau đó có thể được sử dụng để tổng hợp thông tin từ các từ khác nhau và bắt chước các mối quan hệ quan trọng trong câu để tiếp tục xử lý trong mô hình.\n\n\n\n\n\n\nMulti-Head Attention là một phần mở rộng của self-attention cho phép mô hình tập trung vào các khía cạnh khác nhau của chuỗi đầu vào cùng một lúc. Đó là việc áp dụng self-attention nhiều lần song song, mỗi attention head tập trung vào một biểu diễn khác nhau của đầu vào. Bằng cách sử dụng nhiều attention head, mô hình có thể nắm bắt các thông tin khác nhau và học các mẫu đa dạng, nâng cao khả năng hiểu và xử lý các chuỗi phức tạp một cách hiệu quả.\n\n\n\n\nXem xét một câu tiếng Anh: “I love cats.” -&gt; Và chúng ta muốn dịch nó sang tiếng Pháp: “J’adore les chats.”\nGiả sử cả hai câu tiếng Anh và tiếng Pháp đã được nhúng thành các vector như sau:\nNhúng từ tiếng Anh:\n\n“I”: [0.1, 0.2, 0.3]\n“love”: [0.4, 0.5, 0.6]\n“cats”: [0.7, 0.8, 0.9]\n\nNhúng từ tiếng Pháp:\n\n“J’adore”: [0.9, 0.8, 0.7]\n“les”: [0.6, 0.5, 0.4]\n“chats”: [0.3, 0.2, 0.1]\n\nTính toán trọng số\n\nGiả sử chúng ta đang sử dụng cơ chế multi-head attention với 2 attention head. Mỗi head sẽ tập trung vào chuỗi đầu vào tiếng Anh (nguồn) và chuỗi đầu vào tiếng Pháp (đích) riêng biệt.\nVới mỗi attention head, chúng ta sẽ tính toán trọng số attention cho cả chuỗi đầu vào tiếng Anh và tiếng Pháp. Các trọng số attention xác định mức độ quan trọng của mỗi từ trong chuỗi nguồn liên quan đến các từ trong chuỗi đích.\nVí dụ,\n\nVới attention head đầu tiên, chúng ta tính toán trọng số attention cho chuỗi đầu vào tiếng Anh:\nAttention_scores_source_head1 = softmax(dot_product(english_source_embeddings, [french_target_embedding1, french_target_embedding2, french_target_embedding3]))\nTương tự, chúng ta tính toán trọng số attention cho chuỗi đích tiếng Pháp:\nAttention_scores_target_head1 = softmax(dot_product(french_target_embeddings, [english_source_embedding1, english_source_embedding2, english_source_embedding3]))\nLặp lại các bước trên cho attention head tiếp theo, kết quả sẽ được các attention score cho cả chuỗi nguồn và chuỗi đích.\nSau khi có các trọng số attention, chúng ta áp dụng chúng vào nhúng tương ứng để có tổng có trọng số cho mỗi chuỗi.\nVí dụ, đối với chuỗi nguồn tiếng Anh và attention head đầu tiên:\nWeighted_sum_source_head1 = attention_score1 * english_source_embedding1 + attention_score2 * english_source_embedding2 + attention_score3 * english_source_embedding3\nTương tự, đối với chuỗi đích tiếng Pháp và attention head đầu tiên:\nWeighted_sum_target_head1 = attention_score1 * french_target_embedding1 + attention_score2 * french_target_embedding2 + attention_score3 * french_target_embedding3\nChúng ta lặp lại các bước trên cho attention head thứ hai.\nCuối cùng, chúng ta nối các đầu ra từ mỗi attention head và thông qua một phép biến đổi tuyến tính để có đầu ra multi-head attention cuối cùng.\nNhư vậy, cơ chế multi-head attention cho phép mô hình nắm bắt các thông tin và mối quan hệ khác nhau giữa chuỗi nguồn và chuỗi đích, tăng khả năng dịch giữa các ngôn ngữ một cách chính xác.\n\n\n\n\nimport torch\nimport torch.nn as nn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.query_projection = nn.Linear(embed_dim, embed_dim)\n        self.key_projection = nn.Linear(embed_dim, embed_dim)\n        self.value_projection = nn.Linear(embed_dim, embed_dim)\n        self.output_projection = nn.Linear(embed_dim, embed_dim)\n        \n    def forward(self, query, key, value):\n        batch_size = query.size(0)\n        \n        # Apply linear projections for query, key, and value\n        query = self.query_projection(query)\n        key = self.key_projection(key)\n        value = self.value_projection(value)\n        \n        # Reshape query, key, and value to split heads\n        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # Compute attention scores\n        scores = torch.matmul(query, key.transpose(-2, -1))\n        scores = scores / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n        attention_weights = torch.softmax(scores, dim=-1)\n        \n        # Apply attention weights to value\n        attended_values = torch.matmul(attention_weights, value)\n        \n        # Reshape and concatenate attended values\n        attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n        \n        # Apply linear projection for output\n        output = self.output_projection(attended_values)\n        \n        return output\n\n\n\n\n\n\n\nPhép mã hóa vị trí (Positional Encoding) là một kỹ thuật được sử dụng trong các mô hình dựa trên chuỗi, đặc biệt là trong kiến trúc Transformer, để đưa thông tin về vị trí tương đối của các yếu tố trong chuỗi đầu vào. Nó giải quyết vấn đề rằng tự chú ý (self-attention) và các cơ chế chú ý khác trong các mô hình này không ngầm định mã hóa thông tin về vị trí.\nTrong các nhiệm vụ xử lý ngôn ngữ tự nhiên, thứ tự của các từ trong một câu mang ý nghĩa và ngữ cảnh quan trọng. Tuy nhiên, mô hình Transformer xử lý chuỗi đầu vào theo cách song song, không rõ ràng ghi nhận thứ tự tuần tự. Phép mã hóa vị trí giúp mô hình phân biệt giữa các yếu tố ở các vị trí khác nhau trong chuỗi.\nMã hóa vị trí được thêm vào nhúng đầu vào của chuỗi trước khi đưa chúng vào mô hình Transformer. Thông thường, điều này được thực hiện bằng cách kết hợp các hàm sin-cos với các nhúng đầu vào. Mỗi chiều của mã hóa vị trí đại diện cho một vị trí cụ thể trong chuỗi.\nSự lựa chọn các hàm sin-cos cho phép mô hình dễ dàng mở rộng cho các độ dài chuỗi dài hơn so với những gì mô hình đã được huấn luyện. Tần số của các hàm sin-cos tuân theo một cấp số học, cho phép mô hình học để chú ý đến các vị trí khác nhau với các tỷ lệ khác nhau.\nBằng cách thêm mã hóa vị trí vào nhúng đầu vào, mô hình Transformer có thể hiệu quả nắm bắt thứ tự và vị trí tương đối của các yếu tố trong chuỗi. Điều này giúp mô hình hiểu các phụ thuộc và mối quan hệ giữa các vị trí khác nhau và cho phép thực hiện các nhiệm vụ yêu cầu xử lý thông tin tuần tự, chẳng hạn như dịch máy hoặc hiểu ngôn ngữ.\n\n\n\n\nGiả sử chúng ta có một chuỗi đầu vào gồm ba từ: [“Tôi”, “yêu”, “mèo”].\nMỗi từ sẽ được biểu diễn bằng một vector nhúng từ, nắm bắt ý nghĩa ngữ nghĩa của nó. Hãy giả định các vector nhúng từ như sau:\n\n“Tôi”: [0.2, 0.3, -0.1]\n“yêu”: [-0.5, 0.7, 0.2]\n“mèo”: [0.7, 0.8, 0.9]\n\nĐể tích hợp thông tin về vị trí, chúng ta sử dụng mã hóa vị trí. Trong ví dụ này, chúng ta sẽ sử dụng các hàm sin và cos để tạo các vector mã hóa vị trí.\nGiả sử chiều nhúng là 3, các vector mã hóa vị trí sẽ được tính như sau:\nCho vị trí 0 (từ đầu tiên): - PE(0) = [sin(0/10000^0), cos(0/10000^0), sin(0/10000^0)]\nCho vị trí 1 (từ thứ hai): - PE(1) = [sin(1/10000^1), cos(1/10000^1), sin(1/10000^1)]\nCho vị trí 2 (từ thứ ba): - PE(2) = [sin(2/10000^0), cos(2/10000^0), sin(2/10000^0)]\nVì sin(0) = 0, sin(1) = 0.8415, sin(2) = 0.9093, cos(0) = 1, cos(1) = 0.5403, cos(2) = -0.4161.\nNên, các vector mã hóa vị trí là:\nPE(0) = [0, 1, 0] PE(1) = [0.8415, 0.5403, 0.8415] PE(2) = [0.9093, -0.4161, 0.9093]\nĐể tích hợp mã hóa vị trí, chúng ta cộng vector mã hóa vị trí tương ứng vào mỗi vector nhúng từ:\n\nTừ “Tôi” với mã hóa vị trí: [0.2, 0.3, -0.1] + [0, 1, 0] = [0.2, 1.3, -0.1]\nTừ “yêu” với mã hóa vị trí: [-0.5, 0.7, 0.2] + [0.8415, 0.5403, 0.8415] =\n\n[0.3415, 1.2403, 1.0415] - Từ “mèo” với mã hóa vị trí: [0.7, 0.8, 0.9] + [0.9093, -0.4161, 0.9093] = [1.6093, 0.3839, 1.8093]\nBằng cách cộng các vector mã hóa vị trí vào các vector nhúng từ, chúng ta giới thiệu thông tin về vị trí tương đối của các từ trong chuỗi đầu vào. Điều này cho phép mô hình Transformer nắm bắt thứ tự tuần tự và các phụ thuộc giữa các từ, điều quan trọng cho các nhiệm vụ như dịch máy hoặc phân tích cảm xúc.\n\n\n\n\nLayer Normalization là một kỹ thuật được sử dụng trong các mô hình học sâu để chuẩn hóa các kích hoạt (kết quả chuyển đổi đầu vào thành đầu ra bằng hàm kích hoạt - tạm gọi là các kích hoạt) của các đơn vị trong một lớp. Nó giải quyết vấn đề của sự thay đổi phân phối đầu vào của lớp trong quá trình huấn luyện, gây khó khăn cho quá trình học.\nCông thức cho Layer Normalization có thể được biểu diễn như sau:\noutput = scale * (input - mean) / sqrt(variance + epsilon) + bias\n\nỞ đây, đầu vào đại diện cho các kích hoạt đầu vào của một lớp, trung bình và phương sai đại diện cho giá trị trung bình và phương sai được tính theo chiều đầu vào, epsilon là một hằng số nhỏ được thêm vào để đảm bảo tính ổn định số học, và scale và bias là các tham số có thể học giúp mô hình điều chỉnh và dịch chuyển các kích hoạt đã được chuẩn hóa.\nLayer Normalization được áp dụng độc lập cho mỗi lô dữ liệu (batch), điều này làm cho nó phù hợp cho các nhiệm vụ với kích thước lô nhỏ. Nó đã được chứng minh là hiệu quả trong ổn định quá trình huấn luyện, tăng tốc quá trình hội tụ và cải thiện hiệu suất tổng quát của các mô hình học sâu.\nTổng quát, Layer Normalization giúp giảm sự phụ thuộc vào tỷ lệ của các kích hoạt đầu vào, thúc đẩy việc truyền gradient ổn định và cải thiện động học huấn luyện tổng thể (overall training dynamics - gồm learning rate, gradient, loss rate) của mạng neural. Nó đã được sử dụng rộng rãi trong các lĩnh vực khác nhau, bao gồm xử lý ngôn ngữ tự nhiên, thị giác máy tính và nhận dạng giọng nói.\n\n\n\nGiả sử chúng ta có một câu: “Tôi thích chơi bóng đá.”\nTrong mô hình ngôn ngữ, mục tiêu là dự đoán từ tiếp theo trong ngữ cảnh đã cho. Tuy nhiên, trong quá trình huấn luyện, chúng ta thường không muốn cho mô hình truy cập vào các từ trong tương lai vì nó có thể dẫn đến rò rỉ dữ liệu và dự đoán không thực tế. Vì vậy, chế che được áp dụng.\nĐể áp dụng cơ chế che, thường được ký hiệu là [MASK], để thay thế một số từ trong chuỗi đầu vào trong quá trình huấn luyện. Trong trường hợp này, chúng ta có thể ngẫu nhiên che một số từ trong câu, dẫn đến:\nCâu Đầu Vào (có cơ chế che): “Tôi thích chơi [MASK] bóng đá.”\nBây giờ, trong quá trình huấn luyện, mục tiêu của mô hình là dự đoán từ bị che dựa trên ngữ cảnh xung quanh. Nó học cách hiểu cấu trúc câu, ý nghĩa và sự phụ thuộc giữa các từ để đưa ra dự đoán chính xác. Mô hình nhận chuỗi đầu vào đã được chỉnh sửa (thêm [MASK]) và cố gắng dự đoán bản gốc, không bị che, ở vị trí đã được che.\nTrong quá trình huấn luyện hoặc đánh giá, cơ chế che không được sử dụng và mô hình được cung cấp với toàn bộ chuỗi đầu vào. Nó có thể tạo ra dự đoán cho từ tiếp theo trong chuỗi dựa trên kiến thức đã học từ quá trình huấn luyện.\nCơ chế che giúp mô hình tổng quát tốt và xử lý các trường hợp mà nó gặp phải các từ chưa được nhìn thấy hoặc thiếu. Nó khuyến khích mô hình dựa vào ngữ cảnh có sẵn để đưa ra dự đoán chính xác và cải thiện khả năng hiểu cấu trúc câu."
  },
  {
    "objectID": "machine-learning/transformer.html#transformer-architecture",
    "href": "machine-learning/transformer.html#transformer-architecture",
    "title": "Transformer",
    "section": "",
    "text": "Kiến trúc Transformer là một mô hình học sâu phổ biến được giới thiệu trong bài báo “Attention is All You Need” của Vaswani và đồng nghiệp (2017). Nó đã cách mạng hóa nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) và trở thành cơ sở cho các mô hình tiên tiến như BERT và GPT.\nKiến trúc Transformer dựa trên khái niệm self-attention, cho phép mô hình tập trung vào các phần khác nhau của chuỗi đầu vào khi xử lý từng phần tử. Dưới đây là một cái nhìn tổng quan về kiến trúc Transformer:\n\n\n\nKiến trúc Transformer\n\n\n\nNhúng Đầu Vào (Input Embeddings):\n\nChuỗi đầu vào được nhúng thành các biểu diễn liên tục được gọi là nhúng đầu vào.\nMỗi từ hoặc thành phần trong chuỗi đầu vào được biểu diễn bằng một vector mật độ.\n\nMã Hóa Vị Trí (Positional Encoding):\n\nMã hóa vị trí được thêm vào nhúng đầu vào để cung cấp thông tin về thứ tự hoặc vị trí của các từ trong chuỗi.\nĐiều này cho phép mô hình bắt chước thông tin tuần tự, vì kiến trúc Transformer không có kết nối lặp lại.\n\nBộ Mã Hóa (Encoder):\n\nTransformer bao gồm một cấu trúc mã hóa-giải mã (encoder-decoder), nhưng để đơn giản, chúng ta tập trung vào bộ mã hóa.\nBộ mã hóa bao gồm nhiều lớp giống nhau, mỗi lớp bao gồm hai lớp con:\n\nMulti-head Self-Attention:\n\nSelf-attention cho phép mô hình đánh trọng số sự quan trọng của các từ khác nhau trong chuỗi đầu vào khi xử lý một từ cụ thể.\nNó tính toán tổng trọng số của các nhúng từ tất cả các từ, trong đó trọng số được xác định bởi sự tương đồng giữa các từ.\n\nFeed-Forward Neural Network:\n\nA simple feed-forward neural network được áp dụng cho từng vị trí trong chuỗi độc lập.\nNó giúp bắt chước mối quan hệ phi tuyến giữa các từ trong chuỗi.\n\n\n\nBộ Giải Mã (Decoder):\n\nBộ giải mã tương tự như bộ mã hóa nhưng có cơ chế attention bổ sung.\nNó tạo ra chuỗi đầu ra bằng cách chú ý đến các biểu diễn đầu ra của bộ mã hóa và các từ đã được tạo ra trước đó.\n\nCơ Chế Che (Masking):\n\nTrong quá trình huấn luyện, cơ chế che được sử dụng để ngăn mô hình chú ý đến các vị trí trong tương lai.\nĐiều này đảm bảo rằng mỗi vị trí chỉ có thể chú ý đến các vị trí trước đó trong chuỗi đầu vào.\n\nTạo Ra Đầu Ra:\n\nLớp cuối cùng của bộ giải mã tạo ra xác suất đầu ra cho mỗi vị trí trong chuỗi đầu ra.\nCác xác suất này thường được tính bằng cách sử dụng hàm kích hoạt softmax.\n\n\nKiến trúc Transformer có một số lợi thế, như khả năng song song hóa, khả năng bắt chước các phụ thuộc xa, và khả năng tổng quát tốt hơn nhờ self-attention. Nó đã đạt được kết quả tiên tiến trên nhiều nhiệm vụ NLP, bao gồm dịch máy, tóm tắt văn bản và hiểu ngôn ngữ."
  },
  {
    "objectID": "machine-learning/transformer.html#self-attention",
    "href": "machine-learning/transformer.html#self-attention",
    "title": "Transformer",
    "section": "",
    "text": "Self-attention, còn được gọi là intra-attention hay scaled dot-product attention (tích vô hướng giữa 2 vector), là một cơ chế tính toán trọng số chú ý cho các phần tử trong cùng một chuỗi đầu vào. Nó cho phép mô hình đánh giá mức độ quan trọng của mỗi phần tử trong chuỗi dựa trên mức liên quan của nó đối với các phần tử khác.\n\n\n\nTrong self-attention, mỗi từ trong chuỗi đầu vào được biểu diễn bằng một vector, thông thường được gọi là nhúng hay vector nhúng (embedding vector). Giả sử nhúng từ cho chuỗi đầu vào như sau:\nVí dụ câu như sau: “Con mèo ngồi trên chiếc thảm.”\nĐể áp dụng self-attention, chúng ta trước tiên biểu diễn mỗi từ trong câu thành một vector nhúng. Hãy giả sử chúng ta có các vector nhúng từ sau đây:\nNhúng từ:\n- \"Con\": [0.2, 0.5, -0.3]\n- \"mèo\": [-0.1, 0.7, 0.2]\n- \"ngồi\": [0.4, -0.2, 0.6]\n- \"trên\": [-0.5, 0.3, -0.1]\n- \"chiếc\": [0.2, 0.5, -0.3]\n- \"thảm\": [0.3, 0.1, 0.8]\n- \".\": [0.0, 0.0, 0.0]\nBây giờ, chúng ta tính toán trọng số self-attention cho mỗi từ trong câu. Trong self-attention, mỗi từ được so sánh với tất cả các từ khác để xác định độ quan trọng hoặc liên quan của nó. Quá trình so sánh được thực hiện bằng cách tính tích vô hướng giữa nhúng từ, sau đó áp dụng phép softmax để thu được trọng số chú ý.\nVí dụ, đối với từ “Con,” tích vô hướng được tính toán với nhúng từ của tất cả các từ khác:\nTrọng số Chú ý cho “Con”:\n- \"Con\" so sánh với \"Con\": 0.3\n- \"Con\" so sánh với \"mèo\": 0.1\n- \"Con\" so sánh với \"ngồi\": 0.2\n- \"Con\" so sánh với \"trên\": 0.05\n- \"Con\" so sánh với \"chiếc\": 0.35\n- \"Con\" so sánh với \"thảm\": 0.0\n- \"Con\" so sánh với \".\": 0.0\nCác trọng số chú ý phản ánh sự quan trọng của mỗi từ đối với “Con” dựa trên sự tương đồng ngữ nghĩa hoặc liên quan ngữ cảnh của chúng. Phép softmax đảm bảo rằng các trọng số tổng cộng bằng 1.\nChúng ta lặp lại quá trình này cho mỗi từ trong câu để thu được trọng số self-attention cho toàn bộ câu. Những trọng số chú ý này sau đó có thể được sử dụng để tổng hợp thông tin từ các từ khác nhau và bắt chước các mối quan hệ quan trọng trong câu để tiếp tục xử lý trong mô hình."
  },
  {
    "objectID": "machine-learning/transformer.html#multi-head-attention",
    "href": "machine-learning/transformer.html#multi-head-attention",
    "title": "Transformer",
    "section": "",
    "text": "Multi-Head Attention là một phần mở rộng của self-attention cho phép mô hình tập trung vào các khía cạnh khác nhau của chuỗi đầu vào cùng một lúc. Đó là việc áp dụng self-attention nhiều lần song song, mỗi attention head tập trung vào một biểu diễn khác nhau của đầu vào. Bằng cách sử dụng nhiều attention head, mô hình có thể nắm bắt các thông tin khác nhau và học các mẫu đa dạng, nâng cao khả năng hiểu và xử lý các chuỗi phức tạp một cách hiệu quả.\n\n\n\n\nXem xét một câu tiếng Anh: “I love cats.” -&gt; Và chúng ta muốn dịch nó sang tiếng Pháp: “J’adore les chats.”\nGiả sử cả hai câu tiếng Anh và tiếng Pháp đã được nhúng thành các vector như sau:\nNhúng từ tiếng Anh:\n\n“I”: [0.1, 0.2, 0.3]\n“love”: [0.4, 0.5, 0.6]\n“cats”: [0.7, 0.8, 0.9]\n\nNhúng từ tiếng Pháp:\n\n“J’adore”: [0.9, 0.8, 0.7]\n“les”: [0.6, 0.5, 0.4]\n“chats”: [0.3, 0.2, 0.1]\n\nTính toán trọng số\n\nGiả sử chúng ta đang sử dụng cơ chế multi-head attention với 2 attention head. Mỗi head sẽ tập trung vào chuỗi đầu vào tiếng Anh (nguồn) và chuỗi đầu vào tiếng Pháp (đích) riêng biệt.\nVới mỗi attention head, chúng ta sẽ tính toán trọng số attention cho cả chuỗi đầu vào tiếng Anh và tiếng Pháp. Các trọng số attention xác định mức độ quan trọng của mỗi từ trong chuỗi nguồn liên quan đến các từ trong chuỗi đích.\nVí dụ,\n\nVới attention head đầu tiên, chúng ta tính toán trọng số attention cho chuỗi đầu vào tiếng Anh:\nAttention_scores_source_head1 = softmax(dot_product(english_source_embeddings, [french_target_embedding1, french_target_embedding2, french_target_embedding3]))\nTương tự, chúng ta tính toán trọng số attention cho chuỗi đích tiếng Pháp:\nAttention_scores_target_head1 = softmax(dot_product(french_target_embeddings, [english_source_embedding1, english_source_embedding2, english_source_embedding3]))\nLặp lại các bước trên cho attention head tiếp theo, kết quả sẽ được các attention score cho cả chuỗi nguồn và chuỗi đích.\nSau khi có các trọng số attention, chúng ta áp dụng chúng vào nhúng tương ứng để có tổng có trọng số cho mỗi chuỗi.\nVí dụ, đối với chuỗi nguồn tiếng Anh và attention head đầu tiên:\nWeighted_sum_source_head1 = attention_score1 * english_source_embedding1 + attention_score2 * english_source_embedding2 + attention_score3 * english_source_embedding3\nTương tự, đối với chuỗi đích tiếng Pháp và attention head đầu tiên:\nWeighted_sum_target_head1 = attention_score1 * french_target_embedding1 + attention_score2 * french_target_embedding2 + attention_score3 * french_target_embedding3\nChúng ta lặp lại các bước trên cho attention head thứ hai.\nCuối cùng, chúng ta nối các đầu ra từ mỗi attention head và thông qua một phép biến đổi tuyến tính để có đầu ra multi-head attention cuối cùng.\nNhư vậy, cơ chế multi-head attention cho phép mô hình nắm bắt các thông tin và mối quan hệ khác nhau giữa chuỗi nguồn và chuỗi đích, tăng khả năng dịch giữa các ngôn ngữ một cách chính xác.\n\n\n\n\nimport torch\nimport torch.nn as nn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        self.query_projection = nn.Linear(embed_dim, embed_dim)\n        self.key_projection = nn.Linear(embed_dim, embed_dim)\n        self.value_projection = nn.Linear(embed_dim, embed_dim)\n        self.output_projection = nn.Linear(embed_dim, embed_dim)\n        \n    def forward(self, query, key, value):\n        batch_size = query.size(0)\n        \n        # Apply linear projections for query, key, and value\n        query = self.query_projection(query)\n        key = self.key_projection(key)\n        value = self.value_projection(value)\n        \n        # Reshape query, key, and value to split heads\n        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # Compute attention scores\n        scores = torch.matmul(query, key.transpose(-2, -1))\n        scores = scores / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n        attention_weights = torch.softmax(scores, dim=-1)\n        \n        # Apply attention weights to value\n        attended_values = torch.matmul(attention_weights, value)\n        \n        # Reshape and concatenate attended values\n        attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n        \n        # Apply linear projection for output\n        output = self.output_projection(attended_values)\n        \n        return output"
  },
  {
    "objectID": "machine-learning/transformer.html#positional-encoding",
    "href": "machine-learning/transformer.html#positional-encoding",
    "title": "Transformer",
    "section": "",
    "text": "Phép mã hóa vị trí (Positional Encoding) là một kỹ thuật được sử dụng trong các mô hình dựa trên chuỗi, đặc biệt là trong kiến trúc Transformer, để đưa thông tin về vị trí tương đối của các yếu tố trong chuỗi đầu vào. Nó giải quyết vấn đề rằng tự chú ý (self-attention) và các cơ chế chú ý khác trong các mô hình này không ngầm định mã hóa thông tin về vị trí.\nTrong các nhiệm vụ xử lý ngôn ngữ tự nhiên, thứ tự của các từ trong một câu mang ý nghĩa và ngữ cảnh quan trọng. Tuy nhiên, mô hình Transformer xử lý chuỗi đầu vào theo cách song song, không rõ ràng ghi nhận thứ tự tuần tự. Phép mã hóa vị trí giúp mô hình phân biệt giữa các yếu tố ở các vị trí khác nhau trong chuỗi.\nMã hóa vị trí được thêm vào nhúng đầu vào của chuỗi trước khi đưa chúng vào mô hình Transformer. Thông thường, điều này được thực hiện bằng cách kết hợp các hàm sin-cos với các nhúng đầu vào. Mỗi chiều của mã hóa vị trí đại diện cho một vị trí cụ thể trong chuỗi.\nSự lựa chọn các hàm sin-cos cho phép mô hình dễ dàng mở rộng cho các độ dài chuỗi dài hơn so với những gì mô hình đã được huấn luyện. Tần số của các hàm sin-cos tuân theo một cấp số học, cho phép mô hình học để chú ý đến các vị trí khác nhau với các tỷ lệ khác nhau.\nBằng cách thêm mã hóa vị trí vào nhúng đầu vào, mô hình Transformer có thể hiệu quả nắm bắt thứ tự và vị trí tương đối của các yếu tố trong chuỗi. Điều này giúp mô hình hiểu các phụ thuộc và mối quan hệ giữa các vị trí khác nhau và cho phép thực hiện các nhiệm vụ yêu cầu xử lý thông tin tuần tự, chẳng hạn như dịch máy hoặc hiểu ngôn ngữ.\n\n\n\n\nGiả sử chúng ta có một chuỗi đầu vào gồm ba từ: [“Tôi”, “yêu”, “mèo”].\nMỗi từ sẽ được biểu diễn bằng một vector nhúng từ, nắm bắt ý nghĩa ngữ nghĩa của nó. Hãy giả định các vector nhúng từ như sau:\n\n“Tôi”: [0.2, 0.3, -0.1]\n“yêu”: [-0.5, 0.7, 0.2]\n“mèo”: [0.7, 0.8, 0.9]\n\nĐể tích hợp thông tin về vị trí, chúng ta sử dụng mã hóa vị trí. Trong ví dụ này, chúng ta sẽ sử dụng các hàm sin và cos để tạo các vector mã hóa vị trí.\nGiả sử chiều nhúng là 3, các vector mã hóa vị trí sẽ được tính như sau:\nCho vị trí 0 (từ đầu tiên): - PE(0) = [sin(0/10000^0), cos(0/10000^0), sin(0/10000^0)]\nCho vị trí 1 (từ thứ hai): - PE(1) = [sin(1/10000^1), cos(1/10000^1), sin(1/10000^1)]\nCho vị trí 2 (từ thứ ba): - PE(2) = [sin(2/10000^0), cos(2/10000^0), sin(2/10000^0)]\nVì sin(0) = 0, sin(1) = 0.8415, sin(2) = 0.9093, cos(0) = 1, cos(1) = 0.5403, cos(2) = -0.4161.\nNên, các vector mã hóa vị trí là:\nPE(0) = [0, 1, 0] PE(1) = [0.8415, 0.5403, 0.8415] PE(2) = [0.9093, -0.4161, 0.9093]\nĐể tích hợp mã hóa vị trí, chúng ta cộng vector mã hóa vị trí tương ứng vào mỗi vector nhúng từ:\n\nTừ “Tôi” với mã hóa vị trí: [0.2, 0.3, -0.1] + [0, 1, 0] = [0.2, 1.3, -0.1]\nTừ “yêu” với mã hóa vị trí: [-0.5, 0.7, 0.2] + [0.8415, 0.5403, 0.8415] =\n\n[0.3415, 1.2403, 1.0415] - Từ “mèo” với mã hóa vị trí: [0.7, 0.8, 0.9] + [0.9093, -0.4161, 0.9093] = [1.6093, 0.3839, 1.8093]\nBằng cách cộng các vector mã hóa vị trí vào các vector nhúng từ, chúng ta giới thiệu thông tin về vị trí tương đối của các từ trong chuỗi đầu vào. Điều này cho phép mô hình Transformer nắm bắt thứ tự tuần tự và các phụ thuộc giữa các từ, điều quan trọng cho các nhiệm vụ như dịch máy hoặc phân tích cảm xúc."
  },
  {
    "objectID": "machine-learning/transformer.html#layer-normalization",
    "href": "machine-learning/transformer.html#layer-normalization",
    "title": "Transformer",
    "section": "",
    "text": "Layer Normalization là một kỹ thuật được sử dụng trong các mô hình học sâu để chuẩn hóa các kích hoạt (kết quả chuyển đổi đầu vào thành đầu ra bằng hàm kích hoạt - tạm gọi là các kích hoạt) của các đơn vị trong một lớp. Nó giải quyết vấn đề của sự thay đổi phân phối đầu vào của lớp trong quá trình huấn luyện, gây khó khăn cho quá trình học.\nCông thức cho Layer Normalization có thể được biểu diễn như sau:\noutput = scale * (input - mean) / sqrt(variance + epsilon) + bias\n\nỞ đây, đầu vào đại diện cho các kích hoạt đầu vào của một lớp, trung bình và phương sai đại diện cho giá trị trung bình và phương sai được tính theo chiều đầu vào, epsilon là một hằng số nhỏ được thêm vào để đảm bảo tính ổn định số học, và scale và bias là các tham số có thể học giúp mô hình điều chỉnh và dịch chuyển các kích hoạt đã được chuẩn hóa.\nLayer Normalization được áp dụng độc lập cho mỗi lô dữ liệu (batch), điều này làm cho nó phù hợp cho các nhiệm vụ với kích thước lô nhỏ. Nó đã được chứng minh là hiệu quả trong ổn định quá trình huấn luyện, tăng tốc quá trình hội tụ và cải thiện hiệu suất tổng quát của các mô hình học sâu.\nTổng quát, Layer Normalization giúp giảm sự phụ thuộc vào tỷ lệ của các kích hoạt đầu vào, thúc đẩy việc truyền gradient ổn định và cải thiện động học huấn luyện tổng thể (overall training dynamics - gồm learning rate, gradient, loss rate) của mạng neural. Nó đã được sử dụng rộng rãi trong các lĩnh vực khác nhau, bao gồm xử lý ngôn ngữ tự nhiên, thị giác máy tính và nhận dạng giọng nói."
  },
  {
    "objectID": "machine-learning/transformer.html#masking",
    "href": "machine-learning/transformer.html#masking",
    "title": "Transformer",
    "section": "",
    "text": "Giả sử chúng ta có một câu: “Tôi thích chơi bóng đá.”\nTrong mô hình ngôn ngữ, mục tiêu là dự đoán từ tiếp theo trong ngữ cảnh đã cho. Tuy nhiên, trong quá trình huấn luyện, chúng ta thường không muốn cho mô hình truy cập vào các từ trong tương lai vì nó có thể dẫn đến rò rỉ dữ liệu và dự đoán không thực tế. Vì vậy, chế che được áp dụng.\nĐể áp dụng cơ chế che, thường được ký hiệu là [MASK], để thay thế một số từ trong chuỗi đầu vào trong quá trình huấn luyện. Trong trường hợp này, chúng ta có thể ngẫu nhiên che một số từ trong câu, dẫn đến:\nCâu Đầu Vào (có cơ chế che): “Tôi thích chơi [MASK] bóng đá.”\nBây giờ, trong quá trình huấn luyện, mục tiêu của mô hình là dự đoán từ bị che dựa trên ngữ cảnh xung quanh. Nó học cách hiểu cấu trúc câu, ý nghĩa và sự phụ thuộc giữa các từ để đưa ra dự đoán chính xác. Mô hình nhận chuỗi đầu vào đã được chỉnh sửa (thêm [MASK]) và cố gắng dự đoán bản gốc, không bị che, ở vị trí đã được che.\nTrong quá trình huấn luyện hoặc đánh giá, cơ chế che không được sử dụng và mô hình được cung cấp với toàn bộ chuỗi đầu vào. Nó có thể tạo ra dự đoán cho từ tiếp theo trong chuỗi dựa trên kiến thức đã học từ quá trình huấn luyện.\nCơ chế che giúp mô hình tổng quát tốt và xử lý các trường hợp mà nó gặp phải các từ chưa được nhìn thấy hoặc thiếu. Nó khuyến khích mô hình dựa vào ngữ cảnh có sẵn để đưa ra dự đoán chính xác và cải thiện khả năng hiểu cấu trúc câu."
  },
  {
    "objectID": "comparision/dplyr-pandas.html",
    "href": "comparision/dplyr-pandas.html",
    "title": "dplyr vs pandas",
    "section": "",
    "text": "Here’s a markdown table comparing dplyr (R) and pandas (Python) for various data manipulation tasks:\n\nData frame verbs\n\nRows\n\n\n\n\n\n\n\n\nFeature\ndplyr (R)\npandas (Python)\n\n\n\n\nArrange\narrange(df, col)\ndf.sort_values(by='col', ascending=True)\n\n\nDistinct\ndistinct(df, col)\ndf.drop_duplicates(subset='col')\n\n\nFilter\nfilter(df, condition)\ndf[df['condition']]\n\n\nSlice\nslice(df, rows)\ndf.iloc[rows]\n\n\n\nColumns\n\n\n\n\n\n\n\n\nFeature\ndplyr (R)\npandas (Python)\n\n\n\n\nGlimpse\nglimpse(df)\ndf.info()\n\n\nMutate\nmutate(df, new_col = func(old_col))\ndf['new_col'] = df['old_col'].apply(func)\n\n\nPull\npull(df, col)\ndf['col']\n\n\nRename\nrename(df, new_col_name = old_col_name)\ndf.rename(columns={'old_col_name': 'new_col_name'}, inplace=True)\n\n\nSelect\nselect(df, col1, col2, ...)\ndf[['col1', 'col2', ...]]\n\n\n\nGroups\n\n\n\n\n\n\n\n\nFeature\ndplyr (R)\npandas (Python)\n\n\n\n\nGroup By\ngroup_by(df, col)\ndf.groupby('col')\n\n\nSummarise\nsummarise(df, new_col = func(col))\ndf.agg({'col': func})\n\n\n\nData frames\n\n\n\n\n\n\n\n\nFeature\ndplyr (R)\npandas (Python)\n\n\n\n\nbind_cols\nbind_cols(df1, df2)\npd.concat([df1, df2], axis=1)\n\n\nbind_rows\nbind_rows(df1, df2)\npd.concat([df1, df2], axis=0)\n\n\nInner Join\ninner_join(df1, df2, by = \"key_column\")\npd.merge(df1, df2, on='key_column', how='inner')\n\n\nLeft Join\nleft_join(df1, df2, by = \"key_column\")\npd.merge(df1, df2, on='key_column', how='left')\n\n\nRight Join\nright_join(df1, df2, by = \"key_column\")\npd.merge(df1, df2, on='key_column', how='right')\n\n\nFull Join\nfull_join(df1, df2, by = \"key_column\")\npd.merge(df1, df2, on='key_column', how='outer')\n\n\nSemi Join\nsemi_join(df1, df2, by = \"key_column\")\nNot directly available; can use merge and isin together for similar effect.\n\n\nAnti Join\nanti_join(df1, df2, by = \"key_column\")\nNot directly available; can use merge and isin together for similar effect.\n\n\n\nVector functions\n\n\n\n\n\n\n\n\nFeature\ndplyr (R)\npandas (Python)\n\n\n\n\nif_else\nmutate(df, new_col = if_else(condition, true_val, false_val))\ndf['new_col'] = np.where(condition, true_val, false_val)\n\n\nna_if\nmutate(df, col = na_if(col, value))\ndf['col'].replace(value, np.nan)\n\n\nn_distinct\nn_distinct(df, col)\ndf['col'].nunique()\n\n\nsample_n\nsample_n(df, n)\ndf.sample(n=n)\n\n\nsample_frac\nsample_frac(df, fraction)\ndf.sample(frac=fraction)\n\n\ncase_when\nmutate(df, new_col = case_when(condition1 ~ value1, condition2 ~ value2, ...))\ndf['new_col'] = np.select([condition1, condition2, ...], [value1, value2, ...], default=default_value)\n\n\ncummean\nmutate(df, new_col = cummean(col))\ndf['new_col'] = df['col'].expanding().mean()\n\n\nrow_number\nmutate(df, row_num = row_number())\ndf['row_num'] = range(1, len(df)+1)\n\n\nmin_rank\nmutate(df, rank = min_rank(col))\ndf['rank'] = df['col'].rank(method='min')\n\n\ndense_rank\nmutate(df, rank = dense_rank(col))\ndf['rank'] = df['col'].rank(method='dense')\n\n\n\nNote that while many functionalities are directly available in both dplyr and pandas, some might require slight variations or custom functions to achieve the same result."
  }
]